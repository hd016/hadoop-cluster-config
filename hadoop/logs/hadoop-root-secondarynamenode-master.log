2019-04-23 19:35:14,376 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting SecondaryNameNode
STARTUP_MSG:   host = master/192.168.178.37
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.7.3
STARTUP_MSG:   classpath = /home/hadoop-2.7.3/etc/hadoop:/home/hadoop-2.7.3/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/stax-api-1.0-2.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/activation-1.1.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/jersey-server-1.9.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/asm-3.2.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/log4j-1.2.17.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/jets3t-0.9.0.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/httpclient-4.2.5.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/httpcore-4.2.5.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/commons-lang-2.6.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/commons-configuration-1.6.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/commons-digester-1.8.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/avro-1.7.4.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/paranamer-2.3.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/commons-compress-1.4.1.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/xz-1.0.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/gson-2.2.4.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/hadoop-auth-2.7.3.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/zookeeper-3.4.6.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/netty-3.6.2.Final.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/curator-framework-2.7.1.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/curator-client-2.7.1.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/jsch-0.1.42.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/htrace-core-3.1.0-incubating.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/junit-4.11.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/hamcrest-core-1.3.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/mockito-all-1.8.5.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/hadoop-annotations-2.7.3.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/guava-11.0.2.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/jsr305-3.0.0.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/commons-cli-1.2.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/commons-math3-3.1.1.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/xmlenc-0.52.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/commons-httpclient-3.1.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/commons-logging-1.1.3.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/commons-codec-1.4.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/commons-io-2.4.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/commons-net-3.1.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/commons-collections-3.2.2.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/servlet-api-2.5.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/jetty-6.1.26.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/jetty-util-6.1.26.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/jsp-api-2.1.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/jersey-core-1.9.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/jersey-json-1.9.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/jettison-1.1.jar:/home/hadoop-2.7.3/share/hadoop/common/hadoop-common-2.7.3.jar:/home/hadoop-2.7.3/share/hadoop/common/hadoop-common-2.7.3-tests.jar:/home/hadoop-2.7.3/share/hadoop/common/hadoop-nfs-2.7.3.jar:/home/hadoop-2.7.3/share/hadoop/hdfs:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/guava-11.0.2.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-io-2.4.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/asm-3.2.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/htrace-core-3.1.0-incubating.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/hadoop-hdfs-2.7.3.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/hadoop-hdfs-2.7.3-tests.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/hadoop-hdfs-nfs-2.7.3.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/commons-lang-2.6.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/guava-11.0.2.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/commons-cli-1.2.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/log4j-1.2.17.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/activation-1.1.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/xz-1.0.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/servlet-api-2.5.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/commons-codec-1.4.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-core-1.9.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-client-1.9.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/guice-3.0.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/javax.inject-1.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/aopalliance-1.0.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/commons-io-2.4.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-server-1.9.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/asm-3.2.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-json-1.9.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/jettison-1.1.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/commons-collections-3.2.2.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/jetty-6.1.26.jar:/home/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-api-2.7.3.jar:/home/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-common-2.7.3.jar:/home/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-common-2.7.3.jar:/home/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.7.3.jar:/home/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.7.3.jar:/home/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.7.3.jar:/home/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.7.3.jar:/home/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-tests-2.7.3.jar:/home/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-client-2.7.3.jar:/home/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.7.3.jar:/home/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.3.jar:/home/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.7.3.jar:/home/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-registry-2.7.3.jar:/hpme/hadoop-2.7.3/share/hadoop/mapreduce/*:/home/hadoop-2.7.3/contrib/capacity-scheduler/*.jar:/home/hadoop-2.7.3/contrib/capacity-scheduler/*.jar:/home/hadoop-2.7.3/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r baa91f7c6bc9cb92be5982de4719c1c8af91ccff; compiled by 'root' on 2016-08-18T01:41Z
STARTUP_MSG:   java = 1.8.0_101
************************************************************/
2019-04-23 19:35:14,390 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: registered UNIX signal handlers for [TERM, HUP, INT]
2019-04-23 19:35:14,821 WARN org.apache.hadoop.util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2019-04-23 19:35:14,904 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2019-04-23 19:35:15,022 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2019-04-23 19:35:15,022 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: SecondaryNameNode metrics system started
2019-04-23 19:35:15,248 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /tmp/hadoop-root/dfs/namesecondary/in_use.lock acquired by nodename 10766@master
2019-04-23 19:35:15,252 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: No KeyProvider found.
2019-04-23 19:35:15,252 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsLock is fair:true
2019-04-23 19:35:15,311 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.block.invalidate.limit=1000
2019-04-23 19:35:15,311 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true
2019-04-23 19:35:15,312 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
2019-04-23 19:35:15,315 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: The block deletion will start around 2019 Apr 23 19:35:15
2019-04-23 19:35:15,317 INFO org.apache.hadoop.util.GSet: Computing capacity for map BlocksMap
2019-04-23 19:35:15,317 INFO org.apache.hadoop.util.GSet: VM type       = 32-bit
2019-04-23 19:35:15,319 INFO org.apache.hadoop.util.GSet: 2.0% max memory 966.7 MB = 19.3 MB
2019-04-23 19:35:15,319 INFO org.apache.hadoop.util.GSet: capacity      = 2^22 = 4194304 entries
2019-04-23 19:35:15,350 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.block.access.token.enable=false
2019-04-23 19:35:15,350 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: defaultReplication         = 2
2019-04-23 19:35:15,350 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplication             = 512
2019-04-23 19:35:15,350 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: minReplication             = 1
2019-04-23 19:35:15,350 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplicationStreams      = 2
2019-04-23 19:35:15,350 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: replicationRecheckInterval = 3000
2019-04-23 19:35:15,350 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: encryptDataTransfer        = false
2019-04-23 19:35:15,350 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxNumBlocksToLog          = 1000
2019-04-23 19:35:15,352 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsOwner             = root (auth:SIMPLE)
2019-04-23 19:35:15,352 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: supergroup          = supergroup
2019-04-23 19:35:15,352 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: isPermissionEnabled = false
2019-04-23 19:35:15,352 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: HA Enabled: false
2019-04-23 19:35:15,353 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Append Enabled: true
2019-04-23 19:35:15,523 INFO org.apache.hadoop.util.GSet: Computing capacity for map INodeMap
2019-04-23 19:35:15,523 INFO org.apache.hadoop.util.GSet: VM type       = 32-bit
2019-04-23 19:35:15,523 INFO org.apache.hadoop.util.GSet: 1.0% max memory 966.7 MB = 9.7 MB
2019-04-23 19:35:15,523 INFO org.apache.hadoop.util.GSet: capacity      = 2^21 = 2097152 entries
2019-04-23 19:35:15,545 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: ACLs enabled? false
2019-04-23 19:35:15,545 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: XAttrs enabled? true
2019-04-23 19:35:15,545 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: Maximum size of an xattr: 16384
2019-04-23 19:35:15,545 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Caching file names occuring more than 10 times
2019-04-23 19:35:15,552 INFO org.apache.hadoop.util.GSet: Computing capacity for map cachedBlocks
2019-04-23 19:35:15,552 INFO org.apache.hadoop.util.GSet: VM type       = 32-bit
2019-04-23 19:35:15,552 INFO org.apache.hadoop.util.GSet: 0.25% max memory 966.7 MB = 2.4 MB
2019-04-23 19:35:15,552 INFO org.apache.hadoop.util.GSet: capacity      = 2^19 = 524288 entries
2019-04-23 19:35:15,553 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.threshold-pct = 0.9990000128746033
2019-04-23 19:35:15,553 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.min.datanodes = 0
2019-04-23 19:35:15,553 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.extension     = 30000
2019-04-23 19:35:15,557 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.window.num.buckets = 10
2019-04-23 19:35:15,557 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.num.users = 10
2019-04-23 19:35:15,557 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25
2019-04-23 19:35:15,571 INFO org.apache.hadoop.hdfs.DFSUtil: Starting Web-server for secondary at: http://0.0.0.0:50090
2019-04-23 19:35:15,637 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2019-04-23 19:35:15,643 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2019-04-23 19:35:15,650 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.secondary is not defined
2019-04-23 19:35:15,655 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2019-04-23 19:35:15,656 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context secondary
2019-04-23 19:35:15,656 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2019-04-23 19:35:15,657 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2019-04-23 19:35:15,675 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 50090
2019-04-23 19:35:15,675 INFO org.mortbay.log: jetty-6.1.26
2019-04-23 19:35:15,824 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50090
2019-04-23 19:35:15,824 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Web server init done
2019-04-23 19:35:15,836 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Checkpoint Period   :3600 secs (60 min)
2019-04-23 19:35:15,837 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Log Size Trigger    :1000000 txns
2019-04-23 19:36:16,000 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Image has changed. Downloading updated image from NN.
2019-04-23 19:36:16,154 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Opening connection to http://master:50070/imagetransfer?getimage=1&txid=0&storageInfo=-63:2027304554:0:CID-c6975e7b-5169-483f-89cf-bbcabdcf5460
2019-04-23 19:36:16,179 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Image Transfer timeout configured to 60000 milliseconds
2019-04-23 19:36:16,400 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Transfer took 0,02s at 0,00 KB/s
2019-04-23 19:36:16,400 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file fsimage.ckpt_0000000000000000000 size 351 bytes.
2019-04-23 19:36:16,409 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Opening connection to http://master:50070/imagetransfer?getedit=1&startTxId=1&endTxId=2&storageInfo=-63:2027304554:0:CID-c6975e7b-5169-483f-89cf-bbcabdcf5460
2019-04-23 19:36:16,413 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Transfer took 0,00s at 0,00 KB/s
2019-04-23 19:36:16,413 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file edits_tmp_0000000000000000001-0000000000000000002_0000000000016597639 size 0 bytes.
2019-04-23 19:36:16,457 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatPBINode: Loading 1 INodes.
2019-04-23 19:36:16,505 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: Loaded FSImage in 0 seconds.
2019-04-23 19:36:16,506 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Loaded image for txid 0 from /tmp/hadoop-root/dfs/namesecondary/current/fsimage_0000000000000000000
2019-04-23 19:36:16,506 INFO org.apache.hadoop.hdfs.server.namenode.NameCache: initialized with 0 entries 0 lookups
2019-04-23 19:36:16,511 INFO org.apache.hadoop.hdfs.server.namenode.Checkpointer: Checkpointer about to load edits from 1 stream(s).
2019-04-23 19:36:16,515 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading /tmp/hadoop-root/dfs/namesecondary/current/edits_0000000000000000001-0000000000000000002 expecting start txid #1
2019-04-23 19:36:16,515 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file /tmp/hadoop-root/dfs/namesecondary/current/edits_0000000000000000001-0000000000000000002
2019-04-23 19:36:16,532 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file /tmp/hadoop-root/dfs/namesecondary/current/edits_0000000000000000001-0000000000000000002 of size 42 edits # 2 loaded in 0 seconds
2019-04-23 19:36:16,539 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: Saving image file /tmp/hadoop-root/dfs/namesecondary/current/fsimage.ckpt_0000000000000000002 using no compression
2019-04-23 19:36:16,580 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: Image file /tmp/hadoop-root/dfs/namesecondary/current/fsimage.ckpt_0000000000000000002 of size 351 bytes saved in 0 seconds.
2019-04-23 19:36:16,584 INFO org.apache.hadoop.hdfs.server.namenode.FSImageTransactionalStorageInspector: No version file in /tmp/hadoop-root/dfs/namesecondary
2019-04-23 19:36:16,618 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Uploaded image with txid 2 to namenode at http://master:50070 in 0.026 seconds
2019-04-23 19:36:16,618 WARN org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Checkpoint done. New Image Size: 351
2019-04-23 19:44:48,567 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: RECEIVED SIGNAL 15: SIGTERM
2019-04-23 19:44:48,568 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down SecondaryNameNode at master/192.168.178.37
************************************************************/
2019-04-25 00:43:52,306 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting SecondaryNameNode
STARTUP_MSG:   host = master/192.168.178.37
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.7.3
STARTUP_MSG:   classpath = /home/hadoop-2.7.3/etc/hadoop:/home/hadoop-2.7.3/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/stax-api-1.0-2.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/activation-1.1.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/jersey-server-1.9.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/asm-3.2.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/log4j-1.2.17.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/jets3t-0.9.0.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/httpclient-4.2.5.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/httpcore-4.2.5.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/commons-lang-2.6.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/commons-configuration-1.6.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/commons-digester-1.8.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/avro-1.7.4.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/paranamer-2.3.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/commons-compress-1.4.1.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/xz-1.0.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/gson-2.2.4.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/hadoop-auth-2.7.3.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/zookeeper-3.4.6.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/netty-3.6.2.Final.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/curator-framework-2.7.1.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/curator-client-2.7.1.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/jsch-0.1.42.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/htrace-core-3.1.0-incubating.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/junit-4.11.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/hamcrest-core-1.3.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/mockito-all-1.8.5.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/hadoop-annotations-2.7.3.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/guava-11.0.2.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/jsr305-3.0.0.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/commons-cli-1.2.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/commons-math3-3.1.1.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/xmlenc-0.52.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/commons-httpclient-3.1.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/commons-logging-1.1.3.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/commons-codec-1.4.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/commons-io-2.4.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/commons-net-3.1.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/commons-collections-3.2.2.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/servlet-api-2.5.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/jetty-6.1.26.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/jetty-util-6.1.26.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/jsp-api-2.1.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/jersey-core-1.9.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/jersey-json-1.9.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/jettison-1.1.jar:/home/hadoop-2.7.3/share/hadoop/common/hadoop-common-2.7.3.jar:/home/hadoop-2.7.3/share/hadoop/common/hadoop-common-2.7.3-tests.jar:/home/hadoop-2.7.3/share/hadoop/common/hadoop-nfs-2.7.3.jar:/home/hadoop-2.7.3/share/hadoop/hdfs:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/guava-11.0.2.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-io-2.4.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/asm-3.2.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/htrace-core-3.1.0-incubating.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/hadoop-hdfs-2.7.3.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/hadoop-hdfs-2.7.3-tests.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/hadoop-hdfs-nfs-2.7.3.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/commons-lang-2.6.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/guava-11.0.2.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/commons-cli-1.2.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/log4j-1.2.17.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/activation-1.1.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/xz-1.0.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/servlet-api-2.5.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/commons-codec-1.4.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-core-1.9.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-client-1.9.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/guice-3.0.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/javax.inject-1.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/aopalliance-1.0.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/commons-io-2.4.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-server-1.9.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/asm-3.2.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-json-1.9.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/jettison-1.1.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/commons-collections-3.2.2.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/jetty-6.1.26.jar:/home/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-api-2.7.3.jar:/home/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-common-2.7.3.jar:/home/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-common-2.7.3.jar:/home/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.7.3.jar:/home/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.7.3.jar:/home/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.7.3.jar:/home/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.7.3.jar:/home/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-tests-2.7.3.jar:/home/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-client-2.7.3.jar:/home/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.7.3.jar:/home/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.3.jar:/home/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.7.3.jar:/home/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-registry-2.7.3.jar:/hpme/hadoop-2.7.3/share/hadoop/mapreduce/*:/home/hadoop-2.7.3/contrib/capacity-scheduler/*.jar:/home/hadoop-2.7.3/contrib/capacity-scheduler/*.jar:/home/hadoop-2.7.3/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r baa91f7c6bc9cb92be5982de4719c1c8af91ccff; compiled by 'root' on 2016-08-18T01:41Z
STARTUP_MSG:   java = 1.8.0_101
************************************************************/
2019-04-25 00:43:52,318 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: registered UNIX signal handlers for [TERM, HUP, INT]
2019-04-25 00:43:52,679 WARN org.apache.hadoop.util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2019-04-25 00:43:52,751 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2019-04-25 00:43:52,815 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2019-04-25 00:43:52,815 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: SecondaryNameNode metrics system started
2019-04-25 00:43:53,041 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /tmp/hadoop-root/dfs/namesecondary/in_use.lock acquired by nodename 15279@master
2019-04-25 00:43:53,077 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: No KeyProvider found.
2019-04-25 00:43:53,077 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsLock is fair:true
2019-04-25 00:43:53,115 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.block.invalidate.limit=1000
2019-04-25 00:43:53,115 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true
2019-04-25 00:43:53,116 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
2019-04-25 00:43:53,119 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: The block deletion will start around 2019 Apr 25 00:43:53
2019-04-25 00:43:53,120 INFO org.apache.hadoop.util.GSet: Computing capacity for map BlocksMap
2019-04-25 00:43:53,120 INFO org.apache.hadoop.util.GSet: VM type       = 32-bit
2019-04-25 00:43:53,121 INFO org.apache.hadoop.util.GSet: 2.0% max memory 966.7 MB = 19.3 MB
2019-04-25 00:43:53,121 INFO org.apache.hadoop.util.GSet: capacity      = 2^22 = 4194304 entries
2019-04-25 00:43:53,147 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.block.access.token.enable=false
2019-04-25 00:43:53,147 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: defaultReplication         = 2
2019-04-25 00:43:53,147 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplication             = 512
2019-04-25 00:43:53,147 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: minReplication             = 1
2019-04-25 00:43:53,147 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplicationStreams      = 2
2019-04-25 00:43:53,147 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: replicationRecheckInterval = 3000
2019-04-25 00:43:53,147 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: encryptDataTransfer        = false
2019-04-25 00:43:53,147 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxNumBlocksToLog          = 1000
2019-04-25 00:43:53,149 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsOwner             = root (auth:SIMPLE)
2019-04-25 00:43:53,149 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: supergroup          = supergroup
2019-04-25 00:43:53,149 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: isPermissionEnabled = false
2019-04-25 00:43:53,149 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: HA Enabled: false
2019-04-25 00:43:53,150 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Append Enabled: true
2019-04-25 00:43:53,304 INFO org.apache.hadoop.util.GSet: Computing capacity for map INodeMap
2019-04-25 00:43:53,304 INFO org.apache.hadoop.util.GSet: VM type       = 32-bit
2019-04-25 00:43:53,304 INFO org.apache.hadoop.util.GSet: 1.0% max memory 966.7 MB = 9.7 MB
2019-04-25 00:43:53,304 INFO org.apache.hadoop.util.GSet: capacity      = 2^21 = 2097152 entries
2019-04-25 00:43:53,322 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: ACLs enabled? false
2019-04-25 00:43:53,322 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: XAttrs enabled? true
2019-04-25 00:43:53,322 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: Maximum size of an xattr: 16384
2019-04-25 00:43:53,322 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Caching file names occuring more than 10 times
2019-04-25 00:43:53,328 INFO org.apache.hadoop.util.GSet: Computing capacity for map cachedBlocks
2019-04-25 00:43:53,328 INFO org.apache.hadoop.util.GSet: VM type       = 32-bit
2019-04-25 00:43:53,328 INFO org.apache.hadoop.util.GSet: 0.25% max memory 966.7 MB = 2.4 MB
2019-04-25 00:43:53,328 INFO org.apache.hadoop.util.GSet: capacity      = 2^19 = 524288 entries
2019-04-25 00:43:53,330 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.threshold-pct = 0.9990000128746033
2019-04-25 00:43:53,330 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.min.datanodes = 0
2019-04-25 00:43:53,330 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.extension     = 30000
2019-04-25 00:43:53,335 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.window.num.buckets = 10
2019-04-25 00:43:53,335 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.num.users = 10
2019-04-25 00:43:53,335 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25
2019-04-25 00:43:53,345 INFO org.apache.hadoop.hdfs.DFSUtil: Starting Web-server for secondary at: http://0.0.0.0:50090
2019-04-25 00:43:53,411 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2019-04-25 00:43:53,417 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2019-04-25 00:43:53,421 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.secondary is not defined
2019-04-25 00:43:53,426 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2019-04-25 00:43:53,427 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context secondary
2019-04-25 00:43:53,427 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2019-04-25 00:43:53,427 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2019-04-25 00:43:53,450 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 50090
2019-04-25 00:43:53,450 INFO org.mortbay.log: jetty-6.1.26
2019-04-25 00:43:53,563 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50090
2019-04-25 00:43:53,564 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Web server init done
2019-04-25 00:43:53,567 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Checkpoint Period   :3600 secs (60 min)
2019-04-25 00:43:53,567 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Log Size Trigger    :1000000 txns
2019-04-25 00:44:42,047 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: RECEIVED SIGNAL 15: SIGTERM
2019-04-25 00:44:42,048 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down SecondaryNameNode at master/192.168.178.37
************************************************************/
2019-04-25 00:48:47,525 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting SecondaryNameNode
STARTUP_MSG:   host = master/192.168.178.37
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.7.3
STARTUP_MSG:   classpath = /home/hadoop-2.7.3/etc/hadoop:/home/hadoop-2.7.3/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/stax-api-1.0-2.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/activation-1.1.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/jersey-server-1.9.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/asm-3.2.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/log4j-1.2.17.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/jets3t-0.9.0.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/httpclient-4.2.5.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/httpcore-4.2.5.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/commons-lang-2.6.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/commons-configuration-1.6.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/commons-digester-1.8.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/avro-1.7.4.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/paranamer-2.3.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/commons-compress-1.4.1.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/xz-1.0.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/gson-2.2.4.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/hadoop-auth-2.7.3.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/zookeeper-3.4.6.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/netty-3.6.2.Final.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/curator-framework-2.7.1.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/curator-client-2.7.1.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/jsch-0.1.42.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/htrace-core-3.1.0-incubating.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/junit-4.11.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/hamcrest-core-1.3.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/mockito-all-1.8.5.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/hadoop-annotations-2.7.3.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/guava-11.0.2.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/jsr305-3.0.0.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/commons-cli-1.2.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/commons-math3-3.1.1.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/xmlenc-0.52.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/commons-httpclient-3.1.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/commons-logging-1.1.3.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/commons-codec-1.4.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/commons-io-2.4.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/commons-net-3.1.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/commons-collections-3.2.2.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/servlet-api-2.5.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/jetty-6.1.26.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/jetty-util-6.1.26.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/jsp-api-2.1.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/jersey-core-1.9.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/jersey-json-1.9.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/jettison-1.1.jar:/home/hadoop-2.7.3/share/hadoop/common/hadoop-common-2.7.3.jar:/home/hadoop-2.7.3/share/hadoop/common/hadoop-common-2.7.3-tests.jar:/home/hadoop-2.7.3/share/hadoop/common/hadoop-nfs-2.7.3.jar:/home/hadoop-2.7.3/share/hadoop/hdfs:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/guava-11.0.2.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-io-2.4.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/asm-3.2.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/htrace-core-3.1.0-incubating.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/hadoop-hdfs-2.7.3.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/hadoop-hdfs-2.7.3-tests.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/hadoop-hdfs-nfs-2.7.3.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/commons-lang-2.6.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/guava-11.0.2.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/commons-cli-1.2.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/log4j-1.2.17.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/activation-1.1.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/xz-1.0.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/servlet-api-2.5.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/commons-codec-1.4.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-core-1.9.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-client-1.9.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/guice-3.0.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/javax.inject-1.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/aopalliance-1.0.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/commons-io-2.4.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-server-1.9.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/asm-3.2.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-json-1.9.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/jettison-1.1.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/commons-collections-3.2.2.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/jetty-6.1.26.jar:/home/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-api-2.7.3.jar:/home/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-common-2.7.3.jar:/home/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-common-2.7.3.jar:/home/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.7.3.jar:/home/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.7.3.jar:/home/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.7.3.jar:/home/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.7.3.jar:/home/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-tests-2.7.3.jar:/home/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-client-2.7.3.jar:/home/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.7.3.jar:/home/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.3.jar:/home/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.7.3.jar:/home/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-registry-2.7.3.jar:/hpme/hadoop-2.7.3/share/hadoop/mapreduce/*:/home/hadoop-2.7.3/contrib/capacity-scheduler/*.jar:/home/hadoop-2.7.3/contrib/capacity-scheduler/*.jar:/home/hadoop-2.7.3/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r baa91f7c6bc9cb92be5982de4719c1c8af91ccff; compiled by 'root' on 2016-08-18T01:41Z
STARTUP_MSG:   java = 1.8.0_101
************************************************************/
2019-04-25 00:48:47,537 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: registered UNIX signal handlers for [TERM, HUP, INT]
2019-04-25 00:48:47,897 WARN org.apache.hadoop.util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2019-04-25 00:48:47,972 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2019-04-25 00:48:48,040 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2019-04-25 00:48:48,040 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: SecondaryNameNode metrics system started
2019-04-25 00:48:48,254 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /tmp/hadoop-root/dfs/namesecondary/in_use.lock acquired by nodename 16261@master
2019-04-25 00:48:48,290 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: No KeyProvider found.
2019-04-25 00:48:48,291 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsLock is fair:true
2019-04-25 00:48:48,330 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.block.invalidate.limit=1000
2019-04-25 00:48:48,330 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true
2019-04-25 00:48:48,330 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
2019-04-25 00:48:48,333 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: The block deletion will start around 2019 Apr 25 00:48:48
2019-04-25 00:48:48,334 INFO org.apache.hadoop.util.GSet: Computing capacity for map BlocksMap
2019-04-25 00:48:48,334 INFO org.apache.hadoop.util.GSet: VM type       = 32-bit
2019-04-25 00:48:48,336 INFO org.apache.hadoop.util.GSet: 2.0% max memory 966.7 MB = 19.3 MB
2019-04-25 00:48:48,336 INFO org.apache.hadoop.util.GSet: capacity      = 2^22 = 4194304 entries
2019-04-25 00:48:48,361 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.block.access.token.enable=false
2019-04-25 00:48:48,361 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: defaultReplication         = 2
2019-04-25 00:48:48,361 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplication             = 512
2019-04-25 00:48:48,361 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: minReplication             = 1
2019-04-25 00:48:48,361 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplicationStreams      = 2
2019-04-25 00:48:48,361 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: replicationRecheckInterval = 3000
2019-04-25 00:48:48,361 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: encryptDataTransfer        = false
2019-04-25 00:48:48,361 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxNumBlocksToLog          = 1000
2019-04-25 00:48:48,363 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsOwner             = root (auth:SIMPLE)
2019-04-25 00:48:48,363 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: supergroup          = supergroup
2019-04-25 00:48:48,363 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: isPermissionEnabled = false
2019-04-25 00:48:48,363 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: HA Enabled: false
2019-04-25 00:48:48,365 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Append Enabled: true
2019-04-25 00:48:48,516 INFO org.apache.hadoop.util.GSet: Computing capacity for map INodeMap
2019-04-25 00:48:48,516 INFO org.apache.hadoop.util.GSet: VM type       = 32-bit
2019-04-25 00:48:48,516 INFO org.apache.hadoop.util.GSet: 1.0% max memory 966.7 MB = 9.7 MB
2019-04-25 00:48:48,516 INFO org.apache.hadoop.util.GSet: capacity      = 2^21 = 2097152 entries
2019-04-25 00:48:48,536 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: ACLs enabled? false
2019-04-25 00:48:48,536 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: XAttrs enabled? true
2019-04-25 00:48:48,536 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: Maximum size of an xattr: 16384
2019-04-25 00:48:48,536 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Caching file names occuring more than 10 times
2019-04-25 00:48:48,542 INFO org.apache.hadoop.util.GSet: Computing capacity for map cachedBlocks
2019-04-25 00:48:48,542 INFO org.apache.hadoop.util.GSet: VM type       = 32-bit
2019-04-25 00:48:48,542 INFO org.apache.hadoop.util.GSet: 0.25% max memory 966.7 MB = 2.4 MB
2019-04-25 00:48:48,542 INFO org.apache.hadoop.util.GSet: capacity      = 2^19 = 524288 entries
2019-04-25 00:48:48,544 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.threshold-pct = 0.9990000128746033
2019-04-25 00:48:48,544 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.min.datanodes = 0
2019-04-25 00:48:48,544 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.extension     = 30000
2019-04-25 00:48:48,548 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.window.num.buckets = 10
2019-04-25 00:48:48,548 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.num.users = 10
2019-04-25 00:48:48,548 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25
2019-04-25 00:48:48,557 INFO org.apache.hadoop.hdfs.DFSUtil: Starting Web-server for secondary at: http://0.0.0.0:50090
2019-04-25 00:48:48,624 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2019-04-25 00:48:48,631 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2019-04-25 00:48:48,635 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.secondary is not defined
2019-04-25 00:48:48,640 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2019-04-25 00:48:48,643 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context secondary
2019-04-25 00:48:48,643 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2019-04-25 00:48:48,643 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2019-04-25 00:48:48,666 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 50090
2019-04-25 00:48:48,667 INFO org.mortbay.log: jetty-6.1.26
2019-04-25 00:48:48,782 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50090
2019-04-25 00:48:48,782 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Web server init done
2019-04-25 00:48:48,786 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Checkpoint Period   :3600 secs (60 min)
2019-04-25 00:48:48,786 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Log Size Trigger    :1000000 txns
2019-04-25 01:03:49,926 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master/192.168.178.37:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-04-25 01:03:50,928 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master/192.168.178.37:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-04-25 01:03:51,931 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master/192.168.178.37:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-04-25 01:03:52,932 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master/192.168.178.37:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-04-25 01:03:53,933 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master/192.168.178.37:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-04-25 01:03:54,933 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master/192.168.178.37:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-04-25 01:03:55,934 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master/192.168.178.37:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-04-25 01:03:56,935 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master/192.168.178.37:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-04-25 01:03:57,936 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master/192.168.178.37:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-04-25 01:03:58,951 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master/192.168.178.37:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-04-25 01:03:58,954 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.net.ConnectException: Call From master/192.168.178.37 to master:9000 failed on connection exception: java.net.ConnectException: Verbindungsaufbau abgelehnt; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1479)
	at org.apache.hadoop.ipc.Client.call(Client.java:1412)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy10.getTransactionId(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolTranslatorPB.getTransactionID(NamenodeProtocolTranslatorPB.java:128)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:191)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy11.getTransactionID(Unknown Source)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.countUncheckpointedTxns(SecondaryNameNode.java:641)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.shouldCheckpointBasedOnCount(SecondaryNameNode.java:649)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:393)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Verbindungsaufbau abgelehnt
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:614)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:712)
	at org.apache.hadoop.ipc.Client$Connection.access$2900(Client.java:375)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1528)
	at org.apache.hadoop.ipc.Client.call(Client.java:1451)
	... 18 more
2019-04-25 01:04:11,498 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: RECEIVED SIGNAL 15: SIGTERM
2019-04-25 01:04:11,500 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down SecondaryNameNode at master/192.168.178.37
************************************************************/
2019-04-25 01:07:15,009 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting SecondaryNameNode
STARTUP_MSG:   host = master/192.168.178.37
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.7.3
STARTUP_MSG:   classpath = /home/hadoop-2.7.3/etc/hadoop:/home/hadoop-2.7.3/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/stax-api-1.0-2.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/activation-1.1.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/jersey-server-1.9.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/asm-3.2.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/log4j-1.2.17.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/jets3t-0.9.0.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/httpclient-4.2.5.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/httpcore-4.2.5.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/commons-lang-2.6.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/commons-configuration-1.6.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/commons-digester-1.8.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/avro-1.7.4.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/paranamer-2.3.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/commons-compress-1.4.1.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/xz-1.0.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/gson-2.2.4.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/hadoop-auth-2.7.3.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/zookeeper-3.4.6.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/netty-3.6.2.Final.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/curator-framework-2.7.1.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/curator-client-2.7.1.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/jsch-0.1.42.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/htrace-core-3.1.0-incubating.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/junit-4.11.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/hamcrest-core-1.3.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/mockito-all-1.8.5.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/hadoop-annotations-2.7.3.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/guava-11.0.2.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/jsr305-3.0.0.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/commons-cli-1.2.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/commons-math3-3.1.1.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/xmlenc-0.52.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/commons-httpclient-3.1.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/commons-logging-1.1.3.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/commons-codec-1.4.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/commons-io-2.4.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/commons-net-3.1.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/commons-collections-3.2.2.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/servlet-api-2.5.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/jetty-6.1.26.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/jetty-util-6.1.26.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/jsp-api-2.1.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/jersey-core-1.9.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/jersey-json-1.9.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/jettison-1.1.jar:/home/hadoop-2.7.3/share/hadoop/common/hadoop-common-2.7.3.jar:/home/hadoop-2.7.3/share/hadoop/common/hadoop-common-2.7.3-tests.jar:/home/hadoop-2.7.3/share/hadoop/common/hadoop-nfs-2.7.3.jar:/home/hadoop-2.7.3/share/hadoop/hdfs:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/guava-11.0.2.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-io-2.4.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/asm-3.2.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/htrace-core-3.1.0-incubating.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/hadoop-hdfs-2.7.3.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/hadoop-hdfs-2.7.3-tests.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/hadoop-hdfs-nfs-2.7.3.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/commons-lang-2.6.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/guava-11.0.2.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/commons-cli-1.2.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/log4j-1.2.17.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/activation-1.1.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/xz-1.0.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/servlet-api-2.5.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/commons-codec-1.4.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-core-1.9.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-client-1.9.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/guice-3.0.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/javax.inject-1.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/aopalliance-1.0.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/commons-io-2.4.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-server-1.9.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/asm-3.2.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-json-1.9.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/jettison-1.1.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/commons-collections-3.2.2.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/jetty-6.1.26.jar:/home/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-api-2.7.3.jar:/home/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-common-2.7.3.jar:/home/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-common-2.7.3.jar:/home/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.7.3.jar:/home/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.7.3.jar:/home/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.7.3.jar:/home/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.7.3.jar:/home/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-tests-2.7.3.jar:/home/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-client-2.7.3.jar:/home/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.7.3.jar:/home/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.3.jar:/home/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.7.3.jar:/home/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-registry-2.7.3.jar:/hpme/hadoop-2.7.3/share/hadoop/mapreduce/*:/home/hadoop-2.7.3/contrib/capacity-scheduler/*.jar:/home/hadoop-2.7.3/contrib/capacity-scheduler/*.jar:/home/hadoop-2.7.3/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r baa91f7c6bc9cb92be5982de4719c1c8af91ccff; compiled by 'root' on 2016-08-18T01:41Z
STARTUP_MSG:   java = 1.8.0_101
************************************************************/
2019-04-25 01:07:15,021 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: registered UNIX signal handlers for [TERM, HUP, INT]
2019-04-25 01:07:15,376 WARN org.apache.hadoop.util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2019-04-25 01:07:15,447 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2019-04-25 01:07:15,522 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2019-04-25 01:07:15,522 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: SecondaryNameNode metrics system started
2019-04-25 01:07:15,735 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /tmp/hadoop-root/dfs/namesecondary/in_use.lock acquired by nodename 17534@master
2019-04-25 01:07:15,772 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: No KeyProvider found.
2019-04-25 01:07:15,772 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsLock is fair:true
2019-04-25 01:07:15,811 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.block.invalidate.limit=1000
2019-04-25 01:07:15,811 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true
2019-04-25 01:07:15,812 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
2019-04-25 01:07:15,814 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: The block deletion will start around 2019 Apr 25 01:07:15
2019-04-25 01:07:15,816 INFO org.apache.hadoop.util.GSet: Computing capacity for map BlocksMap
2019-04-25 01:07:15,816 INFO org.apache.hadoop.util.GSet: VM type       = 32-bit
2019-04-25 01:07:15,817 INFO org.apache.hadoop.util.GSet: 2.0% max memory 966.7 MB = 19.3 MB
2019-04-25 01:07:15,817 INFO org.apache.hadoop.util.GSet: capacity      = 2^22 = 4194304 entries
2019-04-25 01:07:15,841 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.block.access.token.enable=false
2019-04-25 01:07:15,841 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: defaultReplication         = 2
2019-04-25 01:07:15,841 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplication             = 512
2019-04-25 01:07:15,841 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: minReplication             = 1
2019-04-25 01:07:15,841 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplicationStreams      = 2
2019-04-25 01:07:15,841 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: replicationRecheckInterval = 3000
2019-04-25 01:07:15,841 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: encryptDataTransfer        = false
2019-04-25 01:07:15,841 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxNumBlocksToLog          = 1000
2019-04-25 01:07:15,843 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsOwner             = root (auth:SIMPLE)
2019-04-25 01:07:15,843 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: supergroup          = supergroup
2019-04-25 01:07:15,843 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: isPermissionEnabled = false
2019-04-25 01:07:15,843 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: HA Enabled: false
2019-04-25 01:07:15,845 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Append Enabled: true
2019-04-25 01:07:15,993 INFO org.apache.hadoop.util.GSet: Computing capacity for map INodeMap
2019-04-25 01:07:15,993 INFO org.apache.hadoop.util.GSet: VM type       = 32-bit
2019-04-25 01:07:15,993 INFO org.apache.hadoop.util.GSet: 1.0% max memory 966.7 MB = 9.7 MB
2019-04-25 01:07:15,993 INFO org.apache.hadoop.util.GSet: capacity      = 2^21 = 2097152 entries
2019-04-25 01:07:16,012 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: ACLs enabled? false
2019-04-25 01:07:16,012 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: XAttrs enabled? true
2019-04-25 01:07:16,012 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: Maximum size of an xattr: 16384
2019-04-25 01:07:16,012 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Caching file names occuring more than 10 times
2019-04-25 01:07:16,018 INFO org.apache.hadoop.util.GSet: Computing capacity for map cachedBlocks
2019-04-25 01:07:16,018 INFO org.apache.hadoop.util.GSet: VM type       = 32-bit
2019-04-25 01:07:16,018 INFO org.apache.hadoop.util.GSet: 0.25% max memory 966.7 MB = 2.4 MB
2019-04-25 01:07:16,018 INFO org.apache.hadoop.util.GSet: capacity      = 2^19 = 524288 entries
2019-04-25 01:07:16,020 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.threshold-pct = 0.9990000128746033
2019-04-25 01:07:16,020 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.min.datanodes = 0
2019-04-25 01:07:16,020 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.extension     = 30000
2019-04-25 01:07:16,024 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.window.num.buckets = 10
2019-04-25 01:07:16,024 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.num.users = 10
2019-04-25 01:07:16,024 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25
2019-04-25 01:07:16,034 INFO org.apache.hadoop.hdfs.DFSUtil: Starting Web-server for secondary at: http://0.0.0.0:50090
2019-04-25 01:07:16,102 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2019-04-25 01:07:16,108 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2019-04-25 01:07:16,112 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.secondary is not defined
2019-04-25 01:07:16,117 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2019-04-25 01:07:16,118 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context secondary
2019-04-25 01:07:16,118 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2019-04-25 01:07:16,118 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2019-04-25 01:07:16,142 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 50090
2019-04-25 01:07:16,142 INFO org.mortbay.log: jetty-6.1.26
2019-04-25 01:07:16,258 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50090
2019-04-25 01:07:16,258 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Web server init done
2019-04-25 01:07:16,264 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Checkpoint Period   :3600 secs (60 min)
2019-04-25 01:07:16,264 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Log Size Trigger    :1000000 txns
2019-04-25 01:40:16,746 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Image has changed. Downloading updated image from NN.
2019-04-25 01:40:16,851 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Opening connection to http://master:50070/imagetransfer?getimage=1&txid=3&storageInfo=-63:2027304554:0:CID-c6975e7b-5169-483f-89cf-bbcabdcf5460
2019-04-25 01:40:16,866 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Image Transfer timeout configured to 60000 milliseconds
2019-04-25 01:40:16,991 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Transfer took 0,00s at 0,00 KB/s
2019-04-25 01:40:16,991 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file fsimage.ckpt_0000000000000000003 size 351 bytes.
2019-04-25 01:40:16,997 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Opening connection to http://master:50070/imagetransfer?getedit=1&startTxId=4&endTxId=4&storageInfo=-63:2027304554:0:CID-c6975e7b-5169-483f-89cf-bbcabdcf5460
2019-04-25 01:40:17,010 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Transfer took 0,01s at 102400,00 KB/s
2019-04-25 01:40:17,010 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file edits_tmp_0000000000000000004-0000000000000000004_0000000000003635483 size 0 bytes.
2019-04-25 01:40:17,011 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Opening connection to http://master:50070/imagetransfer?getedit=1&startTxId=5&endTxId=5&storageInfo=-63:2027304554:0:CID-c6975e7b-5169-483f-89cf-bbcabdcf5460
2019-04-25 01:40:17,020 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Transfer took 0,01s at 204800,00 KB/s
2019-04-25 01:40:17,020 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file edits_tmp_0000000000000000005-0000000000000000005_0000000000003635497 size 0 bytes.
2019-04-25 01:40:17,020 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Opening connection to http://master:50070/imagetransfer?getedit=1&startTxId=6&endTxId=7&storageInfo=-63:2027304554:0:CID-c6975e7b-5169-483f-89cf-bbcabdcf5460
2019-04-25 01:40:17,022 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Transfer took 0,00s at 0,00 KB/s
2019-04-25 01:40:17,022 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file edits_tmp_0000000000000000006-0000000000000000007_0000000000003635506 size 0 bytes.
2019-04-25 01:40:17,062 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatPBINode: Loading 1 INodes.
2019-04-25 01:40:17,086 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: Loaded FSImage in 0 seconds.
2019-04-25 01:40:17,086 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Loaded image for txid 3 from /tmp/hadoop-root/dfs/namesecondary/current/fsimage_0000000000000000003
2019-04-25 01:40:17,086 INFO org.apache.hadoop.hdfs.server.namenode.NameCache: initialized with 0 entries 0 lookups
2019-04-25 01:40:17,090 INFO org.apache.hadoop.hdfs.server.namenode.Checkpointer: Checkpointer about to load edits from 3 stream(s).
2019-04-25 01:40:17,094 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading /tmp/hadoop-root/dfs/namesecondary/current/edits_0000000000000000004-0000000000000000004 expecting start txid #4
2019-04-25 01:40:17,094 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file /tmp/hadoop-root/dfs/namesecondary/current/edits_0000000000000000004-0000000000000000004
2019-04-25 01:40:17,112 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file /tmp/hadoop-root/dfs/namesecondary/current/edits_0000000000000000004-0000000000000000004 of size 1048576 edits # 1 loaded in 0 seconds
2019-04-25 01:40:17,112 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading /tmp/hadoop-root/dfs/namesecondary/current/edits_0000000000000000005-0000000000000000005 expecting start txid #5
2019-04-25 01:40:17,112 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file /tmp/hadoop-root/dfs/namesecondary/current/edits_0000000000000000005-0000000000000000005
2019-04-25 01:40:17,122 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file /tmp/hadoop-root/dfs/namesecondary/current/edits_0000000000000000005-0000000000000000005 of size 1048576 edits # 1 loaded in 0 seconds
2019-04-25 01:40:17,122 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading /tmp/hadoop-root/dfs/namesecondary/current/edits_0000000000000000006-0000000000000000007 expecting start txid #6
2019-04-25 01:40:17,122 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file /tmp/hadoop-root/dfs/namesecondary/current/edits_0000000000000000006-0000000000000000007
2019-04-25 01:40:17,122 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file /tmp/hadoop-root/dfs/namesecondary/current/edits_0000000000000000006-0000000000000000007 of size 42 edits # 2 loaded in 0 seconds
2019-04-25 01:40:17,128 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: Saving image file /tmp/hadoop-root/dfs/namesecondary/current/fsimage.ckpt_0000000000000000007 using no compression
2019-04-25 01:40:17,161 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: Image file /tmp/hadoop-root/dfs/namesecondary/current/fsimage.ckpt_0000000000000000007 of size 351 bytes saved in 0 seconds.
2019-04-25 01:40:17,166 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Going to retain 2 images with txid >= 3
2019-04-25 01:40:17,166 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Purging old image FSImageFile(file=/tmp/hadoop-root/dfs/namesecondary/current/fsimage_0000000000000000000, cpktTxId=0000000000000000000)
2019-04-25 01:40:17,166 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Purging old image FSImageFile(file=/tmp/hadoop-root/dfs/namesecondary/current/fsimage_0000000000000000002, cpktTxId=0000000000000000002)
2019-04-25 01:40:17,190 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Uploaded image with txid 7 to namenode at http://master:50070 in 0.016 seconds
2019-04-25 01:40:17,190 WARN org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Checkpoint done. New Image Size: 351
2019-04-25 02:40:17,610 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Image has not changed. Will not download image.
2019-04-25 02:40:17,610 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Opening connection to http://master:50070/imagetransfer?getedit=1&startTxId=8&endTxId=9&storageInfo=-63:2027304554:0:CID-c6975e7b-5169-483f-89cf-bbcabdcf5460
2019-04-25 02:40:17,616 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Transfer took 0,00s at 0,00 KB/s
2019-04-25 02:40:17,616 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file edits_tmp_0000000000000000008-0000000000000000009_0000000000007236096 size 0 bytes.
2019-04-25 02:40:17,616 INFO org.apache.hadoop.hdfs.server.namenode.Checkpointer: Checkpointer about to load edits from 1 stream(s).
2019-04-25 02:40:17,616 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading /tmp/hadoop-root/dfs/namesecondary/current/edits_0000000000000000008-0000000000000000009 expecting start txid #8
2019-04-25 02:40:17,616 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file /tmp/hadoop-root/dfs/namesecondary/current/edits_0000000000000000008-0000000000000000009
2019-04-25 02:40:17,617 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file /tmp/hadoop-root/dfs/namesecondary/current/edits_0000000000000000008-0000000000000000009 of size 42 edits # 2 loaded in 0 seconds
2019-04-25 02:40:17,617 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: Saving image file /tmp/hadoop-root/dfs/namesecondary/current/fsimage.ckpt_0000000000000000009 using no compression
2019-04-25 02:40:17,633 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: Image file /tmp/hadoop-root/dfs/namesecondary/current/fsimage.ckpt_0000000000000000009 of size 351 bytes saved in 0 seconds.
2019-04-25 02:40:17,635 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Going to retain 2 images with txid >= 7
2019-04-25 02:40:17,635 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Purging old image FSImageFile(file=/tmp/hadoop-root/dfs/namesecondary/current/fsimage_0000000000000000003, cpktTxId=0000000000000000003)
2019-04-25 02:40:17,647 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Uploaded image with txid 9 to namenode at http://master:50070 in 0.009 seconds
2019-04-25 02:40:17,647 WARN org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Checkpoint done. New Image Size: 351
2019-04-25 02:45:08,957 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: RECEIVED SIGNAL 15: SIGTERM
2019-04-25 02:45:08,958 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down SecondaryNameNode at master/192.168.178.37
************************************************************/
2019-04-27 00:57:08,629 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting SecondaryNameNode
STARTUP_MSG:   host = master/192.168.178.37
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.7.3
STARTUP_MSG:   classpath = /home/hadoop-2.7.3/etc/hadoop:/home/hadoop-2.7.3/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/stax-api-1.0-2.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/activation-1.1.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/jersey-server-1.9.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/asm-3.2.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/log4j-1.2.17.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/jets3t-0.9.0.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/httpclient-4.2.5.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/httpcore-4.2.5.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/commons-lang-2.6.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/commons-configuration-1.6.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/commons-digester-1.8.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/avro-1.7.4.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/paranamer-2.3.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/commons-compress-1.4.1.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/xz-1.0.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/gson-2.2.4.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/hadoop-auth-2.7.3.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/zookeeper-3.4.6.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/netty-3.6.2.Final.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/curator-framework-2.7.1.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/curator-client-2.7.1.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/jsch-0.1.42.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/htrace-core-3.1.0-incubating.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/junit-4.11.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/hamcrest-core-1.3.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/mockito-all-1.8.5.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/hadoop-annotations-2.7.3.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/guava-11.0.2.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/jsr305-3.0.0.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/commons-cli-1.2.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/commons-math3-3.1.1.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/xmlenc-0.52.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/commons-httpclient-3.1.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/commons-logging-1.1.3.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/commons-codec-1.4.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/commons-io-2.4.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/commons-net-3.1.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/commons-collections-3.2.2.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/servlet-api-2.5.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/jetty-6.1.26.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/jetty-util-6.1.26.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/jsp-api-2.1.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/jersey-core-1.9.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/jersey-json-1.9.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/jettison-1.1.jar:/home/hadoop-2.7.3/share/hadoop/common/hadoop-common-2.7.3.jar:/home/hadoop-2.7.3/share/hadoop/common/hadoop-common-2.7.3-tests.jar:/home/hadoop-2.7.3/share/hadoop/common/hadoop-nfs-2.7.3.jar:/home/hadoop-2.7.3/share/hadoop/hdfs:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/guava-11.0.2.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-io-2.4.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/asm-3.2.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/htrace-core-3.1.0-incubating.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/hadoop-hdfs-2.7.3.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/hadoop-hdfs-2.7.3-tests.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/hadoop-hdfs-nfs-2.7.3.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/commons-lang-2.6.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/guava-11.0.2.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/commons-cli-1.2.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/log4j-1.2.17.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/activation-1.1.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/xz-1.0.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/servlet-api-2.5.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/commons-codec-1.4.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-core-1.9.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-client-1.9.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/guice-3.0.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/javax.inject-1.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/aopalliance-1.0.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/commons-io-2.4.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-server-1.9.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/asm-3.2.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-json-1.9.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/jettison-1.1.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/commons-collections-3.2.2.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/jetty-6.1.26.jar:/home/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-api-2.7.3.jar:/home/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-common-2.7.3.jar:/home/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-common-2.7.3.jar:/home/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.7.3.jar:/home/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.7.3.jar:/home/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.7.3.jar:/home/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.7.3.jar:/home/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-tests-2.7.3.jar:/home/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-client-2.7.3.jar:/home/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.7.3.jar:/home/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.3.jar:/home/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.7.3.jar:/home/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-registry-2.7.3.jar:/hpme/hadoop-2.7.3/share/hadoop/mapreduce/*:/home/hadoop-2.7.3/contrib/capacity-scheduler/*.jar:/home/hadoop-2.7.3/contrib/capacity-scheduler/*.jar:/home/hadoop-2.7.3/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r baa91f7c6bc9cb92be5982de4719c1c8af91ccff; compiled by 'root' on 2016-08-18T01:41Z
STARTUP_MSG:   java = 1.8.0_101
************************************************************/
2019-04-27 00:57:08,641 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: registered UNIX signal handlers for [TERM, HUP, INT]
2019-04-27 00:57:09,006 WARN org.apache.hadoop.util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2019-04-27 00:57:09,077 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2019-04-27 00:57:09,146 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2019-04-27 00:57:09,146 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: SecondaryNameNode metrics system started
2019-04-27 00:57:09,384 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /tmp/hadoop-root/dfs/namesecondary/in_use.lock acquired by nodename 16101@master
2019-04-27 00:57:09,420 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: No KeyProvider found.
2019-04-27 00:57:09,420 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsLock is fair:true
2019-04-27 00:57:09,460 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.block.invalidate.limit=1000
2019-04-27 00:57:09,460 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true
2019-04-27 00:57:09,460 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
2019-04-27 00:57:09,463 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: The block deletion will start around 2019 Apr 27 00:57:09
2019-04-27 00:57:09,465 INFO org.apache.hadoop.util.GSet: Computing capacity for map BlocksMap
2019-04-27 00:57:09,465 INFO org.apache.hadoop.util.GSet: VM type       = 32-bit
2019-04-27 00:57:09,466 INFO org.apache.hadoop.util.GSet: 2.0% max memory 966.7 MB = 19.3 MB
2019-04-27 00:57:09,466 INFO org.apache.hadoop.util.GSet: capacity      = 2^22 = 4194304 entries
2019-04-27 00:57:09,499 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.block.access.token.enable=false
2019-04-27 00:57:09,499 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: defaultReplication         = 2
2019-04-27 00:57:09,499 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplication             = 512
2019-04-27 00:57:09,499 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: minReplication             = 1
2019-04-27 00:57:09,499 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplicationStreams      = 2
2019-04-27 00:57:09,499 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: replicationRecheckInterval = 3000
2019-04-27 00:57:09,499 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: encryptDataTransfer        = false
2019-04-27 00:57:09,499 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxNumBlocksToLog          = 1000
2019-04-27 00:57:09,501 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsOwner             = root (auth:SIMPLE)
2019-04-27 00:57:09,501 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: supergroup          = supergroup
2019-04-27 00:57:09,501 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: isPermissionEnabled = false
2019-04-27 00:57:09,501 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: HA Enabled: false
2019-04-27 00:57:09,504 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Append Enabled: true
2019-04-27 00:57:09,658 INFO org.apache.hadoop.util.GSet: Computing capacity for map INodeMap
2019-04-27 00:57:09,658 INFO org.apache.hadoop.util.GSet: VM type       = 32-bit
2019-04-27 00:57:09,658 INFO org.apache.hadoop.util.GSet: 1.0% max memory 966.7 MB = 9.7 MB
2019-04-27 00:57:09,659 INFO org.apache.hadoop.util.GSet: capacity      = 2^21 = 2097152 entries
2019-04-27 00:57:09,680 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: ACLs enabled? false
2019-04-27 00:57:09,680 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: XAttrs enabled? true
2019-04-27 00:57:09,680 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: Maximum size of an xattr: 16384
2019-04-27 00:57:09,680 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Caching file names occuring more than 10 times
2019-04-27 00:57:09,686 INFO org.apache.hadoop.util.GSet: Computing capacity for map cachedBlocks
2019-04-27 00:57:09,686 INFO org.apache.hadoop.util.GSet: VM type       = 32-bit
2019-04-27 00:57:09,686 INFO org.apache.hadoop.util.GSet: 0.25% max memory 966.7 MB = 2.4 MB
2019-04-27 00:57:09,686 INFO org.apache.hadoop.util.GSet: capacity      = 2^19 = 524288 entries
2019-04-27 00:57:09,691 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.threshold-pct = 0.9990000128746033
2019-04-27 00:57:09,691 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.min.datanodes = 0
2019-04-27 00:57:09,691 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.extension     = 30000
2019-04-27 00:57:09,694 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.window.num.buckets = 10
2019-04-27 00:57:09,694 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.num.users = 10
2019-04-27 00:57:09,694 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25
2019-04-27 00:57:09,706 INFO org.apache.hadoop.hdfs.DFSUtil: Starting Web-server for secondary at: http://0.0.0.0:50090
2019-04-27 00:57:09,786 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2019-04-27 00:57:09,792 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2019-04-27 00:57:09,796 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.secondary is not defined
2019-04-27 00:57:09,801 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2019-04-27 00:57:09,802 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context secondary
2019-04-27 00:57:09,802 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2019-04-27 00:57:09,803 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2019-04-27 00:57:09,823 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 50090
2019-04-27 00:57:09,823 INFO org.mortbay.log: jetty-6.1.26
2019-04-27 00:57:09,945 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50090
2019-04-27 00:57:09,945 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Web server init done
2019-04-27 00:57:09,948 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Checkpoint Period   :3600 secs (60 min)
2019-04-27 00:57:09,948 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Log Size Trigger    :1000000 txns
2019-04-27 00:59:08,377 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: RECEIVED SIGNAL 15: SIGTERM
2019-04-27 00:59:08,378 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down SecondaryNameNode at master/192.168.178.37
************************************************************/
2019-04-27 01:01:38,228 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting SecondaryNameNode
STARTUP_MSG:   host = master/192.168.178.37
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.7.3
STARTUP_MSG:   classpath = /home/hadoop-2.7.3/etc/hadoop:/home/hadoop-2.7.3/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/stax-api-1.0-2.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/activation-1.1.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/jersey-server-1.9.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/asm-3.2.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/log4j-1.2.17.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/jets3t-0.9.0.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/httpclient-4.2.5.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/httpcore-4.2.5.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/commons-lang-2.6.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/commons-configuration-1.6.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/commons-digester-1.8.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/avro-1.7.4.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/paranamer-2.3.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/commons-compress-1.4.1.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/xz-1.0.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/gson-2.2.4.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/hadoop-auth-2.7.3.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/zookeeper-3.4.6.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/netty-3.6.2.Final.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/curator-framework-2.7.1.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/curator-client-2.7.1.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/jsch-0.1.42.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/htrace-core-3.1.0-incubating.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/junit-4.11.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/hamcrest-core-1.3.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/mockito-all-1.8.5.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/hadoop-annotations-2.7.3.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/guava-11.0.2.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/jsr305-3.0.0.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/commons-cli-1.2.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/commons-math3-3.1.1.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/xmlenc-0.52.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/commons-httpclient-3.1.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/commons-logging-1.1.3.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/commons-codec-1.4.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/commons-io-2.4.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/commons-net-3.1.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/commons-collections-3.2.2.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/servlet-api-2.5.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/jetty-6.1.26.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/jetty-util-6.1.26.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/jsp-api-2.1.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/jersey-core-1.9.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/jersey-json-1.9.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/jettison-1.1.jar:/home/hadoop-2.7.3/share/hadoop/common/hadoop-common-2.7.3.jar:/home/hadoop-2.7.3/share/hadoop/common/hadoop-common-2.7.3-tests.jar:/home/hadoop-2.7.3/share/hadoop/common/hadoop-nfs-2.7.3.jar:/home/hadoop-2.7.3/share/hadoop/hdfs:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/guava-11.0.2.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-io-2.4.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/asm-3.2.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/htrace-core-3.1.0-incubating.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/hadoop-hdfs-2.7.3.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/hadoop-hdfs-2.7.3-tests.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/hadoop-hdfs-nfs-2.7.3.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/commons-lang-2.6.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/guava-11.0.2.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/commons-cli-1.2.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/log4j-1.2.17.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/activation-1.1.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/xz-1.0.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/servlet-api-2.5.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/commons-codec-1.4.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-core-1.9.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-client-1.9.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/guice-3.0.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/javax.inject-1.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/aopalliance-1.0.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/commons-io-2.4.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-server-1.9.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/asm-3.2.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-json-1.9.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/jettison-1.1.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/commons-collections-3.2.2.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/jetty-6.1.26.jar:/home/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-api-2.7.3.jar:/home/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-common-2.7.3.jar:/home/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-common-2.7.3.jar:/home/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.7.3.jar:/home/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.7.3.jar:/home/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.7.3.jar:/home/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.7.3.jar:/home/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-tests-2.7.3.jar:/home/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-client-2.7.3.jar:/home/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.7.3.jar:/home/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.3.jar:/home/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.7.3.jar:/home/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-registry-2.7.3.jar:/hpme/hadoop-2.7.3/share/hadoop/mapreduce/*:/home/hadoop-2.7.3/contrib/capacity-scheduler/*.jar:/home/hadoop-2.7.3/contrib/capacity-scheduler/*.jar:/home/hadoop-2.7.3/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r baa91f7c6bc9cb92be5982de4719c1c8af91ccff; compiled by 'root' on 2016-08-18T01:41Z
STARTUP_MSG:   java = 1.8.0_101
************************************************************/
2019-04-27 01:01:38,241 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: registered UNIX signal handlers for [TERM, HUP, INT]
2019-04-27 01:01:38,601 WARN org.apache.hadoop.util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2019-04-27 01:01:38,677 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2019-04-27 01:01:38,749 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2019-04-27 01:01:38,749 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: SecondaryNameNode metrics system started
2019-04-27 01:01:38,969 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /tmp/hadoop-root/dfs/namesecondary/in_use.lock acquired by nodename 17132@master
2019-04-27 01:01:39,003 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: No KeyProvider found.
2019-04-27 01:01:39,003 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsLock is fair:true
2019-04-27 01:01:39,043 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.block.invalidate.limit=1000
2019-04-27 01:01:39,043 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true
2019-04-27 01:01:39,044 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
2019-04-27 01:01:39,046 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: The block deletion will start around 2019 Apr 27 01:01:39
2019-04-27 01:01:39,048 INFO org.apache.hadoop.util.GSet: Computing capacity for map BlocksMap
2019-04-27 01:01:39,048 INFO org.apache.hadoop.util.GSet: VM type       = 32-bit
2019-04-27 01:01:39,049 INFO org.apache.hadoop.util.GSet: 2.0% max memory 966.7 MB = 19.3 MB
2019-04-27 01:01:39,049 INFO org.apache.hadoop.util.GSet: capacity      = 2^22 = 4194304 entries
2019-04-27 01:01:39,081 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.block.access.token.enable=false
2019-04-27 01:01:39,081 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: defaultReplication         = 2
2019-04-27 01:01:39,081 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplication             = 512
2019-04-27 01:01:39,081 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: minReplication             = 1
2019-04-27 01:01:39,081 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplicationStreams      = 2
2019-04-27 01:01:39,081 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: replicationRecheckInterval = 3000
2019-04-27 01:01:39,081 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: encryptDataTransfer        = false
2019-04-27 01:01:39,081 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxNumBlocksToLog          = 1000
2019-04-27 01:01:39,082 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsOwner             = root (auth:SIMPLE)
2019-04-27 01:01:39,082 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: supergroup          = supergroup
2019-04-27 01:01:39,082 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: isPermissionEnabled = false
2019-04-27 01:01:39,082 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: HA Enabled: false
2019-04-27 01:01:39,084 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Append Enabled: true
2019-04-27 01:01:39,235 INFO org.apache.hadoop.util.GSet: Computing capacity for map INodeMap
2019-04-27 01:01:39,235 INFO org.apache.hadoop.util.GSet: VM type       = 32-bit
2019-04-27 01:01:39,235 INFO org.apache.hadoop.util.GSet: 1.0% max memory 966.7 MB = 9.7 MB
2019-04-27 01:01:39,235 INFO org.apache.hadoop.util.GSet: capacity      = 2^21 = 2097152 entries
2019-04-27 01:01:39,256 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: ACLs enabled? false
2019-04-27 01:01:39,256 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: XAttrs enabled? true
2019-04-27 01:01:39,256 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: Maximum size of an xattr: 16384
2019-04-27 01:01:39,256 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Caching file names occuring more than 10 times
2019-04-27 01:01:39,262 INFO org.apache.hadoop.util.GSet: Computing capacity for map cachedBlocks
2019-04-27 01:01:39,262 INFO org.apache.hadoop.util.GSet: VM type       = 32-bit
2019-04-27 01:01:39,262 INFO org.apache.hadoop.util.GSet: 0.25% max memory 966.7 MB = 2.4 MB
2019-04-27 01:01:39,262 INFO org.apache.hadoop.util.GSet: capacity      = 2^19 = 524288 entries
2019-04-27 01:01:39,267 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.threshold-pct = 0.9990000128746033
2019-04-27 01:01:39,267 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.min.datanodes = 0
2019-04-27 01:01:39,267 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.extension     = 30000
2019-04-27 01:01:39,269 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.window.num.buckets = 10
2019-04-27 01:01:39,269 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.num.users = 10
2019-04-27 01:01:39,269 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25
2019-04-27 01:01:39,281 INFO org.apache.hadoop.hdfs.DFSUtil: Starting Web-server for secondary at: http://0.0.0.0:50090
2019-04-27 01:01:39,358 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2019-04-27 01:01:39,364 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2019-04-27 01:01:39,369 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.secondary is not defined
2019-04-27 01:01:39,374 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2019-04-27 01:01:39,375 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context secondary
2019-04-27 01:01:39,376 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2019-04-27 01:01:39,376 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2019-04-27 01:01:39,392 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 50090
2019-04-27 01:01:39,397 INFO org.mortbay.log: jetty-6.1.26
2019-04-27 01:01:39,510 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50090
2019-04-27 01:01:39,510 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Web server init done
2019-04-27 01:01:39,514 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Checkpoint Period   :3600 secs (60 min)
2019-04-27 01:01:39,514 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Log Size Trigger    :1000000 txns
2019-04-27 01:05:39,663 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Image has changed. Downloading updated image from NN.
2019-04-27 01:05:39,784 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Opening connection to http://master:50070/imagetransfer?getimage=1&txid=10&storageInfo=-63:2027304554:0:CID-c6975e7b-5169-483f-89cf-bbcabdcf5460
2019-04-27 01:05:39,795 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Image Transfer timeout configured to 60000 milliseconds
2019-04-27 01:05:39,965 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Transfer took 0,01s at 0,00 KB/s
2019-04-27 01:05:39,965 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file fsimage.ckpt_0000000000000000010 size 351 bytes.
2019-04-27 01:05:39,970 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Opening connection to http://master:50070/imagetransfer?getedit=1&startTxId=11&endTxId=11&storageInfo=-63:2027304554:0:CID-c6975e7b-5169-483f-89cf-bbcabdcf5460
2019-04-27 01:05:39,981 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Transfer took 0,01s at 204800,00 KB/s
2019-04-27 01:05:39,981 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file edits_tmp_0000000000000000011-0000000000000000011_0000000000003606670 size 0 bytes.
2019-04-27 01:05:39,982 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Opening connection to http://master:50070/imagetransfer?getedit=1&startTxId=12&endTxId=13&storageInfo=-63:2027304554:0:CID-c6975e7b-5169-483f-89cf-bbcabdcf5460
2019-04-27 01:05:39,985 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Transfer took 0,00s at 0,00 KB/s
2019-04-27 01:05:39,985 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file edits_tmp_0000000000000000012-0000000000000000013_0000000000003606682 size 0 bytes.
2019-04-27 01:05:40,028 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatPBINode: Loading 1 INodes.
2019-04-27 01:05:40,061 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: Loaded FSImage in 0 seconds.
2019-04-27 01:05:40,061 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Loaded image for txid 10 from /tmp/hadoop-root/dfs/namesecondary/current/fsimage_0000000000000000010
2019-04-27 01:05:40,061 INFO org.apache.hadoop.hdfs.server.namenode.NameCache: initialized with 0 entries 0 lookups
2019-04-27 01:05:40,068 INFO org.apache.hadoop.hdfs.server.namenode.Checkpointer: Checkpointer about to load edits from 2 stream(s).
2019-04-27 01:05:40,071 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading /tmp/hadoop-root/dfs/namesecondary/current/edits_0000000000000000011-0000000000000000011 expecting start txid #11
2019-04-27 01:05:40,071 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file /tmp/hadoop-root/dfs/namesecondary/current/edits_0000000000000000011-0000000000000000011
2019-04-27 01:05:40,085 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file /tmp/hadoop-root/dfs/namesecondary/current/edits_0000000000000000011-0000000000000000011 of size 1048576 edits # 1 loaded in 0 seconds
2019-04-27 01:05:40,085 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading /tmp/hadoop-root/dfs/namesecondary/current/edits_0000000000000000012-0000000000000000013 expecting start txid #12
2019-04-27 01:05:40,085 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file /tmp/hadoop-root/dfs/namesecondary/current/edits_0000000000000000012-0000000000000000013
2019-04-27 01:05:40,085 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file /tmp/hadoop-root/dfs/namesecondary/current/edits_0000000000000000012-0000000000000000013 of size 42 edits # 2 loaded in 0 seconds
2019-04-27 01:05:40,090 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: Saving image file /tmp/hadoop-root/dfs/namesecondary/current/fsimage.ckpt_0000000000000000013 using no compression
2019-04-27 01:05:40,123 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: Image file /tmp/hadoop-root/dfs/namesecondary/current/fsimage.ckpt_0000000000000000013 of size 351 bytes saved in 0 seconds.
2019-04-27 01:05:40,129 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Going to retain 2 images with txid >= 10
2019-04-27 01:05:40,129 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Purging old image FSImageFile(file=/tmp/hadoop-root/dfs/namesecondary/current/fsimage_0000000000000000007, cpktTxId=0000000000000000007)
2019-04-27 01:05:40,129 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Purging old image FSImageFile(file=/tmp/hadoop-root/dfs/namesecondary/current/fsimage_0000000000000000009, cpktTxId=0000000000000000009)
2019-04-27 01:05:40,156 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Uploaded image with txid 13 to namenode at http://master:50070 in 0.019 seconds
2019-04-27 01:05:40,156 WARN org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Checkpoint done. New Image Size: 351
2019-04-27 01:08:41,170 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master/192.168.178.37:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-04-27 01:08:42,173 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master/192.168.178.37:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-04-27 01:08:43,176 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master/192.168.178.37:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-04-27 01:08:44,180 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master/192.168.178.37:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-04-27 01:08:45,187 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master/192.168.178.37:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-04-27 01:08:46,190 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master/192.168.178.37:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-04-27 01:08:47,194 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master/192.168.178.37:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-04-27 01:08:47,380 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: RECEIVED SIGNAL 15: SIGTERM
2019-04-27 01:08:47,382 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down SecondaryNameNode at master/192.168.178.37
************************************************************/
2019-04-27 01:12:44,863 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting SecondaryNameNode
STARTUP_MSG:   host = master/192.168.178.37
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.7.3
STARTUP_MSG:   classpath = /home/hadoop-2.7.3/etc/hadoop:/home/hadoop-2.7.3/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/stax-api-1.0-2.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/activation-1.1.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/jersey-server-1.9.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/asm-3.2.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/log4j-1.2.17.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/jets3t-0.9.0.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/httpclient-4.2.5.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/httpcore-4.2.5.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/commons-lang-2.6.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/commons-configuration-1.6.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/commons-digester-1.8.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/avro-1.7.4.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/paranamer-2.3.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/commons-compress-1.4.1.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/xz-1.0.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/gson-2.2.4.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/hadoop-auth-2.7.3.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/zookeeper-3.4.6.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/netty-3.6.2.Final.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/curator-framework-2.7.1.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/curator-client-2.7.1.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/jsch-0.1.42.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/htrace-core-3.1.0-incubating.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/junit-4.11.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/hamcrest-core-1.3.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/mockito-all-1.8.5.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/hadoop-annotations-2.7.3.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/guava-11.0.2.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/jsr305-3.0.0.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/commons-cli-1.2.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/commons-math3-3.1.1.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/xmlenc-0.52.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/commons-httpclient-3.1.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/commons-logging-1.1.3.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/commons-codec-1.4.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/commons-io-2.4.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/commons-net-3.1.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/commons-collections-3.2.2.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/servlet-api-2.5.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/jetty-6.1.26.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/jetty-util-6.1.26.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/jsp-api-2.1.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/jersey-core-1.9.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/jersey-json-1.9.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/jettison-1.1.jar:/home/hadoop-2.7.3/share/hadoop/common/hadoop-common-2.7.3.jar:/home/hadoop-2.7.3/share/hadoop/common/hadoop-common-2.7.3-tests.jar:/home/hadoop-2.7.3/share/hadoop/common/hadoop-nfs-2.7.3.jar:/home/hadoop-2.7.3/share/hadoop/hdfs:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/guava-11.0.2.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-io-2.4.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/asm-3.2.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/htrace-core-3.1.0-incubating.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/hadoop-hdfs-2.7.3.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/hadoop-hdfs-2.7.3-tests.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/hadoop-hdfs-nfs-2.7.3.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/commons-lang-2.6.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/guava-11.0.2.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/commons-cli-1.2.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/log4j-1.2.17.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/activation-1.1.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/xz-1.0.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/servlet-api-2.5.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/commons-codec-1.4.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-core-1.9.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-client-1.9.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/guice-3.0.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/javax.inject-1.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/aopalliance-1.0.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/commons-io-2.4.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-server-1.9.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/asm-3.2.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-json-1.9.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/jettison-1.1.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/commons-collections-3.2.2.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/jetty-6.1.26.jar:/home/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-api-2.7.3.jar:/home/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-common-2.7.3.jar:/home/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-common-2.7.3.jar:/home/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.7.3.jar:/home/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.7.3.jar:/home/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.7.3.jar:/home/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.7.3.jar:/home/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-tests-2.7.3.jar:/home/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-client-2.7.3.jar:/home/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.7.3.jar:/home/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.3.jar:/home/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.7.3.jar:/home/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-registry-2.7.3.jar:/hpme/hadoop-2.7.3/share/hadoop/mapreduce/*:/home/hadoop-2.7.3/contrib/capacity-scheduler/*.jar:/home/hadoop-2.7.3/contrib/capacity-scheduler/*.jar:/home/hadoop-2.7.3/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r baa91f7c6bc9cb92be5982de4719c1c8af91ccff; compiled by 'root' on 2016-08-18T01:41Z
STARTUP_MSG:   java = 1.8.0_101
************************************************************/
2019-04-27 01:12:44,875 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: registered UNIX signal handlers for [TERM, HUP, INT]
2019-04-27 01:12:45,242 WARN org.apache.hadoop.util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2019-04-27 01:12:45,318 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2019-04-27 01:12:45,391 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2019-04-27 01:12:45,391 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: SecondaryNameNode metrics system started
2019-04-27 01:12:45,631 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /tmp/hadoop-root/dfs/namesecondary/in_use.lock acquired by nodename 5940@master
2019-04-27 01:12:45,669 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: No KeyProvider found.
2019-04-27 01:12:45,669 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsLock is fair:true
2019-04-27 01:12:45,708 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.block.invalidate.limit=1000
2019-04-27 01:12:45,708 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true
2019-04-27 01:12:45,709 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
2019-04-27 01:12:45,711 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: The block deletion will start around 2019 Apr 27 01:12:45
2019-04-27 01:12:45,713 INFO org.apache.hadoop.util.GSet: Computing capacity for map BlocksMap
2019-04-27 01:12:45,713 INFO org.apache.hadoop.util.GSet: VM type       = 32-bit
2019-04-27 01:12:45,714 INFO org.apache.hadoop.util.GSet: 2.0% max memory 966.7 MB = 19.3 MB
2019-04-27 01:12:45,714 INFO org.apache.hadoop.util.GSet: capacity      = 2^22 = 4194304 entries
2019-04-27 01:12:45,751 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.block.access.token.enable=false
2019-04-27 01:12:45,751 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: defaultReplication         = 2
2019-04-27 01:12:45,751 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplication             = 512
2019-04-27 01:12:45,751 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: minReplication             = 1
2019-04-27 01:12:45,751 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplicationStreams      = 2
2019-04-27 01:12:45,751 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: replicationRecheckInterval = 3000
2019-04-27 01:12:45,751 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: encryptDataTransfer        = false
2019-04-27 01:12:45,751 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxNumBlocksToLog          = 1000
2019-04-27 01:12:45,754 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsOwner             = root (auth:SIMPLE)
2019-04-27 01:12:45,754 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: supergroup          = supergroup
2019-04-27 01:12:45,754 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: isPermissionEnabled = false
2019-04-27 01:12:45,754 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: HA Enabled: false
2019-04-27 01:12:45,756 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Append Enabled: true
2019-04-27 01:12:45,912 INFO org.apache.hadoop.util.GSet: Computing capacity for map INodeMap
2019-04-27 01:12:45,912 INFO org.apache.hadoop.util.GSet: VM type       = 32-bit
2019-04-27 01:12:45,912 INFO org.apache.hadoop.util.GSet: 1.0% max memory 966.7 MB = 9.7 MB
2019-04-27 01:12:45,913 INFO org.apache.hadoop.util.GSet: capacity      = 2^21 = 2097152 entries
2019-04-27 01:12:45,933 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: ACLs enabled? false
2019-04-27 01:12:45,934 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: XAttrs enabled? true
2019-04-27 01:12:45,934 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: Maximum size of an xattr: 16384
2019-04-27 01:12:45,934 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Caching file names occuring more than 10 times
2019-04-27 01:12:45,940 INFO org.apache.hadoop.util.GSet: Computing capacity for map cachedBlocks
2019-04-27 01:12:45,940 INFO org.apache.hadoop.util.GSet: VM type       = 32-bit
2019-04-27 01:12:45,940 INFO org.apache.hadoop.util.GSet: 0.25% max memory 966.7 MB = 2.4 MB
2019-04-27 01:12:45,940 INFO org.apache.hadoop.util.GSet: capacity      = 2^19 = 524288 entries
2019-04-27 01:12:45,952 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.threshold-pct = 0.9990000128746033
2019-04-27 01:12:45,952 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.min.datanodes = 0
2019-04-27 01:12:45,952 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.extension     = 30000
2019-04-27 01:12:45,954 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.window.num.buckets = 10
2019-04-27 01:12:45,954 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.num.users = 10
2019-04-27 01:12:45,954 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25
2019-04-27 01:12:45,965 INFO org.apache.hadoop.hdfs.DFSUtil: Starting Web-server for secondary at: http://0.0.0.0:50090
2019-04-27 01:12:46,040 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2019-04-27 01:12:46,046 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2019-04-27 01:12:46,051 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.secondary is not defined
2019-04-27 01:12:46,055 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2019-04-27 01:12:46,056 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context secondary
2019-04-27 01:12:46,056 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2019-04-27 01:12:46,056 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2019-04-27 01:12:46,078 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 50090
2019-04-27 01:12:46,078 INFO org.mortbay.log: jetty-6.1.26
2019-04-27 01:12:46,199 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50090
2019-04-27 01:12:46,199 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Web server init done
2019-04-27 01:12:46,203 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Checkpoint Period   :3600 secs (60 min)
2019-04-27 01:12:46,203 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Log Size Trigger    :1000000 txns
2019-04-27 01:14:24,945 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: RECEIVED SIGNAL 15: SIGTERM
2019-04-27 01:14:24,947 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down SecondaryNameNode at master/192.168.178.37
************************************************************/
2019-04-27 01:15:54,930 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting SecondaryNameNode
STARTUP_MSG:   host = master/192.168.178.37
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.7.3
STARTUP_MSG:   classpath = /home/hadoop-2.7.3/etc/hadoop:/home/hadoop-2.7.3/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/stax-api-1.0-2.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/activation-1.1.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/jersey-server-1.9.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/asm-3.2.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/log4j-1.2.17.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/jets3t-0.9.0.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/httpclient-4.2.5.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/httpcore-4.2.5.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/commons-lang-2.6.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/commons-configuration-1.6.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/commons-digester-1.8.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/avro-1.7.4.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/paranamer-2.3.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/commons-compress-1.4.1.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/xz-1.0.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/gson-2.2.4.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/hadoop-auth-2.7.3.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/zookeeper-3.4.6.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/netty-3.6.2.Final.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/curator-framework-2.7.1.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/curator-client-2.7.1.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/jsch-0.1.42.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/htrace-core-3.1.0-incubating.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/junit-4.11.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/hamcrest-core-1.3.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/mockito-all-1.8.5.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/hadoop-annotations-2.7.3.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/guava-11.0.2.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/jsr305-3.0.0.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/commons-cli-1.2.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/commons-math3-3.1.1.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/xmlenc-0.52.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/commons-httpclient-3.1.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/commons-logging-1.1.3.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/commons-codec-1.4.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/commons-io-2.4.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/commons-net-3.1.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/commons-collections-3.2.2.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/servlet-api-2.5.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/jetty-6.1.26.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/jetty-util-6.1.26.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/jsp-api-2.1.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/jersey-core-1.9.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/jersey-json-1.9.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/jettison-1.1.jar:/home/hadoop-2.7.3/share/hadoop/common/hadoop-common-2.7.3.jar:/home/hadoop-2.7.3/share/hadoop/common/hadoop-common-2.7.3-tests.jar:/home/hadoop-2.7.3/share/hadoop/common/hadoop-nfs-2.7.3.jar:/home/hadoop-2.7.3/share/hadoop/hdfs:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/guava-11.0.2.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-io-2.4.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/asm-3.2.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/htrace-core-3.1.0-incubating.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/hadoop-hdfs-2.7.3.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/hadoop-hdfs-2.7.3-tests.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/hadoop-hdfs-nfs-2.7.3.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/commons-lang-2.6.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/guava-11.0.2.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/commons-cli-1.2.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/log4j-1.2.17.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/activation-1.1.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/xz-1.0.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/servlet-api-2.5.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/commons-codec-1.4.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-core-1.9.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-client-1.9.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/guice-3.0.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/javax.inject-1.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/aopalliance-1.0.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/commons-io-2.4.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-server-1.9.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/asm-3.2.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-json-1.9.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/jettison-1.1.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/commons-collections-3.2.2.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/jetty-6.1.26.jar:/home/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-api-2.7.3.jar:/home/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-common-2.7.3.jar:/home/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-common-2.7.3.jar:/home/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.7.3.jar:/home/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.7.3.jar:/home/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.7.3.jar:/home/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.7.3.jar:/home/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-tests-2.7.3.jar:/home/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-client-2.7.3.jar:/home/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.7.3.jar:/home/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.3.jar:/home/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.7.3.jar:/home/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-registry-2.7.3.jar:/hpme/hadoop-2.7.3/share/hadoop/mapreduce/*:/home/hadoop-2.7.3/contrib/capacity-scheduler/*.jar:/home/hadoop-2.7.3/contrib/capacity-scheduler/*.jar:/home/hadoop-2.7.3/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r baa91f7c6bc9cb92be5982de4719c1c8af91ccff; compiled by 'root' on 2016-08-18T01:41Z
STARTUP_MSG:   java = 1.8.0_101
************************************************************/
2019-04-27 01:15:54,942 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: registered UNIX signal handlers for [TERM, HUP, INT]
2019-04-27 01:15:55,303 WARN org.apache.hadoop.util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2019-04-27 01:15:55,376 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2019-04-27 01:15:55,444 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2019-04-27 01:15:55,444 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: SecondaryNameNode metrics system started
2019-04-27 01:15:55,707 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /tmp/hadoop-root/dfs/namesecondary/in_use.lock acquired by nodename 6924@master
2019-04-27 01:15:55,743 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: No KeyProvider found.
2019-04-27 01:15:55,743 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsLock is fair:true
2019-04-27 01:15:55,784 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.block.invalidate.limit=1000
2019-04-27 01:15:55,784 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true
2019-04-27 01:15:55,785 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
2019-04-27 01:15:55,788 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: The block deletion will start around 2019 Apr 27 01:15:55
2019-04-27 01:15:55,790 INFO org.apache.hadoop.util.GSet: Computing capacity for map BlocksMap
2019-04-27 01:15:55,790 INFO org.apache.hadoop.util.GSet: VM type       = 32-bit
2019-04-27 01:15:55,791 INFO org.apache.hadoop.util.GSet: 2.0% max memory 966.7 MB = 19.3 MB
2019-04-27 01:15:55,791 INFO org.apache.hadoop.util.GSet: capacity      = 2^22 = 4194304 entries
2019-04-27 01:15:55,820 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.block.access.token.enable=false
2019-04-27 01:15:55,820 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: defaultReplication         = 2
2019-04-27 01:15:55,820 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplication             = 512
2019-04-27 01:15:55,820 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: minReplication             = 1
2019-04-27 01:15:55,820 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplicationStreams      = 2
2019-04-27 01:15:55,820 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: replicationRecheckInterval = 3000
2019-04-27 01:15:55,820 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: encryptDataTransfer        = false
2019-04-27 01:15:55,820 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxNumBlocksToLog          = 1000
2019-04-27 01:15:55,822 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsOwner             = root (auth:SIMPLE)
2019-04-27 01:15:55,822 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: supergroup          = supergroup
2019-04-27 01:15:55,822 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: isPermissionEnabled = false
2019-04-27 01:15:55,823 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: HA Enabled: false
2019-04-27 01:15:55,827 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Append Enabled: true
2019-04-27 01:15:55,979 INFO org.apache.hadoop.util.GSet: Computing capacity for map INodeMap
2019-04-27 01:15:55,979 INFO org.apache.hadoop.util.GSet: VM type       = 32-bit
2019-04-27 01:15:55,979 INFO org.apache.hadoop.util.GSet: 1.0% max memory 966.7 MB = 9.7 MB
2019-04-27 01:15:55,979 INFO org.apache.hadoop.util.GSet: capacity      = 2^21 = 2097152 entries
2019-04-27 01:15:55,998 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: ACLs enabled? false
2019-04-27 01:15:55,998 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: XAttrs enabled? true
2019-04-27 01:15:55,998 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: Maximum size of an xattr: 16384
2019-04-27 01:15:55,998 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Caching file names occuring more than 10 times
2019-04-27 01:15:56,004 INFO org.apache.hadoop.util.GSet: Computing capacity for map cachedBlocks
2019-04-27 01:15:56,004 INFO org.apache.hadoop.util.GSet: VM type       = 32-bit
2019-04-27 01:15:56,004 INFO org.apache.hadoop.util.GSet: 0.25% max memory 966.7 MB = 2.4 MB
2019-04-27 01:15:56,004 INFO org.apache.hadoop.util.GSet: capacity      = 2^19 = 524288 entries
2019-04-27 01:15:56,011 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.threshold-pct = 0.9990000128746033
2019-04-27 01:15:56,011 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.min.datanodes = 0
2019-04-27 01:15:56,011 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.extension     = 30000
2019-04-27 01:15:56,013 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.window.num.buckets = 10
2019-04-27 01:15:56,014 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.num.users = 10
2019-04-27 01:15:56,014 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25
2019-04-27 01:15:56,027 INFO org.apache.hadoop.hdfs.DFSUtil: Starting Web-server for secondary at: http://0.0.0.0:50090
2019-04-27 01:15:56,100 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2019-04-27 01:15:56,106 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2019-04-27 01:15:56,111 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.secondary is not defined
2019-04-27 01:15:56,115 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2019-04-27 01:15:56,116 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context secondary
2019-04-27 01:15:56,116 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2019-04-27 01:15:56,116 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2019-04-27 01:15:56,137 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 50090
2019-04-27 01:15:56,137 INFO org.mortbay.log: jetty-6.1.26
2019-04-27 01:15:56,253 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50090
2019-04-27 01:15:56,253 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Web server init done
2019-04-27 01:15:56,256 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Checkpoint Period   :3600 secs (60 min)
2019-04-27 01:15:56,256 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Log Size Trigger    :1000000 txns
2019-04-27 01:21:57,355 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master/192.168.178.37:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-04-27 01:21:58,356 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master/192.168.178.37:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-04-27 01:21:59,357 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master/192.168.178.37:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-04-27 01:22:00,359 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master/192.168.178.37:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-04-27 01:22:01,361 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master/192.168.178.37:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-04-27 01:22:02,365 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master/192.168.178.37:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-04-27 01:22:03,370 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master/192.168.178.37:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-04-27 01:22:04,370 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master/192.168.178.37:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-04-27 01:22:05,373 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master/192.168.178.37:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-04-27 01:22:06,389 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master/192.168.178.37:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-04-27 01:22:06,393 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.net.ConnectException: Call From master/192.168.178.37 to master:9000 failed on connection exception: java.net.ConnectException: Verbindungsaufbau abgelehnt; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1479)
	at org.apache.hadoop.ipc.Client.call(Client.java:1412)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy10.getTransactionId(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolTranslatorPB.getTransactionID(NamenodeProtocolTranslatorPB.java:128)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:191)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy11.getTransactionID(Unknown Source)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.countUncheckpointedTxns(SecondaryNameNode.java:641)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.shouldCheckpointBasedOnCount(SecondaryNameNode.java:649)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:393)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Verbindungsaufbau abgelehnt
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:614)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:712)
	at org.apache.hadoop.ipc.Client$Connection.access$2900(Client.java:375)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1528)
	at org.apache.hadoop.ipc.Client.call(Client.java:1451)
	... 18 more
2019-04-27 01:22:14,806 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: RECEIVED SIGNAL 15: SIGTERM
2019-04-27 01:22:14,806 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down SecondaryNameNode at master/192.168.178.37
************************************************************/
2019-04-27 01:24:28,813 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting SecondaryNameNode
STARTUP_MSG:   host = master/192.168.178.37
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.7.3
STARTUP_MSG:   classpath = /home/hadoop-2.7.3/etc/hadoop:/home/hadoop-2.7.3/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/stax-api-1.0-2.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/activation-1.1.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/jersey-server-1.9.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/asm-3.2.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/log4j-1.2.17.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/jets3t-0.9.0.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/httpclient-4.2.5.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/httpcore-4.2.5.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/commons-lang-2.6.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/commons-configuration-1.6.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/commons-digester-1.8.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/avro-1.7.4.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/paranamer-2.3.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/commons-compress-1.4.1.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/xz-1.0.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/gson-2.2.4.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/hadoop-auth-2.7.3.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/zookeeper-3.4.6.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/netty-3.6.2.Final.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/curator-framework-2.7.1.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/curator-client-2.7.1.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/jsch-0.1.42.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/htrace-core-3.1.0-incubating.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/junit-4.11.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/hamcrest-core-1.3.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/mockito-all-1.8.5.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/hadoop-annotations-2.7.3.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/guava-11.0.2.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/jsr305-3.0.0.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/commons-cli-1.2.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/commons-math3-3.1.1.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/xmlenc-0.52.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/commons-httpclient-3.1.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/commons-logging-1.1.3.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/commons-codec-1.4.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/commons-io-2.4.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/commons-net-3.1.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/commons-collections-3.2.2.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/servlet-api-2.5.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/jetty-6.1.26.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/jetty-util-6.1.26.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/jsp-api-2.1.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/jersey-core-1.9.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/jersey-json-1.9.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/jettison-1.1.jar:/home/hadoop-2.7.3/share/hadoop/common/hadoop-common-2.7.3.jar:/home/hadoop-2.7.3/share/hadoop/common/hadoop-common-2.7.3-tests.jar:/home/hadoop-2.7.3/share/hadoop/common/hadoop-nfs-2.7.3.jar:/home/hadoop-2.7.3/share/hadoop/hdfs:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/guava-11.0.2.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-io-2.4.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/asm-3.2.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/htrace-core-3.1.0-incubating.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/hadoop-hdfs-2.7.3.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/hadoop-hdfs-2.7.3-tests.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/hadoop-hdfs-nfs-2.7.3.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/commons-lang-2.6.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/guava-11.0.2.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/commons-cli-1.2.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/log4j-1.2.17.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/activation-1.1.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/xz-1.0.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/servlet-api-2.5.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/commons-codec-1.4.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-core-1.9.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-client-1.9.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/guice-3.0.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/javax.inject-1.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/aopalliance-1.0.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/commons-io-2.4.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-server-1.9.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/asm-3.2.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-json-1.9.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/jettison-1.1.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/commons-collections-3.2.2.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/jetty-6.1.26.jar:/home/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-api-2.7.3.jar:/home/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-common-2.7.3.jar:/home/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-common-2.7.3.jar:/home/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.7.3.jar:/home/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.7.3.jar:/home/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.7.3.jar:/home/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.7.3.jar:/home/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-tests-2.7.3.jar:/home/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-client-2.7.3.jar:/home/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.7.3.jar:/home/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.3.jar:/home/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.7.3.jar:/home/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-registry-2.7.3.jar:/hpme/hadoop-2.7.3/share/hadoop/mapreduce/*:/home/hadoop-2.7.3/contrib/capacity-scheduler/*.jar:/home/hadoop-2.7.3/contrib/capacity-scheduler/*.jar:/home/hadoop-2.7.3/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r baa91f7c6bc9cb92be5982de4719c1c8af91ccff; compiled by 'root' on 2016-08-18T01:41Z
STARTUP_MSG:   java = 1.8.0_101
************************************************************/
2019-04-27 01:24:28,824 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: registered UNIX signal handlers for [TERM, HUP, INT]
2019-04-27 01:24:29,194 WARN org.apache.hadoop.util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2019-04-27 01:24:29,272 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2019-04-27 01:24:29,348 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2019-04-27 01:24:29,348 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: SecondaryNameNode metrics system started
2019-04-27 01:24:29,610 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /tmp/hadoop-root/dfs/namesecondary/in_use.lock acquired by nodename 8040@master
2019-04-27 01:24:29,645 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: No KeyProvider found.
2019-04-27 01:24:29,645 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsLock is fair:true
2019-04-27 01:24:29,684 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.block.invalidate.limit=1000
2019-04-27 01:24:29,684 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true
2019-04-27 01:24:29,685 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
2019-04-27 01:24:29,688 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: The block deletion will start around 2019 Apr 27 01:24:29
2019-04-27 01:24:29,689 INFO org.apache.hadoop.util.GSet: Computing capacity for map BlocksMap
2019-04-27 01:24:29,689 INFO org.apache.hadoop.util.GSet: VM type       = 32-bit
2019-04-27 01:24:29,691 INFO org.apache.hadoop.util.GSet: 2.0% max memory 966.7 MB = 19.3 MB
2019-04-27 01:24:29,691 INFO org.apache.hadoop.util.GSet: capacity      = 2^22 = 4194304 entries
2019-04-27 01:24:29,717 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.block.access.token.enable=false
2019-04-27 01:24:29,717 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: defaultReplication         = 2
2019-04-27 01:24:29,717 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplication             = 512
2019-04-27 01:24:29,717 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: minReplication             = 1
2019-04-27 01:24:29,717 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplicationStreams      = 2
2019-04-27 01:24:29,717 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: replicationRecheckInterval = 3000
2019-04-27 01:24:29,717 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: encryptDataTransfer        = false
2019-04-27 01:24:29,717 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxNumBlocksToLog          = 1000
2019-04-27 01:24:29,719 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsOwner             = root (auth:SIMPLE)
2019-04-27 01:24:29,719 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: supergroup          = supergroup
2019-04-27 01:24:29,719 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: isPermissionEnabled = false
2019-04-27 01:24:29,719 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: HA Enabled: false
2019-04-27 01:24:29,721 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Append Enabled: true
2019-04-27 01:24:29,874 INFO org.apache.hadoop.util.GSet: Computing capacity for map INodeMap
2019-04-27 01:24:29,874 INFO org.apache.hadoop.util.GSet: VM type       = 32-bit
2019-04-27 01:24:29,874 INFO org.apache.hadoop.util.GSet: 1.0% max memory 966.7 MB = 9.7 MB
2019-04-27 01:24:29,874 INFO org.apache.hadoop.util.GSet: capacity      = 2^21 = 2097152 entries
2019-04-27 01:24:29,895 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: ACLs enabled? false
2019-04-27 01:24:29,895 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: XAttrs enabled? true
2019-04-27 01:24:29,895 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: Maximum size of an xattr: 16384
2019-04-27 01:24:29,895 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Caching file names occuring more than 10 times
2019-04-27 01:24:29,909 INFO org.apache.hadoop.util.GSet: Computing capacity for map cachedBlocks
2019-04-27 01:24:29,909 INFO org.apache.hadoop.util.GSet: VM type       = 32-bit
2019-04-27 01:24:29,910 INFO org.apache.hadoop.util.GSet: 0.25% max memory 966.7 MB = 2.4 MB
2019-04-27 01:24:29,910 INFO org.apache.hadoop.util.GSet: capacity      = 2^19 = 524288 entries
2019-04-27 01:24:29,913 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.threshold-pct = 0.9990000128746033
2019-04-27 01:24:29,913 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.min.datanodes = 0
2019-04-27 01:24:29,913 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.extension     = 30000
2019-04-27 01:24:29,915 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.window.num.buckets = 10
2019-04-27 01:24:29,915 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.num.users = 10
2019-04-27 01:24:29,915 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25
2019-04-27 01:24:29,925 INFO org.apache.hadoop.hdfs.DFSUtil: Starting Web-server for secondary at: http://0.0.0.0:50090
2019-04-27 01:24:29,995 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2019-04-27 01:24:30,001 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2019-04-27 01:24:30,006 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.secondary is not defined
2019-04-27 01:24:30,012 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2019-04-27 01:24:30,013 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context secondary
2019-04-27 01:24:30,018 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2019-04-27 01:24:30,018 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2019-04-27 01:24:30,035 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 50090
2019-04-27 01:24:30,035 INFO org.mortbay.log: jetty-6.1.26
2019-04-27 01:24:30,156 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50090
2019-04-27 01:24:30,156 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Web server init done
2019-04-27 01:24:30,160 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Checkpoint Period   :3600 secs (60 min)
2019-04-27 01:24:30,160 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Log Size Trigger    :1000000 txns
2019-04-27 01:33:18,962 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: RECEIVED SIGNAL 15: SIGTERM
2019-04-27 01:33:18,964 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down SecondaryNameNode at master/192.168.178.37
************************************************************/
2019-04-27 01:34:20,367 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting SecondaryNameNode
STARTUP_MSG:   host = master/192.168.178.37
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.7.3
STARTUP_MSG:   classpath = /home/hadoop-2.7.3/etc/hadoop:/home/hadoop-2.7.3/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/stax-api-1.0-2.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/activation-1.1.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/jersey-server-1.9.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/asm-3.2.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/log4j-1.2.17.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/jets3t-0.9.0.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/httpclient-4.2.5.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/httpcore-4.2.5.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/commons-lang-2.6.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/commons-configuration-1.6.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/commons-digester-1.8.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/avro-1.7.4.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/paranamer-2.3.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/commons-compress-1.4.1.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/xz-1.0.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/gson-2.2.4.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/hadoop-auth-2.7.3.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/zookeeper-3.4.6.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/netty-3.6.2.Final.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/curator-framework-2.7.1.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/curator-client-2.7.1.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/jsch-0.1.42.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/htrace-core-3.1.0-incubating.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/junit-4.11.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/hamcrest-core-1.3.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/mockito-all-1.8.5.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/hadoop-annotations-2.7.3.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/guava-11.0.2.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/jsr305-3.0.0.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/commons-cli-1.2.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/commons-math3-3.1.1.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/xmlenc-0.52.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/commons-httpclient-3.1.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/commons-logging-1.1.3.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/commons-codec-1.4.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/commons-io-2.4.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/commons-net-3.1.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/commons-collections-3.2.2.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/servlet-api-2.5.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/jetty-6.1.26.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/jetty-util-6.1.26.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/jsp-api-2.1.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/jersey-core-1.9.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/jersey-json-1.9.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/jettison-1.1.jar:/home/hadoop-2.7.3/share/hadoop/common/hadoop-common-2.7.3.jar:/home/hadoop-2.7.3/share/hadoop/common/hadoop-common-2.7.3-tests.jar:/home/hadoop-2.7.3/share/hadoop/common/hadoop-nfs-2.7.3.jar:/home/hadoop-2.7.3/share/hadoop/hdfs:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/guava-11.0.2.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-io-2.4.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/asm-3.2.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/htrace-core-3.1.0-incubating.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/hadoop-hdfs-2.7.3.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/hadoop-hdfs-2.7.3-tests.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/hadoop-hdfs-nfs-2.7.3.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/commons-lang-2.6.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/guava-11.0.2.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/commons-cli-1.2.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/log4j-1.2.17.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/activation-1.1.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/xz-1.0.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/servlet-api-2.5.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/commons-codec-1.4.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-core-1.9.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-client-1.9.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/guice-3.0.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/javax.inject-1.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/aopalliance-1.0.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/commons-io-2.4.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-server-1.9.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/asm-3.2.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-json-1.9.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/jettison-1.1.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/commons-collections-3.2.2.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/jetty-6.1.26.jar:/home/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-api-2.7.3.jar:/home/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-common-2.7.3.jar:/home/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-common-2.7.3.jar:/home/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.7.3.jar:/home/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.7.3.jar:/home/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.7.3.jar:/home/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.7.3.jar:/home/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-tests-2.7.3.jar:/home/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-client-2.7.3.jar:/home/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.7.3.jar:/home/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.3.jar:/home/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.7.3.jar:/home/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-registry-2.7.3.jar:/hpme/hadoop-2.7.3/share/hadoop/mapreduce/*:/home/hadoop-2.7.3/contrib/capacity-scheduler/*.jar:/home/hadoop-2.7.3/contrib/capacity-scheduler/*.jar:/home/hadoop-2.7.3/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r baa91f7c6bc9cb92be5982de4719c1c8af91ccff; compiled by 'root' on 2016-08-18T01:41Z
STARTUP_MSG:   java = 1.8.0_101
************************************************************/
2019-04-27 01:34:20,380 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: registered UNIX signal handlers for [TERM, HUP, INT]
2019-04-27 01:34:20,738 WARN org.apache.hadoop.util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2019-04-27 01:34:20,819 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2019-04-27 01:34:20,888 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2019-04-27 01:34:20,889 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: SecondaryNameNode metrics system started
2019-04-27 01:34:21,146 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /tmp/hadoop-root/dfs/namesecondary/in_use.lock acquired by nodename 9120@master
2019-04-27 01:34:21,182 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: No KeyProvider found.
2019-04-27 01:34:21,182 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsLock is fair:true
2019-04-27 01:34:21,222 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.block.invalidate.limit=1000
2019-04-27 01:34:21,222 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true
2019-04-27 01:34:21,223 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
2019-04-27 01:34:21,225 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: The block deletion will start around 2019 Apr 27 01:34:21
2019-04-27 01:34:21,227 INFO org.apache.hadoop.util.GSet: Computing capacity for map BlocksMap
2019-04-27 01:34:21,227 INFO org.apache.hadoop.util.GSet: VM type       = 32-bit
2019-04-27 01:34:21,228 INFO org.apache.hadoop.util.GSet: 2.0% max memory 966.7 MB = 19.3 MB
2019-04-27 01:34:21,228 INFO org.apache.hadoop.util.GSet: capacity      = 2^22 = 4194304 entries
2019-04-27 01:34:21,255 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.block.access.token.enable=false
2019-04-27 01:34:21,256 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: defaultReplication         = 2
2019-04-27 01:34:21,256 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplication             = 512
2019-04-27 01:34:21,256 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: minReplication             = 1
2019-04-27 01:34:21,256 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplicationStreams      = 2
2019-04-27 01:34:21,256 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: replicationRecheckInterval = 3000
2019-04-27 01:34:21,256 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: encryptDataTransfer        = false
2019-04-27 01:34:21,256 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxNumBlocksToLog          = 1000
2019-04-27 01:34:21,257 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsOwner             = root (auth:SIMPLE)
2019-04-27 01:34:21,257 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: supergroup          = supergroup
2019-04-27 01:34:21,257 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: isPermissionEnabled = false
2019-04-27 01:34:21,257 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: HA Enabled: false
2019-04-27 01:34:21,259 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Append Enabled: true
2019-04-27 01:34:21,415 INFO org.apache.hadoop.util.GSet: Computing capacity for map INodeMap
2019-04-27 01:34:21,415 INFO org.apache.hadoop.util.GSet: VM type       = 32-bit
2019-04-27 01:34:21,416 INFO org.apache.hadoop.util.GSet: 1.0% max memory 966.7 MB = 9.7 MB
2019-04-27 01:34:21,416 INFO org.apache.hadoop.util.GSet: capacity      = 2^21 = 2097152 entries
2019-04-27 01:34:21,434 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: ACLs enabled? false
2019-04-27 01:34:21,434 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: XAttrs enabled? true
2019-04-27 01:34:21,434 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: Maximum size of an xattr: 16384
2019-04-27 01:34:21,434 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Caching file names occuring more than 10 times
2019-04-27 01:34:21,440 INFO org.apache.hadoop.util.GSet: Computing capacity for map cachedBlocks
2019-04-27 01:34:21,440 INFO org.apache.hadoop.util.GSet: VM type       = 32-bit
2019-04-27 01:34:21,440 INFO org.apache.hadoop.util.GSet: 0.25% max memory 966.7 MB = 2.4 MB
2019-04-27 01:34:21,440 INFO org.apache.hadoop.util.GSet: capacity      = 2^19 = 524288 entries
2019-04-27 01:34:21,442 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.threshold-pct = 0.9990000128746033
2019-04-27 01:34:21,442 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.min.datanodes = 0
2019-04-27 01:34:21,442 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.extension     = 30000
2019-04-27 01:34:21,447 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.window.num.buckets = 10
2019-04-27 01:34:21,447 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.num.users = 10
2019-04-27 01:34:21,447 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25
2019-04-27 01:34:21,464 INFO org.apache.hadoop.hdfs.DFSUtil: Starting Web-server for secondary at: http://0.0.0.0:50090
2019-04-27 01:34:21,527 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2019-04-27 01:34:21,534 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2019-04-27 01:34:21,538 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.secondary is not defined
2019-04-27 01:34:21,542 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2019-04-27 01:34:21,543 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context secondary
2019-04-27 01:34:21,543 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2019-04-27 01:34:21,543 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2019-04-27 01:34:21,569 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 50090
2019-04-27 01:34:21,569 INFO org.mortbay.log: jetty-6.1.26
2019-04-27 01:34:21,690 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50090
2019-04-27 01:34:21,690 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Web server init done
2019-04-27 01:34:21,692 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Checkpoint Period   :3600 secs (60 min)
2019-04-27 01:34:21,692 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Log Size Trigger    :1000000 txns
2019-04-27 01:38:22,787 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master/192.168.178.37:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-04-27 01:38:23,789 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master/192.168.178.37:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-04-27 01:38:24,794 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master/192.168.178.37:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-04-27 01:38:25,799 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master/192.168.178.37:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-04-27 01:38:26,805 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master/192.168.178.37:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-04-27 01:38:26,998 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: RECEIVED SIGNAL 15: SIGTERM
2019-04-27 01:38:26,999 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down SecondaryNameNode at master/192.168.178.37
************************************************************/
2019-04-27 17:34:02,764 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting SecondaryNameNode
STARTUP_MSG:   host = master/192.168.178.37
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.7.3
STARTUP_MSG:   classpath = /home/hadoop-2.7.3/etc/hadoop:/home/hadoop-2.7.3/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/stax-api-1.0-2.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/activation-1.1.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/jersey-server-1.9.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/asm-3.2.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/log4j-1.2.17.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/jets3t-0.9.0.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/httpclient-4.2.5.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/httpcore-4.2.5.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/commons-lang-2.6.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/commons-configuration-1.6.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/commons-digester-1.8.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/avro-1.7.4.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/paranamer-2.3.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/commons-compress-1.4.1.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/xz-1.0.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/gson-2.2.4.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/hadoop-auth-2.7.3.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/zookeeper-3.4.6.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/netty-3.6.2.Final.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/curator-framework-2.7.1.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/curator-client-2.7.1.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/jsch-0.1.42.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/htrace-core-3.1.0-incubating.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/junit-4.11.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/hamcrest-core-1.3.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/mockito-all-1.8.5.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/hadoop-annotations-2.7.3.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/guava-11.0.2.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/jsr305-3.0.0.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/commons-cli-1.2.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/commons-math3-3.1.1.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/xmlenc-0.52.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/commons-httpclient-3.1.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/commons-logging-1.1.3.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/commons-codec-1.4.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/commons-io-2.4.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/commons-net-3.1.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/commons-collections-3.2.2.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/servlet-api-2.5.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/jetty-6.1.26.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/jetty-util-6.1.26.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/jsp-api-2.1.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/jersey-core-1.9.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/jersey-json-1.9.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/jettison-1.1.jar:/home/hadoop-2.7.3/share/hadoop/common/hadoop-common-2.7.3.jar:/home/hadoop-2.7.3/share/hadoop/common/hadoop-common-2.7.3-tests.jar:/home/hadoop-2.7.3/share/hadoop/common/hadoop-nfs-2.7.3.jar:/home/hadoop-2.7.3/share/hadoop/hdfs:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/guava-11.0.2.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-io-2.4.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/asm-3.2.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/htrace-core-3.1.0-incubating.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/hadoop-hdfs-2.7.3.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/hadoop-hdfs-2.7.3-tests.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/hadoop-hdfs-nfs-2.7.3.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/commons-lang-2.6.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/guava-11.0.2.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/commons-cli-1.2.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/log4j-1.2.17.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/activation-1.1.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/xz-1.0.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/servlet-api-2.5.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/commons-codec-1.4.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-core-1.9.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-client-1.9.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/guice-3.0.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/javax.inject-1.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/aopalliance-1.0.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/commons-io-2.4.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-server-1.9.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/asm-3.2.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-json-1.9.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/jettison-1.1.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/commons-collections-3.2.2.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/jetty-6.1.26.jar:/home/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-api-2.7.3.jar:/home/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-common-2.7.3.jar:/home/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-common-2.7.3.jar:/home/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.7.3.jar:/home/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.7.3.jar:/home/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.7.3.jar:/home/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.7.3.jar:/home/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-tests-2.7.3.jar:/home/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-client-2.7.3.jar:/home/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.7.3.jar:/home/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.3.jar:/home/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.7.3.jar:/home/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-registry-2.7.3.jar:/hpme/hadoop-2.7.3/share/hadoop/mapreduce/*:/home/hadoop-2.7.3/contrib/capacity-scheduler/*.jar:/home/hadoop-2.7.3/contrib/capacity-scheduler/*.jar:/home/hadoop-2.7.3/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r baa91f7c6bc9cb92be5982de4719c1c8af91ccff; compiled by 'root' on 2016-08-18T01:41Z
STARTUP_MSG:   java = 1.8.0_101
************************************************************/
2019-04-27 17:34:02,777 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: registered UNIX signal handlers for [TERM, HUP, INT]
2019-04-27 17:34:03,180 WARN org.apache.hadoop.util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2019-04-27 17:34:03,272 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2019-04-27 17:34:03,352 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2019-04-27 17:34:03,352 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: SecondaryNameNode metrics system started
2019-04-27 17:34:03,589 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /tmp/hadoop-root/dfs/namesecondary/in_use.lock acquired by nodename 8530@master
2019-04-27 17:34:03,627 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: No KeyProvider found.
2019-04-27 17:34:03,627 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsLock is fair:true
2019-04-27 17:34:03,672 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.block.invalidate.limit=1000
2019-04-27 17:34:03,672 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true
2019-04-27 17:34:03,672 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
2019-04-27 17:34:03,675 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: The block deletion will start around 2019 Apr 27 17:34:03
2019-04-27 17:34:03,677 INFO org.apache.hadoop.util.GSet: Computing capacity for map BlocksMap
2019-04-27 17:34:03,677 INFO org.apache.hadoop.util.GSet: VM type       = 32-bit
2019-04-27 17:34:03,679 INFO org.apache.hadoop.util.GSet: 2.0% max memory 966.7 MB = 19.3 MB
2019-04-27 17:34:03,679 INFO org.apache.hadoop.util.GSet: capacity      = 2^22 = 4194304 entries
2019-04-27 17:34:03,719 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.block.access.token.enable=false
2019-04-27 17:34:03,719 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: defaultReplication         = 2
2019-04-27 17:34:03,719 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplication             = 512
2019-04-27 17:34:03,719 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: minReplication             = 1
2019-04-27 17:34:03,719 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplicationStreams      = 2
2019-04-27 17:34:03,719 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: replicationRecheckInterval = 3000
2019-04-27 17:34:03,719 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: encryptDataTransfer        = false
2019-04-27 17:34:03,719 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxNumBlocksToLog          = 1000
2019-04-27 17:34:03,721 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsOwner             = root (auth:SIMPLE)
2019-04-27 17:34:03,721 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: supergroup          = supergroup
2019-04-27 17:34:03,721 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: isPermissionEnabled = false
2019-04-27 17:34:03,721 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: HA Enabled: false
2019-04-27 17:34:03,724 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Append Enabled: true
2019-04-27 17:34:03,902 INFO org.apache.hadoop.util.GSet: Computing capacity for map INodeMap
2019-04-27 17:34:03,902 INFO org.apache.hadoop.util.GSet: VM type       = 32-bit
2019-04-27 17:34:03,903 INFO org.apache.hadoop.util.GSet: 1.0% max memory 966.7 MB = 9.7 MB
2019-04-27 17:34:03,903 INFO org.apache.hadoop.util.GSet: capacity      = 2^21 = 2097152 entries
2019-04-27 17:34:03,930 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: ACLs enabled? false
2019-04-27 17:34:03,930 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: XAttrs enabled? true
2019-04-27 17:34:03,930 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: Maximum size of an xattr: 16384
2019-04-27 17:34:03,930 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Caching file names occuring more than 10 times
2019-04-27 17:34:03,940 INFO org.apache.hadoop.util.GSet: Computing capacity for map cachedBlocks
2019-04-27 17:34:03,940 INFO org.apache.hadoop.util.GSet: VM type       = 32-bit
2019-04-27 17:34:03,940 INFO org.apache.hadoop.util.GSet: 0.25% max memory 966.7 MB = 2.4 MB
2019-04-27 17:34:03,940 INFO org.apache.hadoop.util.GSet: capacity      = 2^19 = 524288 entries
2019-04-27 17:34:03,943 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.threshold-pct = 0.9990000128746033
2019-04-27 17:34:03,943 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.min.datanodes = 0
2019-04-27 17:34:03,943 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.extension     = 30000
2019-04-27 17:34:03,946 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.window.num.buckets = 10
2019-04-27 17:34:03,946 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.num.users = 10
2019-04-27 17:34:03,946 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25
2019-04-27 17:34:03,957 INFO org.apache.hadoop.hdfs.DFSUtil: Starting Web-server for secondary at: http://0.0.0.0:50090
2019-04-27 17:34:04,031 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2019-04-27 17:34:04,038 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2019-04-27 17:34:04,044 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.secondary is not defined
2019-04-27 17:34:04,049 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2019-04-27 17:34:04,050 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context secondary
2019-04-27 17:34:04,050 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2019-04-27 17:34:04,050 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2019-04-27 17:34:04,067 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 50090
2019-04-27 17:34:04,068 INFO org.mortbay.log: jetty-6.1.26
2019-04-27 17:34:04,198 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50090
2019-04-27 17:34:04,198 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Web server init done
2019-04-27 17:34:04,202 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Checkpoint Period   :3600 secs (60 min)
2019-04-27 17:34:04,202 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Log Size Trigger    :1000000 txns
2019-04-27 17:35:04,343 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Image has changed. Downloading updated image from NN.
2019-04-27 17:35:04,474 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Opening connection to http://master:50070/imagetransfer?getimage=1&txid=18&storageInfo=-63:2027304554:0:CID-c6975e7b-5169-483f-89cf-bbcabdcf5460
2019-04-27 17:35:04,486 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Image Transfer timeout configured to 60000 milliseconds
2019-04-27 17:35:04,666 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Transfer took 0,00s at 0,00 KB/s
2019-04-27 17:35:04,666 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file fsimage.ckpt_0000000000000000018 size 351 bytes.
2019-04-27 17:35:04,672 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Opening connection to http://master:50070/imagetransfer?getedit=1&startTxId=19&endTxId=20&storageInfo=-63:2027304554:0:CID-c6975e7b-5169-483f-89cf-bbcabdcf5460
2019-04-27 17:35:04,676 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Transfer took 0,00s at 0,00 KB/s
2019-04-27 17:35:04,676 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file edits_tmp_0000000000000000019-0000000000000000020_0000000000010982349 size 0 bytes.
2019-04-27 17:35:04,719 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatPBINode: Loading 1 INodes.
2019-04-27 17:35:04,754 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: Loaded FSImage in 0 seconds.
2019-04-27 17:35:04,755 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Loaded image for txid 18 from /tmp/hadoop-root/dfs/namesecondary/current/fsimage_0000000000000000018
2019-04-27 17:35:04,755 INFO org.apache.hadoop.hdfs.server.namenode.NameCache: initialized with 0 entries 0 lookups
2019-04-27 17:35:04,759 INFO org.apache.hadoop.hdfs.server.namenode.Checkpointer: Checkpointer about to load edits from 1 stream(s).
2019-04-27 17:35:04,763 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading /tmp/hadoop-root/dfs/namesecondary/current/edits_0000000000000000019-0000000000000000020 expecting start txid #19
2019-04-27 17:35:04,763 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file /tmp/hadoop-root/dfs/namesecondary/current/edits_0000000000000000019-0000000000000000020
2019-04-27 17:35:04,777 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file /tmp/hadoop-root/dfs/namesecondary/current/edits_0000000000000000019-0000000000000000020 of size 42 edits # 2 loaded in 0 seconds
2019-04-27 17:35:04,783 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: Saving image file /tmp/hadoop-root/dfs/namesecondary/current/fsimage.ckpt_0000000000000000020 using no compression
2019-04-27 17:35:04,824 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: Image file /tmp/hadoop-root/dfs/namesecondary/current/fsimage.ckpt_0000000000000000020 of size 351 bytes saved in 0 seconds.
2019-04-27 17:35:04,829 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Going to retain 2 images with txid >= 18
2019-04-27 17:35:04,830 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Purging old image FSImageFile(file=/tmp/hadoop-root/dfs/namesecondary/current/fsimage_0000000000000000010, cpktTxId=0000000000000000010)
2019-04-27 17:35:04,830 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Purging old image FSImageFile(file=/tmp/hadoop-root/dfs/namesecondary/current/fsimage_0000000000000000013, cpktTxId=0000000000000000013)
2019-04-27 17:35:04,859 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Uploaded image with txid 20 to namenode at http://master:50070 in 0.021 seconds
2019-04-27 17:35:04,859 WARN org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Checkpoint done. New Image Size: 351
2019-04-27 17:37:05,869 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master/192.168.178.37:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-04-27 17:37:06,870 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master/192.168.178.37:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-04-27 17:37:07,871 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master/192.168.178.37:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-04-27 17:37:08,877 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master/192.168.178.37:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-04-27 17:37:09,878 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master/192.168.178.37:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-04-27 17:37:10,884 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master/192.168.178.37:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-04-27 17:37:11,890 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master/192.168.178.37:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-04-27 17:37:12,892 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master/192.168.178.37:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-04-27 17:37:13,893 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master/192.168.178.37:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-04-27 17:37:14,894 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master/192.168.178.37:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-04-27 17:37:14,898 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.net.ConnectException: Call From master/192.168.178.37 to master:9000 failed on connection exception: java.net.ConnectException: Verbindungsaufbau abgelehnt; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1479)
	at org.apache.hadoop.ipc.Client.call(Client.java:1412)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy10.getTransactionId(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolTranslatorPB.getTransactionID(NamenodeProtocolTranslatorPB.java:128)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:191)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy11.getTransactionID(Unknown Source)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.countUncheckpointedTxns(SecondaryNameNode.java:641)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.shouldCheckpointBasedOnCount(SecondaryNameNode.java:649)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:393)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Verbindungsaufbau abgelehnt
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:614)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:712)
	at org.apache.hadoop.ipc.Client$Connection.access$2900(Client.java:375)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1528)
	at org.apache.hadoop.ipc.Client.call(Client.java:1451)
	... 18 more
2019-04-27 17:37:43,400 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: RECEIVED SIGNAL 15: SIGTERM
2019-04-27 17:37:43,401 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down SecondaryNameNode at master/192.168.178.37
************************************************************/
2019-04-27 17:39:12,497 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting SecondaryNameNode
STARTUP_MSG:   host = master/192.168.178.37
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.7.3
STARTUP_MSG:   classpath = /home/hadoop-2.7.3/etc/hadoop:/home/hadoop-2.7.3/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/stax-api-1.0-2.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/activation-1.1.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/jersey-server-1.9.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/asm-3.2.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/log4j-1.2.17.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/jets3t-0.9.0.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/httpclient-4.2.5.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/httpcore-4.2.5.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/commons-lang-2.6.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/commons-configuration-1.6.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/commons-digester-1.8.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/avro-1.7.4.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/paranamer-2.3.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/commons-compress-1.4.1.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/xz-1.0.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/gson-2.2.4.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/hadoop-auth-2.7.3.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/zookeeper-3.4.6.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/netty-3.6.2.Final.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/curator-framework-2.7.1.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/curator-client-2.7.1.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/jsch-0.1.42.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/htrace-core-3.1.0-incubating.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/junit-4.11.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/hamcrest-core-1.3.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/mockito-all-1.8.5.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/hadoop-annotations-2.7.3.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/guava-11.0.2.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/jsr305-3.0.0.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/commons-cli-1.2.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/commons-math3-3.1.1.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/xmlenc-0.52.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/commons-httpclient-3.1.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/commons-logging-1.1.3.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/commons-codec-1.4.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/commons-io-2.4.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/commons-net-3.1.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/commons-collections-3.2.2.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/servlet-api-2.5.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/jetty-6.1.26.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/jetty-util-6.1.26.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/jsp-api-2.1.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/jersey-core-1.9.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/jersey-json-1.9.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/jettison-1.1.jar:/home/hadoop-2.7.3/share/hadoop/common/hadoop-common-2.7.3.jar:/home/hadoop-2.7.3/share/hadoop/common/hadoop-common-2.7.3-tests.jar:/home/hadoop-2.7.3/share/hadoop/common/hadoop-nfs-2.7.3.jar:/home/hadoop-2.7.3/share/hadoop/hdfs:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/guava-11.0.2.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-io-2.4.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/asm-3.2.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/htrace-core-3.1.0-incubating.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/hadoop-hdfs-2.7.3.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/hadoop-hdfs-2.7.3-tests.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/hadoop-hdfs-nfs-2.7.3.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/commons-lang-2.6.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/guava-11.0.2.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/commons-cli-1.2.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/log4j-1.2.17.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/activation-1.1.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/xz-1.0.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/servlet-api-2.5.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/commons-codec-1.4.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-core-1.9.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-client-1.9.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/guice-3.0.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/javax.inject-1.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/aopalliance-1.0.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/commons-io-2.4.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-server-1.9.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/asm-3.2.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-json-1.9.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/jettison-1.1.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/commons-collections-3.2.2.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/jetty-6.1.26.jar:/home/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-api-2.7.3.jar:/home/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-common-2.7.3.jar:/home/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-common-2.7.3.jar:/home/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.7.3.jar:/home/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.7.3.jar:/home/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.7.3.jar:/home/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.7.3.jar:/home/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-tests-2.7.3.jar:/home/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-client-2.7.3.jar:/home/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.7.3.jar:/home/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.3.jar:/home/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.7.3.jar:/home/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-registry-2.7.3.jar:/hpme/hadoop-2.7.3/share/hadoop/mapreduce/*:/home/hadoop-2.7.3/contrib/capacity-scheduler/*.jar:/home/hadoop-2.7.3/contrib/capacity-scheduler/*.jar:/home/hadoop-2.7.3/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r baa91f7c6bc9cb92be5982de4719c1c8af91ccff; compiled by 'root' on 2016-08-18T01:41Z
STARTUP_MSG:   java = 1.8.0_101
************************************************************/
2019-04-27 17:39:12,510 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: registered UNIX signal handlers for [TERM, HUP, INT]
2019-04-27 17:39:12,910 WARN org.apache.hadoop.util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2019-04-27 17:39:12,998 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2019-04-27 17:39:13,065 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2019-04-27 17:39:13,065 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: SecondaryNameNode metrics system started
2019-04-27 17:39:13,309 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /tmp/hadoop-root/dfs/namesecondary/in_use.lock acquired by nodename 9420@master
2019-04-27 17:39:13,349 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: No KeyProvider found.
2019-04-27 17:39:13,349 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsLock is fair:true
2019-04-27 17:39:13,393 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.block.invalidate.limit=1000
2019-04-27 17:39:13,393 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true
2019-04-27 17:39:13,394 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
2019-04-27 17:39:13,397 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: The block deletion will start around 2019 Apr 27 17:39:13
2019-04-27 17:39:13,399 INFO org.apache.hadoop.util.GSet: Computing capacity for map BlocksMap
2019-04-27 17:39:13,399 INFO org.apache.hadoop.util.GSet: VM type       = 32-bit
2019-04-27 17:39:13,400 INFO org.apache.hadoop.util.GSet: 2.0% max memory 966.7 MB = 19.3 MB
2019-04-27 17:39:13,400 INFO org.apache.hadoop.util.GSet: capacity      = 2^22 = 4194304 entries
2019-04-27 17:39:13,426 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.block.access.token.enable=false
2019-04-27 17:39:13,426 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: defaultReplication         = 2
2019-04-27 17:39:13,426 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplication             = 512
2019-04-27 17:39:13,426 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: minReplication             = 1
2019-04-27 17:39:13,426 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplicationStreams      = 2
2019-04-27 17:39:13,426 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: replicationRecheckInterval = 3000
2019-04-27 17:39:13,426 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: encryptDataTransfer        = false
2019-04-27 17:39:13,426 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxNumBlocksToLog          = 1000
2019-04-27 17:39:13,428 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsOwner             = root (auth:SIMPLE)
2019-04-27 17:39:13,428 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: supergroup          = supergroup
2019-04-27 17:39:13,428 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: isPermissionEnabled = false
2019-04-27 17:39:13,428 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: HA Enabled: false
2019-04-27 17:39:13,429 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Append Enabled: true
2019-04-27 17:39:13,602 INFO org.apache.hadoop.util.GSet: Computing capacity for map INodeMap
2019-04-27 17:39:13,602 INFO org.apache.hadoop.util.GSet: VM type       = 32-bit
2019-04-27 17:39:13,602 INFO org.apache.hadoop.util.GSet: 1.0% max memory 966.7 MB = 9.7 MB
2019-04-27 17:39:13,602 INFO org.apache.hadoop.util.GSet: capacity      = 2^21 = 2097152 entries
2019-04-27 17:39:13,623 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: ACLs enabled? false
2019-04-27 17:39:13,623 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: XAttrs enabled? true
2019-04-27 17:39:13,623 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: Maximum size of an xattr: 16384
2019-04-27 17:39:13,623 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Caching file names occuring more than 10 times
2019-04-27 17:39:13,630 INFO org.apache.hadoop.util.GSet: Computing capacity for map cachedBlocks
2019-04-27 17:39:13,630 INFO org.apache.hadoop.util.GSet: VM type       = 32-bit
2019-04-27 17:39:13,630 INFO org.apache.hadoop.util.GSet: 0.25% max memory 966.7 MB = 2.4 MB
2019-04-27 17:39:13,630 INFO org.apache.hadoop.util.GSet: capacity      = 2^19 = 524288 entries
2019-04-27 17:39:13,633 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.threshold-pct = 0.9990000128746033
2019-04-27 17:39:13,633 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.min.datanodes = 0
2019-04-27 17:39:13,633 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.extension     = 30000
2019-04-27 17:39:13,636 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.window.num.buckets = 10
2019-04-27 17:39:13,636 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.num.users = 10
2019-04-27 17:39:13,636 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25
2019-04-27 17:39:13,646 INFO org.apache.hadoop.hdfs.DFSUtil: Starting Web-server for secondary at: http://0.0.0.0:50090
2019-04-27 17:39:13,710 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2019-04-27 17:39:13,717 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2019-04-27 17:39:13,723 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.secondary is not defined
2019-04-27 17:39:13,728 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2019-04-27 17:39:13,729 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context secondary
2019-04-27 17:39:13,729 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2019-04-27 17:39:13,729 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2019-04-27 17:39:13,756 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 50090
2019-04-27 17:39:13,756 INFO org.mortbay.log: jetty-6.1.26
2019-04-27 17:39:13,882 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50090
2019-04-27 17:39:13,882 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Web server init done
2019-04-27 17:39:13,886 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Checkpoint Period   :3600 secs (60 min)
2019-04-27 17:39:13,886 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Log Size Trigger    :1000000 txns
2019-04-27 17:40:14,029 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Image has changed. Downloading updated image from NN.
2019-04-27 17:40:14,158 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Opening connection to http://master:50070/imagetransfer?getimage=1&txid=20&storageInfo=-63:2027304554:0:CID-c6975e7b-5169-483f-89cf-bbcabdcf5460
2019-04-27 17:40:14,169 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Image Transfer timeout configured to 60000 milliseconds
2019-04-27 17:40:14,352 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Transfer took 0,00s at 0,00 KB/s
2019-04-27 17:40:14,352 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file fsimage.ckpt_0000000000000000020 size 351 bytes.
2019-04-27 17:40:14,367 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Opening connection to http://master:50070/imagetransfer?getedit=1&startTxId=21&endTxId=21&storageInfo=-63:2027304554:0:CID-c6975e7b-5169-483f-89cf-bbcabdcf5460
2019-04-27 17:40:14,379 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Transfer took 0,01s at 170666,67 KB/s
2019-04-27 17:40:14,379 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file edits_tmp_0000000000000000021-0000000000000000021_0000000000011292044 size 0 bytes.
2019-04-27 17:40:14,380 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Opening connection to http://master:50070/imagetransfer?getedit=1&startTxId=22&endTxId=23&storageInfo=-63:2027304554:0:CID-c6975e7b-5169-483f-89cf-bbcabdcf5460
2019-04-27 17:40:14,383 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Transfer took 0,00s at 0,00 KB/s
2019-04-27 17:40:14,383 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file edits_tmp_0000000000000000022-0000000000000000023_0000000000011292056 size 0 bytes.
2019-04-27 17:40:14,438 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatPBINode: Loading 1 INodes.
2019-04-27 17:40:14,478 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: Loaded FSImage in 0 seconds.
2019-04-27 17:40:14,478 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Loaded image for txid 20 from /tmp/hadoop-root/dfs/namesecondary/current/fsimage_0000000000000000020
2019-04-27 17:40:14,479 INFO org.apache.hadoop.hdfs.server.namenode.NameCache: initialized with 0 entries 0 lookups
2019-04-27 17:40:14,486 INFO org.apache.hadoop.hdfs.server.namenode.Checkpointer: Checkpointer about to load edits from 2 stream(s).
2019-04-27 17:40:14,497 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading /tmp/hadoop-root/dfs/namesecondary/current/edits_0000000000000000021-0000000000000000021 expecting start txid #21
2019-04-27 17:40:14,497 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file /tmp/hadoop-root/dfs/namesecondary/current/edits_0000000000000000021-0000000000000000021
2019-04-27 17:40:14,513 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file /tmp/hadoop-root/dfs/namesecondary/current/edits_0000000000000000021-0000000000000000021 of size 1048576 edits # 1 loaded in 0 seconds
2019-04-27 17:40:14,513 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading /tmp/hadoop-root/dfs/namesecondary/current/edits_0000000000000000022-0000000000000000023 expecting start txid #22
2019-04-27 17:40:14,513 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file /tmp/hadoop-root/dfs/namesecondary/current/edits_0000000000000000022-0000000000000000023
2019-04-27 17:40:14,514 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file /tmp/hadoop-root/dfs/namesecondary/current/edits_0000000000000000022-0000000000000000023 of size 42 edits # 2 loaded in 0 seconds
2019-04-27 17:40:14,519 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: Saving image file /tmp/hadoop-root/dfs/namesecondary/current/fsimage.ckpt_0000000000000000023 using no compression
2019-04-27 17:40:14,560 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: Image file /tmp/hadoop-root/dfs/namesecondary/current/fsimage.ckpt_0000000000000000023 of size 351 bytes saved in 0 seconds.
2019-04-27 17:40:14,566 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Going to retain 2 images with txid >= 20
2019-04-27 17:40:14,566 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Purging old image FSImageFile(file=/tmp/hadoop-root/dfs/namesecondary/current/fsimage_0000000000000000018, cpktTxId=0000000000000000018)
2019-04-27 17:40:14,596 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Uploaded image with txid 23 to namenode at http://master:50070 in 0.021 seconds
2019-04-27 17:40:14,597 WARN org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Checkpoint done. New Image Size: 351
2019-04-27 17:41:15,606 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master/192.168.178.37:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-04-27 17:41:16,611 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master/192.168.178.37:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-04-27 17:41:17,614 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master/192.168.178.37:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-04-27 17:41:18,628 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master/192.168.178.37:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-04-27 17:41:19,630 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master/192.168.178.37:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-04-27 17:41:20,631 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master/192.168.178.37:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-04-27 17:41:21,098 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: RECEIVED SIGNAL 15: SIGTERM
2019-04-27 17:41:21,100 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down SecondaryNameNode at master/192.168.178.37
************************************************************/
2019-04-27 17:43:08,464 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting SecondaryNameNode
STARTUP_MSG:   host = master/192.168.178.37
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.7.3
STARTUP_MSG:   classpath = /home/hadoop-2.7.3/etc/hadoop:/home/hadoop-2.7.3/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/stax-api-1.0-2.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/activation-1.1.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/jersey-server-1.9.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/asm-3.2.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/log4j-1.2.17.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/jets3t-0.9.0.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/httpclient-4.2.5.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/httpcore-4.2.5.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/commons-lang-2.6.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/commons-configuration-1.6.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/commons-digester-1.8.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/avro-1.7.4.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/paranamer-2.3.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/commons-compress-1.4.1.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/xz-1.0.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/gson-2.2.4.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/hadoop-auth-2.7.3.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/zookeeper-3.4.6.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/netty-3.6.2.Final.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/curator-framework-2.7.1.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/curator-client-2.7.1.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/jsch-0.1.42.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/htrace-core-3.1.0-incubating.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/junit-4.11.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/hamcrest-core-1.3.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/mockito-all-1.8.5.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/hadoop-annotations-2.7.3.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/guava-11.0.2.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/jsr305-3.0.0.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/commons-cli-1.2.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/commons-math3-3.1.1.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/xmlenc-0.52.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/commons-httpclient-3.1.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/commons-logging-1.1.3.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/commons-codec-1.4.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/commons-io-2.4.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/commons-net-3.1.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/commons-collections-3.2.2.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/servlet-api-2.5.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/jetty-6.1.26.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/jetty-util-6.1.26.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/jsp-api-2.1.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/jersey-core-1.9.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/jersey-json-1.9.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/jettison-1.1.jar:/home/hadoop-2.7.3/share/hadoop/common/hadoop-common-2.7.3.jar:/home/hadoop-2.7.3/share/hadoop/common/hadoop-common-2.7.3-tests.jar:/home/hadoop-2.7.3/share/hadoop/common/hadoop-nfs-2.7.3.jar:/home/hadoop-2.7.3/share/hadoop/hdfs:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/guava-11.0.2.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-io-2.4.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/asm-3.2.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/htrace-core-3.1.0-incubating.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/hadoop-hdfs-2.7.3.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/hadoop-hdfs-2.7.3-tests.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/hadoop-hdfs-nfs-2.7.3.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/commons-lang-2.6.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/guava-11.0.2.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/commons-cli-1.2.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/log4j-1.2.17.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/activation-1.1.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/xz-1.0.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/servlet-api-2.5.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/commons-codec-1.4.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-core-1.9.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-client-1.9.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/guice-3.0.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/javax.inject-1.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/aopalliance-1.0.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/commons-io-2.4.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-server-1.9.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/asm-3.2.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-json-1.9.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/jettison-1.1.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/commons-collections-3.2.2.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/jetty-6.1.26.jar:/home/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-api-2.7.3.jar:/home/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-common-2.7.3.jar:/home/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-common-2.7.3.jar:/home/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.7.3.jar:/home/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.7.3.jar:/home/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.7.3.jar:/home/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.7.3.jar:/home/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-tests-2.7.3.jar:/home/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-client-2.7.3.jar:/home/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.7.3.jar:/home/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.3.jar:/home/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.7.3.jar:/home/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-registry-2.7.3.jar:/hpme/hadoop-2.7.3/share/hadoop/mapreduce/*:/home/hadoop-2.7.3/contrib/capacity-scheduler/*.jar:/home/hadoop-2.7.3/contrib/capacity-scheduler/*.jar:/home/hadoop-2.7.3/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r baa91f7c6bc9cb92be5982de4719c1c8af91ccff; compiled by 'root' on 2016-08-18T01:41Z
STARTUP_MSG:   java = 1.8.0_101
************************************************************/
2019-04-27 17:43:08,478 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: registered UNIX signal handlers for [TERM, HUP, INT]
2019-04-27 17:43:08,867 WARN org.apache.hadoop.util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2019-04-27 17:43:08,955 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2019-04-27 17:43:09,022 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2019-04-27 17:43:09,022 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: SecondaryNameNode metrics system started
2019-04-27 17:43:09,258 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /tmp/hadoop-root/dfs/namesecondary/in_use.lock acquired by nodename 10415@master
2019-04-27 17:43:09,296 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: No KeyProvider found.
2019-04-27 17:43:09,296 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsLock is fair:true
2019-04-27 17:43:09,340 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.block.invalidate.limit=1000
2019-04-27 17:43:09,340 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true
2019-04-27 17:43:09,341 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
2019-04-27 17:43:09,346 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: The block deletion will start around 2019 Apr 27 17:43:09
2019-04-27 17:43:09,347 INFO org.apache.hadoop.util.GSet: Computing capacity for map BlocksMap
2019-04-27 17:43:09,347 INFO org.apache.hadoop.util.GSet: VM type       = 32-bit
2019-04-27 17:43:09,349 INFO org.apache.hadoop.util.GSet: 2.0% max memory 966.7 MB = 19.3 MB
2019-04-27 17:43:09,349 INFO org.apache.hadoop.util.GSet: capacity      = 2^22 = 4194304 entries
2019-04-27 17:43:09,380 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.block.access.token.enable=false
2019-04-27 17:43:09,380 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: defaultReplication         = 2
2019-04-27 17:43:09,380 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplication             = 512
2019-04-27 17:43:09,380 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: minReplication             = 1
2019-04-27 17:43:09,380 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplicationStreams      = 2
2019-04-27 17:43:09,380 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: replicationRecheckInterval = 3000
2019-04-27 17:43:09,380 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: encryptDataTransfer        = false
2019-04-27 17:43:09,380 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxNumBlocksToLog          = 1000
2019-04-27 17:43:09,381 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsOwner             = root (auth:SIMPLE)
2019-04-27 17:43:09,381 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: supergroup          = supergroup
2019-04-27 17:43:09,381 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: isPermissionEnabled = false
2019-04-27 17:43:09,382 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: HA Enabled: false
2019-04-27 17:43:09,383 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Append Enabled: true
2019-04-27 17:43:09,560 INFO org.apache.hadoop.util.GSet: Computing capacity for map INodeMap
2019-04-27 17:43:09,560 INFO org.apache.hadoop.util.GSet: VM type       = 32-bit
2019-04-27 17:43:09,561 INFO org.apache.hadoop.util.GSet: 1.0% max memory 966.7 MB = 9.7 MB
2019-04-27 17:43:09,561 INFO org.apache.hadoop.util.GSet: capacity      = 2^21 = 2097152 entries
2019-04-27 17:43:09,585 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: ACLs enabled? false
2019-04-27 17:43:09,585 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: XAttrs enabled? true
2019-04-27 17:43:09,585 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: Maximum size of an xattr: 16384
2019-04-27 17:43:09,585 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Caching file names occuring more than 10 times
2019-04-27 17:43:09,598 INFO org.apache.hadoop.util.GSet: Computing capacity for map cachedBlocks
2019-04-27 17:43:09,598 INFO org.apache.hadoop.util.GSet: VM type       = 32-bit
2019-04-27 17:43:09,598 INFO org.apache.hadoop.util.GSet: 0.25% max memory 966.7 MB = 2.4 MB
2019-04-27 17:43:09,598 INFO org.apache.hadoop.util.GSet: capacity      = 2^19 = 524288 entries
2019-04-27 17:43:09,600 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.threshold-pct = 0.9990000128746033
2019-04-27 17:43:09,600 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.min.datanodes = 0
2019-04-27 17:43:09,600 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.extension     = 30000
2019-04-27 17:43:09,603 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.window.num.buckets = 10
2019-04-27 17:43:09,603 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.num.users = 10
2019-04-27 17:43:09,603 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25
2019-04-27 17:43:09,613 INFO org.apache.hadoop.hdfs.DFSUtil: Starting Web-server for secondary at: http://0.0.0.0:50090
2019-04-27 17:43:09,683 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2019-04-27 17:43:09,690 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2019-04-27 17:43:09,695 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.secondary is not defined
2019-04-27 17:43:09,699 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2019-04-27 17:43:09,701 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context secondary
2019-04-27 17:43:09,701 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2019-04-27 17:43:09,701 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2019-04-27 17:43:09,724 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 50090
2019-04-27 17:43:09,724 INFO org.mortbay.log: jetty-6.1.26
2019-04-27 17:43:09,852 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50090
2019-04-27 17:43:09,852 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Web server init done
2019-04-27 17:43:09,856 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Checkpoint Period   :3600 secs (60 min)
2019-04-27 17:43:09,856 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Log Size Trigger    :1000000 txns
2019-04-27 17:44:10,002 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Image has changed. Downloading updated image from NN.
2019-04-27 17:44:10,126 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Opening connection to http://master:50070/imagetransfer?getimage=1&txid=23&storageInfo=-63:2027304554:0:CID-c6975e7b-5169-483f-89cf-bbcabdcf5460
2019-04-27 17:44:10,137 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Image Transfer timeout configured to 60000 milliseconds
2019-04-27 17:44:10,315 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Transfer took 0,00s at 0,00 KB/s
2019-04-27 17:44:10,315 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file fsimage.ckpt_0000000000000000023 size 351 bytes.
2019-04-27 17:44:10,323 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Opening connection to http://master:50070/imagetransfer?getedit=1&startTxId=24&endTxId=24&storageInfo=-63:2027304554:0:CID-c6975e7b-5169-483f-89cf-bbcabdcf5460
2019-04-27 17:44:10,334 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Transfer took 0,01s at 204800,00 KB/s
2019-04-27 17:44:10,334 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file edits_tmp_0000000000000000024-0000000000000000024_0000000000011527999 size 0 bytes.
2019-04-27 17:44:10,334 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Opening connection to http://master:50070/imagetransfer?getedit=1&startTxId=25&endTxId=26&storageInfo=-63:2027304554:0:CID-c6975e7b-5169-483f-89cf-bbcabdcf5460
2019-04-27 17:44:10,338 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Transfer took 0,00s at 0,00 KB/s
2019-04-27 17:44:10,338 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file edits_tmp_0000000000000000025-0000000000000000026_0000000000011528011 size 0 bytes.
2019-04-27 17:44:10,377 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatPBINode: Loading 1 INodes.
2019-04-27 17:44:10,413 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: Loaded FSImage in 0 seconds.
2019-04-27 17:44:10,413 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Loaded image for txid 23 from /tmp/hadoop-root/dfs/namesecondary/current/fsimage_0000000000000000023
2019-04-27 17:44:10,413 INFO org.apache.hadoop.hdfs.server.namenode.NameCache: initialized with 0 entries 0 lookups
2019-04-27 17:44:10,418 INFO org.apache.hadoop.hdfs.server.namenode.Checkpointer: Checkpointer about to load edits from 2 stream(s).
2019-04-27 17:44:10,422 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading /tmp/hadoop-root/dfs/namesecondary/current/edits_0000000000000000024-0000000000000000024 expecting start txid #24
2019-04-27 17:44:10,422 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file /tmp/hadoop-root/dfs/namesecondary/current/edits_0000000000000000024-0000000000000000024
2019-04-27 17:44:10,438 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file /tmp/hadoop-root/dfs/namesecondary/current/edits_0000000000000000024-0000000000000000024 of size 1048576 edits # 1 loaded in 0 seconds
2019-04-27 17:44:10,438 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading /tmp/hadoop-root/dfs/namesecondary/current/edits_0000000000000000025-0000000000000000026 expecting start txid #25
2019-04-27 17:44:10,438 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file /tmp/hadoop-root/dfs/namesecondary/current/edits_0000000000000000025-0000000000000000026
2019-04-27 17:44:10,438 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file /tmp/hadoop-root/dfs/namesecondary/current/edits_0000000000000000025-0000000000000000026 of size 42 edits # 2 loaded in 0 seconds
2019-04-27 17:44:10,444 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: Saving image file /tmp/hadoop-root/dfs/namesecondary/current/fsimage.ckpt_0000000000000000026 using no compression
2019-04-27 17:44:10,491 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: Image file /tmp/hadoop-root/dfs/namesecondary/current/fsimage.ckpt_0000000000000000026 of size 351 bytes saved in 0 seconds.
2019-04-27 17:44:10,497 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Going to retain 2 images with txid >= 23
2019-04-27 17:44:10,497 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Purging old image FSImageFile(file=/tmp/hadoop-root/dfs/namesecondary/current/fsimage_0000000000000000020, cpktTxId=0000000000000000020)
2019-04-27 17:44:10,526 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Uploaded image with txid 26 to namenode at http://master:50070 in 0.019 seconds
2019-04-27 17:44:10,526 WARN org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Checkpoint done. New Image Size: 351
2019-04-27 18:44:11,056 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Image has not changed. Will not download image.
2019-04-27 18:44:11,056 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Opening connection to http://master:50070/imagetransfer?getedit=1&startTxId=27&endTxId=28&storageInfo=-63:2027304554:0:CID-c6975e7b-5169-483f-89cf-bbcabdcf5460
2019-04-27 18:44:11,066 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Transfer took 0,01s at 0,00 KB/s
2019-04-27 18:44:11,066 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file edits_tmp_0000000000000000027-0000000000000000028_0000000000015128733 size 0 bytes.
2019-04-27 18:44:11,066 INFO org.apache.hadoop.hdfs.server.namenode.Checkpointer: Checkpointer about to load edits from 1 stream(s).
2019-04-27 18:44:11,066 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading /tmp/hadoop-root/dfs/namesecondary/current/edits_0000000000000000027-0000000000000000028 expecting start txid #27
2019-04-27 18:44:11,066 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file /tmp/hadoop-root/dfs/namesecondary/current/edits_0000000000000000027-0000000000000000028
2019-04-27 18:44:11,067 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file /tmp/hadoop-root/dfs/namesecondary/current/edits_0000000000000000027-0000000000000000028 of size 42 edits # 2 loaded in 0 seconds
2019-04-27 18:44:11,067 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: Saving image file /tmp/hadoop-root/dfs/namesecondary/current/fsimage.ckpt_0000000000000000028 using no compression
2019-04-27 18:44:11,081 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: Image file /tmp/hadoop-root/dfs/namesecondary/current/fsimage.ckpt_0000000000000000028 of size 351 bytes saved in 0 seconds.
2019-04-27 18:44:11,083 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Going to retain 2 images with txid >= 26
2019-04-27 18:44:11,083 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Purging old image FSImageFile(file=/tmp/hadoop-root/dfs/namesecondary/current/fsimage_0000000000000000023, cpktTxId=0000000000000000023)
2019-04-27 18:44:11,107 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Uploaded image with txid 28 to namenode at http://master:50070 in 0.02 seconds
2019-04-27 18:44:11,108 WARN org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Checkpoint done. New Image Size: 351
2019-04-27 19:01:49,464 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: RECEIVED SIGNAL 15: SIGTERM
2019-04-27 19:01:49,466 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down SecondaryNameNode at master/192.168.178.37
************************************************************/
2019-04-27 19:02:58,556 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting SecondaryNameNode
STARTUP_MSG:   host = master/192.168.178.37
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.7.3
STARTUP_MSG:   classpath = /home/hadoop-2.7.3/etc/hadoop:/home/hadoop-2.7.3/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/stax-api-1.0-2.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/activation-1.1.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/jersey-server-1.9.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/asm-3.2.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/log4j-1.2.17.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/jets3t-0.9.0.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/httpclient-4.2.5.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/httpcore-4.2.5.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/commons-lang-2.6.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/commons-configuration-1.6.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/commons-digester-1.8.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/avro-1.7.4.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/paranamer-2.3.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/commons-compress-1.4.1.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/xz-1.0.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/gson-2.2.4.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/hadoop-auth-2.7.3.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/zookeeper-3.4.6.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/netty-3.6.2.Final.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/curator-framework-2.7.1.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/curator-client-2.7.1.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/jsch-0.1.42.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/htrace-core-3.1.0-incubating.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/junit-4.11.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/hamcrest-core-1.3.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/mockito-all-1.8.5.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/hadoop-annotations-2.7.3.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/guava-11.0.2.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/jsr305-3.0.0.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/commons-cli-1.2.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/commons-math3-3.1.1.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/xmlenc-0.52.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/commons-httpclient-3.1.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/commons-logging-1.1.3.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/commons-codec-1.4.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/commons-io-2.4.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/commons-net-3.1.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/commons-collections-3.2.2.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/servlet-api-2.5.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/jetty-6.1.26.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/jetty-util-6.1.26.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/jsp-api-2.1.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/jersey-core-1.9.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/jersey-json-1.9.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/jettison-1.1.jar:/home/hadoop-2.7.3/share/hadoop/common/hadoop-common-2.7.3.jar:/home/hadoop-2.7.3/share/hadoop/common/hadoop-common-2.7.3-tests.jar:/home/hadoop-2.7.3/share/hadoop/common/hadoop-nfs-2.7.3.jar:/home/hadoop-2.7.3/share/hadoop/hdfs:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/guava-11.0.2.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-io-2.4.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/asm-3.2.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/htrace-core-3.1.0-incubating.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/hadoop-hdfs-2.7.3.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/hadoop-hdfs-2.7.3-tests.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/hadoop-hdfs-nfs-2.7.3.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/commons-lang-2.6.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/guava-11.0.2.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/commons-cli-1.2.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/log4j-1.2.17.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/activation-1.1.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/xz-1.0.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/servlet-api-2.5.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/commons-codec-1.4.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-core-1.9.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-client-1.9.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/guice-3.0.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/javax.inject-1.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/aopalliance-1.0.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/commons-io-2.4.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-server-1.9.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/asm-3.2.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-json-1.9.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/jettison-1.1.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/commons-collections-3.2.2.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/jetty-6.1.26.jar:/home/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-api-2.7.3.jar:/home/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-common-2.7.3.jar:/home/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-common-2.7.3.jar:/home/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.7.3.jar:/home/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.7.3.jar:/home/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.7.3.jar:/home/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.7.3.jar:/home/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-tests-2.7.3.jar:/home/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-client-2.7.3.jar:/home/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.7.3.jar:/home/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.3.jar:/home/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.7.3.jar:/home/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-registry-2.7.3.jar:/hpme/hadoop-2.7.3/share/hadoop/mapreduce/*:/home/hadoop-2.7.3/contrib/capacity-scheduler/*.jar:/home/hadoop-2.7.3/contrib/capacity-scheduler/*.jar:/home/hadoop-2.7.3/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r baa91f7c6bc9cb92be5982de4719c1c8af91ccff; compiled by 'root' on 2016-08-18T01:41Z
STARTUP_MSG:   java = 1.8.0_101
************************************************************/
2019-04-27 19:02:58,571 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: registered UNIX signal handlers for [TERM, HUP, INT]
2019-04-27 19:02:58,971 WARN org.apache.hadoop.util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2019-04-27 19:02:59,053 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2019-04-27 19:02:59,123 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2019-04-27 19:02:59,124 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: SecondaryNameNode metrics system started
2019-04-27 19:02:59,404 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /tmp/hadoop-root/dfs/namesecondary/in_use.lock acquired by nodename 12489@master
2019-04-27 19:02:59,442 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: No KeyProvider found.
2019-04-27 19:02:59,443 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsLock is fair:true
2019-04-27 19:02:59,487 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.block.invalidate.limit=1000
2019-04-27 19:02:59,487 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true
2019-04-27 19:02:59,488 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
2019-04-27 19:02:59,490 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: The block deletion will start around 2019 Apr 27 19:02:59
2019-04-27 19:02:59,493 INFO org.apache.hadoop.util.GSet: Computing capacity for map BlocksMap
2019-04-27 19:02:59,493 INFO org.apache.hadoop.util.GSet: VM type       = 32-bit
2019-04-27 19:02:59,494 INFO org.apache.hadoop.util.GSet: 2.0% max memory 966.7 MB = 19.3 MB
2019-04-27 19:02:59,494 INFO org.apache.hadoop.util.GSet: capacity      = 2^22 = 4194304 entries
2019-04-27 19:02:59,524 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.block.access.token.enable=false
2019-04-27 19:02:59,524 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: defaultReplication         = 2
2019-04-27 19:02:59,524 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplication             = 512
2019-04-27 19:02:59,524 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: minReplication             = 1
2019-04-27 19:02:59,524 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplicationStreams      = 2
2019-04-27 19:02:59,524 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: replicationRecheckInterval = 3000
2019-04-27 19:02:59,524 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: encryptDataTransfer        = false
2019-04-27 19:02:59,524 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxNumBlocksToLog          = 1000
2019-04-27 19:02:59,526 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsOwner             = root (auth:SIMPLE)
2019-04-27 19:02:59,526 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: supergroup          = supergroup
2019-04-27 19:02:59,527 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: isPermissionEnabled = false
2019-04-27 19:02:59,527 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: HA Enabled: false
2019-04-27 19:02:59,529 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Append Enabled: true
2019-04-27 19:02:59,705 INFO org.apache.hadoop.util.GSet: Computing capacity for map INodeMap
2019-04-27 19:02:59,705 INFO org.apache.hadoop.util.GSet: VM type       = 32-bit
2019-04-27 19:02:59,705 INFO org.apache.hadoop.util.GSet: 1.0% max memory 966.7 MB = 9.7 MB
2019-04-27 19:02:59,706 INFO org.apache.hadoop.util.GSet: capacity      = 2^21 = 2097152 entries
2019-04-27 19:02:59,727 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: ACLs enabled? false
2019-04-27 19:02:59,727 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: XAttrs enabled? true
2019-04-27 19:02:59,727 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: Maximum size of an xattr: 16384
2019-04-27 19:02:59,727 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Caching file names occuring more than 10 times
2019-04-27 19:02:59,734 INFO org.apache.hadoop.util.GSet: Computing capacity for map cachedBlocks
2019-04-27 19:02:59,734 INFO org.apache.hadoop.util.GSet: VM type       = 32-bit
2019-04-27 19:02:59,734 INFO org.apache.hadoop.util.GSet: 0.25% max memory 966.7 MB = 2.4 MB
2019-04-27 19:02:59,734 INFO org.apache.hadoop.util.GSet: capacity      = 2^19 = 524288 entries
2019-04-27 19:02:59,736 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.threshold-pct = 0.9990000128746033
2019-04-27 19:02:59,736 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.min.datanodes = 0
2019-04-27 19:02:59,736 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.extension     = 30000
2019-04-27 19:02:59,740 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.window.num.buckets = 10
2019-04-27 19:02:59,740 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.num.users = 10
2019-04-27 19:02:59,740 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25
2019-04-27 19:02:59,751 INFO org.apache.hadoop.hdfs.DFSUtil: Starting Web-server for secondary at: http://0.0.0.0:50090
2019-04-27 19:02:59,814 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2019-04-27 19:02:59,820 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2019-04-27 19:02:59,825 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.secondary is not defined
2019-04-27 19:02:59,830 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2019-04-27 19:02:59,831 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context secondary
2019-04-27 19:02:59,831 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2019-04-27 19:02:59,831 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2019-04-27 19:02:59,855 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 50090
2019-04-27 19:02:59,855 INFO org.mortbay.log: jetty-6.1.26
2019-04-27 19:02:59,978 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50090
2019-04-27 19:02:59,978 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Web server init done
2019-04-27 19:02:59,982 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Checkpoint Period   :3600 secs (60 min)
2019-04-27 19:02:59,982 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Log Size Trigger    :1000000 txns
2019-04-27 19:04:00,626 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Image has changed. Downloading updated image from NN.
2019-04-27 19:04:01,149 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Opening connection to http://master:50070/imagetransfer?getimage=1&txid=28&storageInfo=-63:2027304554:0:CID-c6975e7b-5169-483f-89cf-bbcabdcf5460
2019-04-27 19:04:01,187 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Image Transfer timeout configured to 60000 milliseconds
2019-04-27 19:04:01,901 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Transfer took 0,06s at 0,00 KB/s
2019-04-27 19:04:01,901 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file fsimage.ckpt_0000000000000000028 size 351 bytes.
2019-04-27 19:04:01,915 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Opening connection to http://master:50070/imagetransfer?getedit=1&startTxId=29&endTxId=29&storageInfo=-63:2027304554:0:CID-c6975e7b-5169-483f-89cf-bbcabdcf5460
2019-04-27 19:04:01,950 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Transfer took 0,01s at 85333,33 KB/s
2019-04-27 19:04:01,950 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file edits_tmp_0000000000000000029-0000000000000000029_0000000000016319591 size 0 bytes.
2019-04-27 19:04:01,951 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Opening connection to http://master:50070/imagetransfer?getedit=1&startTxId=30&endTxId=31&storageInfo=-63:2027304554:0:CID-c6975e7b-5169-483f-89cf-bbcabdcf5460
2019-04-27 19:04:01,987 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Transfer took 0,03s at 0,00 KB/s
2019-04-27 19:04:01,987 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file edits_tmp_0000000000000000030-0000000000000000031_0000000000016319628 size 0 bytes.
2019-04-27 19:04:02,070 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatPBINode: Loading 1 INodes.
2019-04-27 19:04:02,243 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: Loaded FSImage in 0 seconds.
2019-04-27 19:04:02,243 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Loaded image for txid 28 from /tmp/hadoop-root/dfs/namesecondary/current/fsimage_0000000000000000028
2019-04-27 19:04:02,243 INFO org.apache.hadoop.hdfs.server.namenode.NameCache: initialized with 0 entries 0 lookups
2019-04-27 19:04:02,248 INFO org.apache.hadoop.hdfs.server.namenode.Checkpointer: Checkpointer about to load edits from 2 stream(s).
2019-04-27 19:04:02,253 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading /tmp/hadoop-root/dfs/namesecondary/current/edits_0000000000000000029-0000000000000000029 expecting start txid #29
2019-04-27 19:04:02,253 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file /tmp/hadoop-root/dfs/namesecondary/current/edits_0000000000000000029-0000000000000000029
2019-04-27 19:04:02,371 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file /tmp/hadoop-root/dfs/namesecondary/current/edits_0000000000000000029-0000000000000000029 of size 1048576 edits # 1 loaded in 0 seconds
2019-04-27 19:04:02,371 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading /tmp/hadoop-root/dfs/namesecondary/current/edits_0000000000000000030-0000000000000000031 expecting start txid #30
2019-04-27 19:04:02,371 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file /tmp/hadoop-root/dfs/namesecondary/current/edits_0000000000000000030-0000000000000000031
2019-04-27 19:04:02,371 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file /tmp/hadoop-root/dfs/namesecondary/current/edits_0000000000000000030-0000000000000000031 of size 42 edits # 2 loaded in 0 seconds
2019-04-27 19:04:02,402 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: Saving image file /tmp/hadoop-root/dfs/namesecondary/current/fsimage.ckpt_0000000000000000031 using no compression
2019-04-27 19:04:02,530 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: Image file /tmp/hadoop-root/dfs/namesecondary/current/fsimage.ckpt_0000000000000000031 of size 351 bytes saved in 0 seconds.
2019-04-27 19:04:02,538 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Going to retain 2 images with txid >= 28
2019-04-27 19:04:02,538 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Purging old image FSImageFile(file=/tmp/hadoop-root/dfs/namesecondary/current/fsimage_0000000000000000026, cpktTxId=0000000000000000026)
2019-04-27 19:04:02,785 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Uploaded image with txid 31 to namenode at http://master:50070 in 0.151 seconds
2019-04-27 19:04:02,786 WARN org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Checkpoint done. New Image Size: 351
2019-04-27 19:08:03,883 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master/192.168.178.37:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-04-27 19:08:04,888 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master/192.168.178.37:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-04-27 19:08:05,894 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master/192.168.178.37:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-04-27 19:08:06,897 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master/192.168.178.37:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-04-27 19:08:07,898 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master/192.168.178.37:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-04-27 19:08:08,899 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master/192.168.178.37:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-04-27 19:08:09,901 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master/192.168.178.37:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-04-27 19:08:10,903 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master/192.168.178.37:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-04-27 19:08:11,910 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master/192.168.178.37:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-04-27 19:08:12,913 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master/192.168.178.37:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-04-27 19:08:12,918 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.net.ConnectException: Call From master/192.168.178.37 to master:9000 failed on connection exception: java.net.ConnectException: Verbindungsaufbau abgelehnt; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1479)
	at org.apache.hadoop.ipc.Client.call(Client.java:1412)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy10.getTransactionId(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolTranslatorPB.getTransactionID(NamenodeProtocolTranslatorPB.java:128)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:191)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy11.getTransactionID(Unknown Source)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.countUncheckpointedTxns(SecondaryNameNode.java:641)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.shouldCheckpointBasedOnCount(SecondaryNameNode.java:649)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:393)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Verbindungsaufbau abgelehnt
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:614)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:712)
	at org.apache.hadoop.ipc.Client$Connection.access$2900(Client.java:375)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1528)
	at org.apache.hadoop.ipc.Client.call(Client.java:1451)
	... 18 more
2019-04-27 19:09:13,928 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master/192.168.178.37:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-04-27 19:09:14,931 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master/192.168.178.37:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-04-27 19:09:15,940 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master/192.168.178.37:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-04-27 19:09:16,950 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master/192.168.178.37:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-04-27 19:09:17,951 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master/192.168.178.37:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-04-27 19:09:18,957 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master/192.168.178.37:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-04-27 19:09:19,959 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master/192.168.178.37:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-04-27 19:09:20,971 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master/192.168.178.37:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-04-27 19:09:21,984 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master/192.168.178.37:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-04-27 19:09:22,985 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master/192.168.178.37:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-04-27 19:09:22,986 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.net.ConnectException: Call From master/192.168.178.37 to master:9000 failed on connection exception: java.net.ConnectException: Verbindungsaufbau abgelehnt; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1479)
	at org.apache.hadoop.ipc.Client.call(Client.java:1412)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy10.getTransactionId(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolTranslatorPB.getTransactionID(NamenodeProtocolTranslatorPB.java:128)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:191)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy11.getTransactionID(Unknown Source)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.countUncheckpointedTxns(SecondaryNameNode.java:641)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.shouldCheckpointBasedOnCount(SecondaryNameNode.java:649)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:393)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Verbindungsaufbau abgelehnt
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:614)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:712)
	at org.apache.hadoop.ipc.Client$Connection.access$2900(Client.java:375)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1528)
	at org.apache.hadoop.ipc.Client.call(Client.java:1451)
	... 18 more
2019-04-27 19:10:23,996 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master/192.168.178.37:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-04-27 19:10:24,998 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master/192.168.178.37:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-04-27 19:10:26,003 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master/192.168.178.37:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-04-27 19:10:27,005 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master/192.168.178.37:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-04-27 19:10:28,006 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master/192.168.178.37:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-04-27 19:10:29,009 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master/192.168.178.37:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-04-27 19:10:30,013 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master/192.168.178.37:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-04-27 19:10:31,018 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master/192.168.178.37:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-04-27 19:10:32,028 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master/192.168.178.37:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-04-27 19:10:33,035 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master/192.168.178.37:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-04-27 19:10:33,035 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.net.ConnectException: Call From master/192.168.178.37 to master:9000 failed on connection exception: java.net.ConnectException: Verbindungsaufbau abgelehnt; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1479)
	at org.apache.hadoop.ipc.Client.call(Client.java:1412)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy10.getTransactionId(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolTranslatorPB.getTransactionID(NamenodeProtocolTranslatorPB.java:128)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:191)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy11.getTransactionID(Unknown Source)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.countUncheckpointedTxns(SecondaryNameNode.java:641)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.shouldCheckpointBasedOnCount(SecondaryNameNode.java:649)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:393)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Verbindungsaufbau abgelehnt
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:614)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:712)
	at org.apache.hadoop.ipc.Client$Connection.access$2900(Client.java:375)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1528)
	at org.apache.hadoop.ipc.Client.call(Client.java:1451)
	... 18 more
2019-04-27 19:11:34,043 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master/192.168.178.37:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-04-27 19:11:35,047 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master/192.168.178.37:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-04-27 19:11:36,049 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master/192.168.178.37:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-04-27 19:11:37,053 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master/192.168.178.37:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-04-27 19:11:38,063 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master/192.168.178.37:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-04-27 19:11:39,068 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master/192.168.178.37:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-04-27 19:11:40,077 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master/192.168.178.37:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-04-27 19:11:41,086 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master/192.168.178.37:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-04-27 19:11:42,095 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master/192.168.178.37:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-04-27 19:11:43,100 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master/192.168.178.37:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-04-27 19:11:43,101 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.net.ConnectException: Call From master/192.168.178.37 to master:9000 failed on connection exception: java.net.ConnectException: Verbindungsaufbau abgelehnt; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1479)
	at org.apache.hadoop.ipc.Client.call(Client.java:1412)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy10.getTransactionId(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolTranslatorPB.getTransactionID(NamenodeProtocolTranslatorPB.java:128)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:191)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy11.getTransactionID(Unknown Source)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.countUncheckpointedTxns(SecondaryNameNode.java:641)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.shouldCheckpointBasedOnCount(SecondaryNameNode.java:649)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:393)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Verbindungsaufbau abgelehnt
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:614)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:712)
	at org.apache.hadoop.ipc.Client$Connection.access$2900(Client.java:375)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1528)
	at org.apache.hadoop.ipc.Client.call(Client.java:1451)
	... 18 more
2019-04-27 19:12:44,106 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master/192.168.178.37:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-04-27 19:12:45,108 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master/192.168.178.37:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-04-27 19:12:46,112 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master/192.168.178.37:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-04-27 19:12:47,117 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master/192.168.178.37:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-04-27 19:12:48,121 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master/192.168.178.37:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-04-27 19:12:49,129 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master/192.168.178.37:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-04-27 19:12:50,134 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master/192.168.178.37:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-04-27 19:12:51,144 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master/192.168.178.37:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-04-27 19:12:52,155 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master/192.168.178.37:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-04-27 19:12:53,156 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master/192.168.178.37:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-04-27 19:12:53,157 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.net.ConnectException: Call From master/192.168.178.37 to master:9000 failed on connection exception: java.net.ConnectException: Verbindungsaufbau abgelehnt; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1479)
	at org.apache.hadoop.ipc.Client.call(Client.java:1412)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy10.getTransactionId(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolTranslatorPB.getTransactionID(NamenodeProtocolTranslatorPB.java:128)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:191)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy11.getTransactionID(Unknown Source)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.countUncheckpointedTxns(SecondaryNameNode.java:641)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.shouldCheckpointBasedOnCount(SecondaryNameNode.java:649)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:393)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Verbindungsaufbau abgelehnt
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:614)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:712)
	at org.apache.hadoop.ipc.Client$Connection.access$2900(Client.java:375)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1528)
	at org.apache.hadoop.ipc.Client.call(Client.java:1451)
	... 18 more
2019-04-27 19:13:54,177 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master/192.168.178.37:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-04-27 19:13:55,188 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master/192.168.178.37:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-04-27 19:13:56,197 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master/192.168.178.37:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-04-27 19:13:57,206 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master/192.168.178.37:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-04-27 19:13:58,220 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master/192.168.178.37:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-04-27 19:13:59,225 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master/192.168.178.37:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-04-27 19:14:00,228 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master/192.168.178.37:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-04-27 19:14:01,239 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master/192.168.178.37:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-04-27 19:14:02,248 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master/192.168.178.37:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-04-27 19:14:03,261 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master/192.168.178.37:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-04-27 19:14:03,262 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.net.ConnectException: Call From master/192.168.178.37 to master:9000 failed on connection exception: java.net.ConnectException: Verbindungsaufbau abgelehnt; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1479)
	at org.apache.hadoop.ipc.Client.call(Client.java:1412)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy10.getTransactionId(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolTranslatorPB.getTransactionID(NamenodeProtocolTranslatorPB.java:128)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:191)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy11.getTransactionID(Unknown Source)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.countUncheckpointedTxns(SecondaryNameNode.java:641)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.shouldCheckpointBasedOnCount(SecondaryNameNode.java:649)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:393)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Verbindungsaufbau abgelehnt
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:614)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:712)
	at org.apache.hadoop.ipc.Client$Connection.access$2900(Client.java:375)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1528)
	at org.apache.hadoop.ipc.Client.call(Client.java:1451)
	... 18 more
2019-04-27 19:14:28,054 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: RECEIVED SIGNAL 15: SIGTERM
2019-04-27 19:14:28,056 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down SecondaryNameNode at master/192.168.178.37
************************************************************/
2019-04-27 19:50:28,132 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting SecondaryNameNode
STARTUP_MSG:   host = master/192.168.178.37
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.7.3
STARTUP_MSG:   classpath = /home/hadoop-2.7.3/etc/hadoop:/home/hadoop-2.7.3/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/stax-api-1.0-2.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/activation-1.1.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/jersey-server-1.9.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/asm-3.2.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/log4j-1.2.17.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/jets3t-0.9.0.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/httpclient-4.2.5.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/httpcore-4.2.5.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/commons-lang-2.6.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/commons-configuration-1.6.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/commons-digester-1.8.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/avro-1.7.4.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/paranamer-2.3.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/commons-compress-1.4.1.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/xz-1.0.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/gson-2.2.4.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/hadoop-auth-2.7.3.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/zookeeper-3.4.6.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/netty-3.6.2.Final.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/curator-framework-2.7.1.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/curator-client-2.7.1.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/jsch-0.1.42.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/htrace-core-3.1.0-incubating.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/junit-4.11.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/hamcrest-core-1.3.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/mockito-all-1.8.5.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/hadoop-annotations-2.7.3.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/guava-11.0.2.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/jsr305-3.0.0.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/commons-cli-1.2.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/commons-math3-3.1.1.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/xmlenc-0.52.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/commons-httpclient-3.1.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/commons-logging-1.1.3.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/commons-codec-1.4.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/commons-io-2.4.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/commons-net-3.1.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/commons-collections-3.2.2.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/servlet-api-2.5.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/jetty-6.1.26.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/jetty-util-6.1.26.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/jsp-api-2.1.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/jersey-core-1.9.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/jersey-json-1.9.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/jettison-1.1.jar:/home/hadoop-2.7.3/share/hadoop/common/hadoop-common-2.7.3.jar:/home/hadoop-2.7.3/share/hadoop/common/hadoop-common-2.7.3-tests.jar:/home/hadoop-2.7.3/share/hadoop/common/hadoop-nfs-2.7.3.jar:/home/hadoop-2.7.3/share/hadoop/hdfs:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/guava-11.0.2.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-io-2.4.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/asm-3.2.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/htrace-core-3.1.0-incubating.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/hadoop-hdfs-2.7.3.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/hadoop-hdfs-2.7.3-tests.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/hadoop-hdfs-nfs-2.7.3.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/commons-lang-2.6.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/guava-11.0.2.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/commons-cli-1.2.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/log4j-1.2.17.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/activation-1.1.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/xz-1.0.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/servlet-api-2.5.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/commons-codec-1.4.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-core-1.9.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-client-1.9.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/guice-3.0.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/javax.inject-1.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/aopalliance-1.0.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/commons-io-2.4.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-server-1.9.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/asm-3.2.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-json-1.9.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/jettison-1.1.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/commons-collections-3.2.2.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/jetty-6.1.26.jar:/home/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-api-2.7.3.jar:/home/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-common-2.7.3.jar:/home/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-common-2.7.3.jar:/home/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.7.3.jar:/home/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.7.3.jar:/home/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.7.3.jar:/home/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.7.3.jar:/home/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-tests-2.7.3.jar:/home/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-client-2.7.3.jar:/home/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.7.3.jar:/home/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.3.jar:/home/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.7.3.jar:/home/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-registry-2.7.3.jar:/hpme/hadoop-2.7.3/share/hadoop/mapreduce/*:/home/hadoop-2.7.3/contrib/capacity-scheduler/*.jar:/home/hadoop-2.7.3/contrib/capacity-scheduler/*.jar:/home/hadoop-2.7.3/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r baa91f7c6bc9cb92be5982de4719c1c8af91ccff; compiled by 'root' on 2016-08-18T01:41Z
STARTUP_MSG:   java = 1.8.0_101
************************************************************/
2019-04-27 19:50:28,144 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: registered UNIX signal handlers for [TERM, HUP, INT]
2019-04-27 19:50:28,604 WARN org.apache.hadoop.util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2019-04-27 19:50:28,683 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2019-04-27 19:50:28,743 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2019-04-27 19:50:28,743 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: SecondaryNameNode metrics system started
2019-04-27 19:50:28,968 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /tmp/hadoop-root/dfs/namesecondary/in_use.lock acquired by nodename 6087@master
2019-04-27 19:50:29,004 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: No KeyProvider found.
2019-04-27 19:50:29,004 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsLock is fair:true
2019-04-27 19:50:29,049 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.block.invalidate.limit=1000
2019-04-27 19:50:29,049 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true
2019-04-27 19:50:29,051 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
2019-04-27 19:50:29,053 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: The block deletion will start around 2019 Apr 27 19:50:29
2019-04-27 19:50:29,055 INFO org.apache.hadoop.util.GSet: Computing capacity for map BlocksMap
2019-04-27 19:50:29,055 INFO org.apache.hadoop.util.GSet: VM type       = 32-bit
2019-04-27 19:50:29,056 INFO org.apache.hadoop.util.GSet: 2.0% max memory 966.7 MB = 19.3 MB
2019-04-27 19:50:29,056 INFO org.apache.hadoop.util.GSet: capacity      = 2^22 = 4194304 entries
2019-04-27 19:50:29,082 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.block.access.token.enable=false
2019-04-27 19:50:29,082 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: defaultReplication         = 2
2019-04-27 19:50:29,082 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplication             = 512
2019-04-27 19:50:29,082 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: minReplication             = 1
2019-04-27 19:50:29,082 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplicationStreams      = 2
2019-04-27 19:50:29,082 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: replicationRecheckInterval = 3000
2019-04-27 19:50:29,082 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: encryptDataTransfer        = false
2019-04-27 19:50:29,082 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxNumBlocksToLog          = 1000
2019-04-27 19:50:29,083 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsOwner             = root (auth:SIMPLE)
2019-04-27 19:50:29,083 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: supergroup          = supergroup
2019-04-27 19:50:29,083 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: isPermissionEnabled = false
2019-04-27 19:50:29,083 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: HA Enabled: false
2019-04-27 19:50:29,085 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Append Enabled: true
2019-04-27 19:50:29,258 INFO org.apache.hadoop.util.GSet: Computing capacity for map INodeMap
2019-04-27 19:50:29,258 INFO org.apache.hadoop.util.GSet: VM type       = 32-bit
2019-04-27 19:50:29,258 INFO org.apache.hadoop.util.GSet: 1.0% max memory 966.7 MB = 9.7 MB
2019-04-27 19:50:29,258 INFO org.apache.hadoop.util.GSet: capacity      = 2^21 = 2097152 entries
2019-04-27 19:50:29,278 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: ACLs enabled? false
2019-04-27 19:50:29,278 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: XAttrs enabled? true
2019-04-27 19:50:29,278 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: Maximum size of an xattr: 16384
2019-04-27 19:50:29,278 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Caching file names occuring more than 10 times
2019-04-27 19:50:29,285 INFO org.apache.hadoop.util.GSet: Computing capacity for map cachedBlocks
2019-04-27 19:50:29,285 INFO org.apache.hadoop.util.GSet: VM type       = 32-bit
2019-04-27 19:50:29,285 INFO org.apache.hadoop.util.GSet: 0.25% max memory 966.7 MB = 2.4 MB
2019-04-27 19:50:29,285 INFO org.apache.hadoop.util.GSet: capacity      = 2^19 = 524288 entries
2019-04-27 19:50:29,290 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.threshold-pct = 0.9990000128746033
2019-04-27 19:50:29,290 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.min.datanodes = 0
2019-04-27 19:50:29,290 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.extension     = 30000
2019-04-27 19:50:29,292 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.window.num.buckets = 10
2019-04-27 19:50:29,292 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.num.users = 10
2019-04-27 19:50:29,292 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25
2019-04-27 19:50:29,303 INFO org.apache.hadoop.hdfs.DFSUtil: Starting Web-server for secondary at: http://0.0.0.0:50090
2019-04-27 19:50:29,361 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2019-04-27 19:50:29,367 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2019-04-27 19:50:29,372 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.secondary is not defined
2019-04-27 19:50:29,376 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2019-04-27 19:50:29,377 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context secondary
2019-04-27 19:50:29,377 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2019-04-27 19:50:29,377 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2019-04-27 19:50:29,399 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 50090
2019-04-27 19:50:29,399 INFO org.mortbay.log: jetty-6.1.26
2019-04-27 19:50:29,519 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50090
2019-04-27 19:50:29,519 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Web server init done
2019-04-27 19:50:29,524 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Checkpoint Period   :3600 secs (60 min)
2019-04-27 19:50:29,524 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Log Size Trigger    :1000000 txns
2019-04-27 19:54:30,608 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master/192.168.178.37:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-04-27 19:54:31,609 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master/192.168.178.37:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-04-27 19:54:32,611 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master/192.168.178.37:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-04-27 19:54:33,612 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master/192.168.178.37:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-04-27 19:54:34,613 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master/192.168.178.37:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-04-27 19:54:35,615 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master/192.168.178.37:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-04-27 19:54:36,616 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master/192.168.178.37:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-04-27 19:54:37,617 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master/192.168.178.37:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-04-27 19:54:38,618 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master/192.168.178.37:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-04-27 19:54:39,621 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master/192.168.178.37:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-04-27 19:54:39,625 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.net.ConnectException: Call From master/192.168.178.37 to master:9000 failed on connection exception: java.net.ConnectException: Verbindungsaufbau abgelehnt; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1479)
	at org.apache.hadoop.ipc.Client.call(Client.java:1412)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy10.getTransactionId(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolTranslatorPB.getTransactionID(NamenodeProtocolTranslatorPB.java:128)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:191)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy11.getTransactionID(Unknown Source)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.countUncheckpointedTxns(SecondaryNameNode.java:641)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.shouldCheckpointBasedOnCount(SecondaryNameNode.java:649)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:393)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Verbindungsaufbau abgelehnt
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:614)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:712)
	at org.apache.hadoop.ipc.Client$Connection.access$2900(Client.java:375)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1528)
	at org.apache.hadoop.ipc.Client.call(Client.java:1451)
	... 18 more
2019-04-27 19:55:01,605 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: RECEIVED SIGNAL 15: SIGTERM
2019-04-27 19:55:01,620 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down SecondaryNameNode at master/192.168.178.37
************************************************************/
2019-04-27 20:01:13,155 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting SecondaryNameNode
STARTUP_MSG:   host = master/192.168.178.37
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.7.3
STARTUP_MSG:   classpath = /home/hadoop-2.7.3/etc/hadoop:/home/hadoop-2.7.3/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/stax-api-1.0-2.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/activation-1.1.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/jersey-server-1.9.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/asm-3.2.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/log4j-1.2.17.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/jets3t-0.9.0.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/httpclient-4.2.5.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/httpcore-4.2.5.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/commons-lang-2.6.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/commons-configuration-1.6.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/commons-digester-1.8.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/avro-1.7.4.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/paranamer-2.3.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/commons-compress-1.4.1.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/xz-1.0.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/gson-2.2.4.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/hadoop-auth-2.7.3.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/zookeeper-3.4.6.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/netty-3.6.2.Final.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/curator-framework-2.7.1.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/curator-client-2.7.1.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/jsch-0.1.42.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/htrace-core-3.1.0-incubating.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/junit-4.11.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/hamcrest-core-1.3.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/mockito-all-1.8.5.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/hadoop-annotations-2.7.3.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/guava-11.0.2.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/jsr305-3.0.0.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/commons-cli-1.2.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/commons-math3-3.1.1.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/xmlenc-0.52.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/commons-httpclient-3.1.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/commons-logging-1.1.3.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/commons-codec-1.4.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/commons-io-2.4.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/commons-net-3.1.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/commons-collections-3.2.2.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/servlet-api-2.5.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/jetty-6.1.26.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/jetty-util-6.1.26.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/jsp-api-2.1.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/jersey-core-1.9.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/jersey-json-1.9.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/jettison-1.1.jar:/home/hadoop-2.7.3/share/hadoop/common/hadoop-common-2.7.3.jar:/home/hadoop-2.7.3/share/hadoop/common/hadoop-common-2.7.3-tests.jar:/home/hadoop-2.7.3/share/hadoop/common/hadoop-nfs-2.7.3.jar:/home/hadoop-2.7.3/share/hadoop/hdfs:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/guava-11.0.2.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-io-2.4.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/asm-3.2.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/htrace-core-3.1.0-incubating.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/hadoop-hdfs-2.7.3.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/hadoop-hdfs-2.7.3-tests.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/hadoop-hdfs-nfs-2.7.3.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/commons-lang-2.6.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/guava-11.0.2.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/commons-cli-1.2.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/log4j-1.2.17.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/activation-1.1.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/xz-1.0.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/servlet-api-2.5.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/commons-codec-1.4.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-core-1.9.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-client-1.9.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/guice-3.0.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/javax.inject-1.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/aopalliance-1.0.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/commons-io-2.4.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-server-1.9.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/asm-3.2.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-json-1.9.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/jettison-1.1.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/commons-collections-3.2.2.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/jetty-6.1.26.jar:/home/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-api-2.7.3.jar:/home/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-common-2.7.3.jar:/home/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-common-2.7.3.jar:/home/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.7.3.jar:/home/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.7.3.jar:/home/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.7.3.jar:/home/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.7.3.jar:/home/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-tests-2.7.3.jar:/home/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-client-2.7.3.jar:/home/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.7.3.jar:/home/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.3.jar:/home/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.7.3.jar:/home/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-registry-2.7.3.jar:/hpme/hadoop-2.7.3/share/hadoop/mapreduce/*:/home/hadoop-2.7.3/contrib/capacity-scheduler/*.jar:/home/hadoop-2.7.3/contrib/capacity-scheduler/*.jar:/home/hadoop-2.7.3/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r baa91f7c6bc9cb92be5982de4719c1c8af91ccff; compiled by 'root' on 2016-08-18T01:41Z
STARTUP_MSG:   java = 1.8.0_101
************************************************************/
2019-04-27 20:01:13,167 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: registered UNIX signal handlers for [TERM, HUP, INT]
2019-04-27 20:01:13,533 WARN org.apache.hadoop.util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2019-04-27 20:01:13,607 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2019-04-27 20:01:13,679 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2019-04-27 20:01:13,679 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: SecondaryNameNode metrics system started
2019-04-27 20:01:13,900 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /tmp/hadoop-root/dfs/namesecondary/in_use.lock acquired by nodename 6039@master
2019-04-27 20:01:13,935 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: No KeyProvider found.
2019-04-27 20:01:13,935 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsLock is fair:true
2019-04-27 20:01:13,974 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.block.invalidate.limit=1000
2019-04-27 20:01:13,974 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true
2019-04-27 20:01:13,975 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
2019-04-27 20:01:13,977 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: The block deletion will start around 2019 Apr 27 20:01:13
2019-04-27 20:01:13,979 INFO org.apache.hadoop.util.GSet: Computing capacity for map BlocksMap
2019-04-27 20:01:13,979 INFO org.apache.hadoop.util.GSet: VM type       = 32-bit
2019-04-27 20:01:13,980 INFO org.apache.hadoop.util.GSet: 2.0% max memory 966.7 MB = 19.3 MB
2019-04-27 20:01:13,980 INFO org.apache.hadoop.util.GSet: capacity      = 2^22 = 4194304 entries
2019-04-27 20:01:14,005 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.block.access.token.enable=false
2019-04-27 20:01:14,006 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: defaultReplication         = 2
2019-04-27 20:01:14,006 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplication             = 512
2019-04-27 20:01:14,006 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: minReplication             = 1
2019-04-27 20:01:14,006 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplicationStreams      = 2
2019-04-27 20:01:14,006 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: replicationRecheckInterval = 3000
2019-04-27 20:01:14,006 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: encryptDataTransfer        = false
2019-04-27 20:01:14,006 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxNumBlocksToLog          = 1000
2019-04-27 20:01:14,007 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsOwner             = root (auth:SIMPLE)
2019-04-27 20:01:14,007 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: supergroup          = supergroup
2019-04-27 20:01:14,007 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: isPermissionEnabled = false
2019-04-27 20:01:14,007 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: HA Enabled: false
2019-04-27 20:01:14,008 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Append Enabled: true
2019-04-27 20:01:14,158 INFO org.apache.hadoop.util.GSet: Computing capacity for map INodeMap
2019-04-27 20:01:14,158 INFO org.apache.hadoop.util.GSet: VM type       = 32-bit
2019-04-27 20:01:14,158 INFO org.apache.hadoop.util.GSet: 1.0% max memory 966.7 MB = 9.7 MB
2019-04-27 20:01:14,158 INFO org.apache.hadoop.util.GSet: capacity      = 2^21 = 2097152 entries
2019-04-27 20:01:14,177 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: ACLs enabled? false
2019-04-27 20:01:14,177 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: XAttrs enabled? true
2019-04-27 20:01:14,177 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: Maximum size of an xattr: 16384
2019-04-27 20:01:14,178 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Caching file names occuring more than 10 times
2019-04-27 20:01:14,184 INFO org.apache.hadoop.util.GSet: Computing capacity for map cachedBlocks
2019-04-27 20:01:14,184 INFO org.apache.hadoop.util.GSet: VM type       = 32-bit
2019-04-27 20:01:14,184 INFO org.apache.hadoop.util.GSet: 0.25% max memory 966.7 MB = 2.4 MB
2019-04-27 20:01:14,184 INFO org.apache.hadoop.util.GSet: capacity      = 2^19 = 524288 entries
2019-04-27 20:01:14,187 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.threshold-pct = 0.9990000128746033
2019-04-27 20:01:14,187 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.min.datanodes = 0
2019-04-27 20:01:14,187 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.extension     = 30000
2019-04-27 20:01:14,189 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.window.num.buckets = 10
2019-04-27 20:01:14,189 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.num.users = 10
2019-04-27 20:01:14,189 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25
2019-04-27 20:01:14,200 INFO org.apache.hadoop.hdfs.DFSUtil: Starting Web-server for secondary at: http://0.0.0.0:50090
2019-04-27 20:01:14,265 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2019-04-27 20:01:14,272 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2019-04-27 20:01:14,276 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.secondary is not defined
2019-04-27 20:01:14,281 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2019-04-27 20:01:14,282 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context secondary
2019-04-27 20:01:14,282 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2019-04-27 20:01:14,283 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2019-04-27 20:01:14,306 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 50090
2019-04-27 20:01:14,306 INFO org.mortbay.log: jetty-6.1.26
2019-04-27 20:01:14,424 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50090
2019-04-27 20:01:14,424 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Web server init done
2019-04-27 20:01:14,428 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Checkpoint Period   :3600 secs (60 min)
2019-04-27 20:01:14,428 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Log Size Trigger    :1000000 txns
2019-04-27 20:43:15,792 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master/192.168.178.37:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-04-27 20:43:16,798 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master/192.168.178.37:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-04-27 20:43:17,804 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master/192.168.178.37:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-04-27 20:43:18,808 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master/192.168.178.37:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-04-27 20:43:19,812 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master/192.168.178.37:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-04-27 20:43:20,819 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master/192.168.178.37:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-04-27 20:43:21,823 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master/192.168.178.37:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-04-27 20:43:22,827 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master/192.168.178.37:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-04-27 20:43:23,829 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master/192.168.178.37:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-04-27 20:43:24,836 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master/192.168.178.37:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-04-27 20:43:24,840 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.net.ConnectException: Call From master/192.168.178.37 to master:9000 failed on connection exception: java.net.ConnectException: Verbindungsaufbau abgelehnt; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1479)
	at org.apache.hadoop.ipc.Client.call(Client.java:1412)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy10.getTransactionId(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolTranslatorPB.getTransactionID(NamenodeProtocolTranslatorPB.java:128)
	at sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:191)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy11.getTransactionID(Unknown Source)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.countUncheckpointedTxns(SecondaryNameNode.java:641)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.shouldCheckpointBasedOnCount(SecondaryNameNode.java:649)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:393)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Verbindungsaufbau abgelehnt
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:614)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:712)
	at org.apache.hadoop.ipc.Client$Connection.access$2900(Client.java:375)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1528)
	at org.apache.hadoop.ipc.Client.call(Client.java:1451)
	... 17 more
2019-04-27 20:44:25,853 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master/192.168.178.37:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-04-27 20:44:26,856 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master/192.168.178.37:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-04-27 20:44:27,859 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master/192.168.178.37:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-04-27 20:44:28,862 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master/192.168.178.37:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-04-27 20:44:29,865 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master/192.168.178.37:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-04-27 20:44:30,868 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master/192.168.178.37:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-04-27 20:44:31,870 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master/192.168.178.37:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-04-27 20:44:32,872 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master/192.168.178.37:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-04-27 20:44:33,876 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master/192.168.178.37:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-04-27 20:44:34,880 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master/192.168.178.37:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-04-27 20:44:34,880 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.net.ConnectException: Call From master/192.168.178.37 to master:9000 failed on connection exception: java.net.ConnectException: Verbindungsaufbau abgelehnt; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1479)
	at org.apache.hadoop.ipc.Client.call(Client.java:1412)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy10.getTransactionId(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolTranslatorPB.getTransactionID(NamenodeProtocolTranslatorPB.java:128)
	at sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:191)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy11.getTransactionID(Unknown Source)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.countUncheckpointedTxns(SecondaryNameNode.java:641)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.shouldCheckpointBasedOnCount(SecondaryNameNode.java:649)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:393)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Verbindungsaufbau abgelehnt
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:614)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:712)
	at org.apache.hadoop.ipc.Client$Connection.access$2900(Client.java:375)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1528)
	at org.apache.hadoop.ipc.Client.call(Client.java:1451)
	... 17 more
2019-04-27 20:45:35,888 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master/192.168.178.37:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-04-27 20:45:36,891 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master/192.168.178.37:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-04-27 20:45:37,892 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master/192.168.178.37:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-04-27 20:45:38,896 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master/192.168.178.37:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-04-27 20:45:39,899 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master/192.168.178.37:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-04-27 20:45:40,902 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master/192.168.178.37:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-04-27 20:45:41,905 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master/192.168.178.37:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-04-27 20:45:42,908 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master/192.168.178.37:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-04-27 20:45:43,911 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master/192.168.178.37:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-04-27 20:45:44,915 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master/192.168.178.37:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-04-27 20:45:44,916 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.net.ConnectException: Call From master/192.168.178.37 to master:9000 failed on connection exception: java.net.ConnectException: Verbindungsaufbau abgelehnt; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1479)
	at org.apache.hadoop.ipc.Client.call(Client.java:1412)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy10.getTransactionId(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolTranslatorPB.getTransactionID(NamenodeProtocolTranslatorPB.java:128)
	at sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:191)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy11.getTransactionID(Unknown Source)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.countUncheckpointedTxns(SecondaryNameNode.java:641)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.shouldCheckpointBasedOnCount(SecondaryNameNode.java:649)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:393)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Verbindungsaufbau abgelehnt
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:614)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:712)
	at org.apache.hadoop.ipc.Client$Connection.access$2900(Client.java:375)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1528)
	at org.apache.hadoop.ipc.Client.call(Client.java:1451)
	... 17 more
2019-04-27 20:46:45,924 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master/192.168.178.37:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-04-27 20:46:46,927 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master/192.168.178.37:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-04-27 20:46:47,932 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master/192.168.178.37:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-04-27 20:46:48,935 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master/192.168.178.37:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-04-27 20:46:49,937 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master/192.168.178.37:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-04-27 20:46:50,938 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master/192.168.178.37:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-04-27 20:46:51,940 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master/192.168.178.37:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-04-27 20:46:52,942 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master/192.168.178.37:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-04-27 20:46:53,945 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master/192.168.178.37:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-04-27 20:46:54,947 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master/192.168.178.37:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-04-27 20:46:54,948 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.net.ConnectException: Call From master/192.168.178.37 to master:9000 failed on connection exception: java.net.ConnectException: Verbindungsaufbau abgelehnt; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1479)
	at org.apache.hadoop.ipc.Client.call(Client.java:1412)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy10.getTransactionId(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolTranslatorPB.getTransactionID(NamenodeProtocolTranslatorPB.java:128)
	at sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:191)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy11.getTransactionID(Unknown Source)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.countUncheckpointedTxns(SecondaryNameNode.java:641)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.shouldCheckpointBasedOnCount(SecondaryNameNode.java:649)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:393)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Verbindungsaufbau abgelehnt
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:614)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:712)
	at org.apache.hadoop.ipc.Client$Connection.access$2900(Client.java:375)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1528)
	at org.apache.hadoop.ipc.Client.call(Client.java:1451)
	... 17 more
2019-04-27 20:47:55,954 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master/192.168.178.37:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-04-27 20:47:56,958 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master/192.168.178.37:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-04-27 20:47:57,960 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master/192.168.178.37:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-04-27 20:47:58,964 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master/192.168.178.37:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-04-27 20:47:59,969 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master/192.168.178.37:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-04-27 20:48:00,971 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master/192.168.178.37:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-04-27 20:48:01,975 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master/192.168.178.37:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-04-27 20:48:02,978 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master/192.168.178.37:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-04-27 20:48:03,979 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master/192.168.178.37:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-04-27 20:48:04,984 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master/192.168.178.37:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-04-27 20:48:04,985 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.net.ConnectException: Call From master/192.168.178.37 to master:9000 failed on connection exception: java.net.ConnectException: Verbindungsaufbau abgelehnt; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1479)
	at org.apache.hadoop.ipc.Client.call(Client.java:1412)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy10.getTransactionId(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolTranslatorPB.getTransactionID(NamenodeProtocolTranslatorPB.java:128)
	at sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:191)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy11.getTransactionID(Unknown Source)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.countUncheckpointedTxns(SecondaryNameNode.java:641)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.shouldCheckpointBasedOnCount(SecondaryNameNode.java:649)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:393)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Verbindungsaufbau abgelehnt
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:614)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:712)
	at org.apache.hadoop.ipc.Client$Connection.access$2900(Client.java:375)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1528)
	at org.apache.hadoop.ipc.Client.call(Client.java:1451)
	... 17 more
2019-04-27 20:49:06,006 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master/192.168.178.37:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-04-27 20:49:07,011 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master/192.168.178.37:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-04-27 20:49:08,018 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master/192.168.178.37:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-04-27 20:49:09,021 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master/192.168.178.37:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-04-27 20:49:10,030 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master/192.168.178.37:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-04-27 20:49:11,031 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master/192.168.178.37:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-04-27 20:49:12,035 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master/192.168.178.37:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-04-27 20:49:13,045 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master/192.168.178.37:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-04-27 20:49:14,055 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master/192.168.178.37:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-04-27 20:49:15,060 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master/192.168.178.37:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-04-27 20:49:15,061 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.net.ConnectException: Call From master/192.168.178.37 to master:9000 failed on connection exception: java.net.ConnectException: Verbindungsaufbau abgelehnt; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1479)
	at org.apache.hadoop.ipc.Client.call(Client.java:1412)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy10.getTransactionId(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolTranslatorPB.getTransactionID(NamenodeProtocolTranslatorPB.java:128)
	at sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:191)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy11.getTransactionID(Unknown Source)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.countUncheckpointedTxns(SecondaryNameNode.java:641)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.shouldCheckpointBasedOnCount(SecondaryNameNode.java:649)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:393)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Verbindungsaufbau abgelehnt
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:614)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:712)
	at org.apache.hadoop.ipc.Client$Connection.access$2900(Client.java:375)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1528)
	at org.apache.hadoop.ipc.Client.call(Client.java:1451)
	... 17 more
2019-04-27 20:50:16,072 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master/192.168.178.37:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-04-27 20:50:17,078 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master/192.168.178.37:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-04-27 20:50:18,083 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master/192.168.178.37:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-04-27 20:50:19,089 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master/192.168.178.37:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-04-27 20:50:20,093 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master/192.168.178.37:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-04-27 20:50:21,098 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master/192.168.178.37:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-04-27 20:50:22,099 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master/192.168.178.37:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-04-27 20:50:23,107 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master/192.168.178.37:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-04-27 20:50:24,114 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master/192.168.178.37:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-04-27 20:50:25,123 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master/192.168.178.37:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-04-27 20:50:25,124 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.net.ConnectException: Call From master/192.168.178.37 to master:9000 failed on connection exception: java.net.ConnectException: Verbindungsaufbau abgelehnt; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1479)
	at org.apache.hadoop.ipc.Client.call(Client.java:1412)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy10.getTransactionId(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolTranslatorPB.getTransactionID(NamenodeProtocolTranslatorPB.java:128)
	at sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:191)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy11.getTransactionID(Unknown Source)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.countUncheckpointedTxns(SecondaryNameNode.java:641)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.shouldCheckpointBasedOnCount(SecondaryNameNode.java:649)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:393)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Verbindungsaufbau abgelehnt
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:614)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:712)
	at org.apache.hadoop.ipc.Client$Connection.access$2900(Client.java:375)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1528)
	at org.apache.hadoop.ipc.Client.call(Client.java:1451)
	... 17 more
2019-04-27 20:51:26,143 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master/192.168.178.37:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-04-27 20:51:27,148 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master/192.168.178.37:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-04-27 20:51:28,157 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master/192.168.178.37:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-04-27 20:51:29,165 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master/192.168.178.37:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-04-27 20:51:30,171 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master/192.168.178.37:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-04-27 20:51:31,182 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master/192.168.178.37:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-04-27 20:51:32,188 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master/192.168.178.37:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-04-27 20:51:33,195 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master/192.168.178.37:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-04-27 20:51:34,196 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master/192.168.178.37:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-04-27 20:51:35,197 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master/192.168.178.37:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-04-27 20:51:35,199 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.net.ConnectException: Call From master/192.168.178.37 to master:9000 failed on connection exception: java.net.ConnectException: Verbindungsaufbau abgelehnt; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1479)
	at org.apache.hadoop.ipc.Client.call(Client.java:1412)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy10.getTransactionId(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolTranslatorPB.getTransactionID(NamenodeProtocolTranslatorPB.java:128)
	at sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:191)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy11.getTransactionID(Unknown Source)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.countUncheckpointedTxns(SecondaryNameNode.java:641)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.shouldCheckpointBasedOnCount(SecondaryNameNode.java:649)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:393)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Verbindungsaufbau abgelehnt
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:614)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:712)
	at org.apache.hadoop.ipc.Client$Connection.access$2900(Client.java:375)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1528)
	at org.apache.hadoop.ipc.Client.call(Client.java:1451)
	... 17 more
2019-04-27 20:52:36,215 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master/192.168.178.37:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-04-27 20:52:37,225 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master/192.168.178.37:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-04-27 20:52:38,228 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master/192.168.178.37:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-04-27 20:52:39,230 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master/192.168.178.37:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-04-27 20:52:40,239 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master/192.168.178.37:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-04-27 20:52:41,248 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master/192.168.178.37:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-04-27 20:52:42,257 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master/192.168.178.37:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-04-27 20:52:43,266 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master/192.168.178.37:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-04-27 20:52:44,267 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master/192.168.178.37:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-04-27 20:52:45,268 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master/192.168.178.37:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-04-27 20:52:45,269 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.net.ConnectException: Call From master/192.168.178.37 to master:9000 failed on connection exception: java.net.ConnectException: Verbindungsaufbau abgelehnt; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1479)
	at org.apache.hadoop.ipc.Client.call(Client.java:1412)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy10.getTransactionId(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolTranslatorPB.getTransactionID(NamenodeProtocolTranslatorPB.java:128)
	at sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:191)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy11.getTransactionID(Unknown Source)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.countUncheckpointedTxns(SecondaryNameNode.java:641)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.shouldCheckpointBasedOnCount(SecondaryNameNode.java:649)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:393)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Verbindungsaufbau abgelehnt
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:614)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:712)
	at org.apache.hadoop.ipc.Client$Connection.access$2900(Client.java:375)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1528)
	at org.apache.hadoop.ipc.Client.call(Client.java:1451)
	... 17 more
2019-04-27 20:53:46,276 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master/192.168.178.37:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-04-27 20:53:47,280 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master/192.168.178.37:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-04-27 20:53:48,284 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master/192.168.178.37:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-04-27 20:53:49,293 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master/192.168.178.37:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-04-27 20:53:50,299 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master/192.168.178.37:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-04-27 20:53:51,306 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master/192.168.178.37:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-04-27 20:53:52,315 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master/192.168.178.37:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-04-27 20:53:53,324 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master/192.168.178.37:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-04-27 20:53:54,326 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master/192.168.178.37:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-04-27 20:53:55,328 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master/192.168.178.37:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-04-27 20:53:55,328 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.net.ConnectException: Call From master/192.168.178.37 to master:9000 failed on connection exception: java.net.ConnectException: Verbindungsaufbau abgelehnt; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1479)
	at org.apache.hadoop.ipc.Client.call(Client.java:1412)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy10.getTransactionId(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolTranslatorPB.getTransactionID(NamenodeProtocolTranslatorPB.java:128)
	at sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:191)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy11.getTransactionID(Unknown Source)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.countUncheckpointedTxns(SecondaryNameNode.java:641)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.shouldCheckpointBasedOnCount(SecondaryNameNode.java:649)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:393)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Verbindungsaufbau abgelehnt
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:614)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:712)
	at org.apache.hadoop.ipc.Client$Connection.access$2900(Client.java:375)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1528)
	at org.apache.hadoop.ipc.Client.call(Client.java:1451)
	... 17 more
2019-04-27 20:54:56,335 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master/192.168.178.37:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-04-27 20:54:57,338 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master/192.168.178.37:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-04-27 20:54:58,340 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master/192.168.178.37:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-04-27 20:54:59,348 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master/192.168.178.37:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-04-27 20:55:00,350 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master/192.168.178.37:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-04-27 20:55:01,354 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master/192.168.178.37:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-04-27 20:55:02,355 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master/192.168.178.37:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-04-27 20:55:03,358 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master/192.168.178.37:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-04-27 20:55:04,362 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master/192.168.178.37:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-04-27 20:55:05,364 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master/192.168.178.37:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-04-27 20:55:05,365 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.net.ConnectException: Call From master/192.168.178.37 to master:9000 failed on connection exception: java.net.ConnectException: Verbindungsaufbau abgelehnt; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1479)
	at org.apache.hadoop.ipc.Client.call(Client.java:1412)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy10.getTransactionId(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolTranslatorPB.getTransactionID(NamenodeProtocolTranslatorPB.java:128)
	at sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:191)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy11.getTransactionID(Unknown Source)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.countUncheckpointedTxns(SecondaryNameNode.java:641)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.shouldCheckpointBasedOnCount(SecondaryNameNode.java:649)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:393)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Verbindungsaufbau abgelehnt
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:614)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:712)
	at org.apache.hadoop.ipc.Client$Connection.access$2900(Client.java:375)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1528)
	at org.apache.hadoop.ipc.Client.call(Client.java:1451)
	... 17 more
2019-04-27 20:56:06,370 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master/192.168.178.37:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-04-27 20:56:07,373 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master/192.168.178.37:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-04-27 20:56:08,374 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master/192.168.178.37:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-04-27 20:56:09,376 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master/192.168.178.37:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-04-27 20:56:10,381 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master/192.168.178.37:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-04-27 20:56:11,384 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master/192.168.178.37:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-04-27 20:56:12,386 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master/192.168.178.37:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-04-27 20:56:13,389 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master/192.168.178.37:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-04-27 20:56:14,392 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master/192.168.178.37:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-04-27 20:56:15,394 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master/192.168.178.37:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-04-27 20:56:15,395 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.net.ConnectException: Call From master/192.168.178.37 to master:9000 failed on connection exception: java.net.ConnectException: Verbindungsaufbau abgelehnt; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1479)
	at org.apache.hadoop.ipc.Client.call(Client.java:1412)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy10.getTransactionId(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolTranslatorPB.getTransactionID(NamenodeProtocolTranslatorPB.java:128)
	at sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:191)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy11.getTransactionID(Unknown Source)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.countUncheckpointedTxns(SecondaryNameNode.java:641)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.shouldCheckpointBasedOnCount(SecondaryNameNode.java:649)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:393)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Verbindungsaufbau abgelehnt
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:614)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:712)
	at org.apache.hadoop.ipc.Client$Connection.access$2900(Client.java:375)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1528)
	at org.apache.hadoop.ipc.Client.call(Client.java:1451)
	... 17 more
2019-04-27 20:57:16,402 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master/192.168.178.37:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-04-27 20:57:17,407 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master/192.168.178.37:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-04-27 20:57:18,411 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master/192.168.178.37:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-04-27 20:57:19,415 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master/192.168.178.37:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-04-27 20:57:20,418 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master/192.168.178.37:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-04-27 20:57:21,420 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master/192.168.178.37:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-04-27 20:57:22,423 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master/192.168.178.37:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-04-27 20:57:23,431 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master/192.168.178.37:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-04-27 20:57:24,437 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master/192.168.178.37:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-04-27 20:57:25,440 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master/192.168.178.37:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-04-27 20:57:25,441 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.net.ConnectException: Call From master/192.168.178.37 to master:9000 failed on connection exception: java.net.ConnectException: Verbindungsaufbau abgelehnt; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1479)
	at org.apache.hadoop.ipc.Client.call(Client.java:1412)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy10.getTransactionId(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolTranslatorPB.getTransactionID(NamenodeProtocolTranslatorPB.java:128)
	at sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:191)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy11.getTransactionID(Unknown Source)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.countUncheckpointedTxns(SecondaryNameNode.java:641)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.shouldCheckpointBasedOnCount(SecondaryNameNode.java:649)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:393)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Verbindungsaufbau abgelehnt
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:614)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:712)
	at org.apache.hadoop.ipc.Client$Connection.access$2900(Client.java:375)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1528)
	at org.apache.hadoop.ipc.Client.call(Client.java:1451)
	... 17 more
2019-04-27 20:58:26,451 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master/192.168.178.37:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-04-27 20:58:27,457 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master/192.168.178.37:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-04-27 20:58:28,462 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master/192.168.178.37:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-04-27 20:58:29,465 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master/192.168.178.37:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-04-27 20:58:30,467 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master/192.168.178.37:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-04-27 20:58:31,473 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master/192.168.178.37:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-04-27 20:58:32,477 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master/192.168.178.37:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-04-27 20:58:33,479 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master/192.168.178.37:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-04-27 20:58:34,480 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master/192.168.178.37:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-04-27 20:58:35,482 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master/192.168.178.37:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-04-27 20:58:35,483 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.net.ConnectException: Call From master/192.168.178.37 to master:9000 failed on connection exception: java.net.ConnectException: Verbindungsaufbau abgelehnt; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1479)
	at org.apache.hadoop.ipc.Client.call(Client.java:1412)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy10.getTransactionId(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolTranslatorPB.getTransactionID(NamenodeProtocolTranslatorPB.java:128)
	at sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:191)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy11.getTransactionID(Unknown Source)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.countUncheckpointedTxns(SecondaryNameNode.java:641)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.shouldCheckpointBasedOnCount(SecondaryNameNode.java:649)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:393)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Verbindungsaufbau abgelehnt
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:614)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:712)
	at org.apache.hadoop.ipc.Client$Connection.access$2900(Client.java:375)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1528)
	at org.apache.hadoop.ipc.Client.call(Client.java:1451)
	... 17 more
2019-04-27 20:59:36,488 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master/192.168.178.37:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-04-27 20:59:37,488 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master/192.168.178.37:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-04-27 20:59:38,492 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master/192.168.178.37:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-04-27 20:59:39,495 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master/192.168.178.37:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-04-27 20:59:40,498 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master/192.168.178.37:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-04-27 20:59:41,499 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master/192.168.178.37:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-04-27 20:59:42,503 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master/192.168.178.37:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-04-27 20:59:43,513 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master/192.168.178.37:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-04-27 20:59:44,515 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master/192.168.178.37:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-04-27 20:59:45,518 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master/192.168.178.37:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-04-27 20:59:45,518 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.net.ConnectException: Call From master/192.168.178.37 to master:9000 failed on connection exception: java.net.ConnectException: Verbindungsaufbau abgelehnt; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1479)
	at org.apache.hadoop.ipc.Client.call(Client.java:1412)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy10.getTransactionId(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolTranslatorPB.getTransactionID(NamenodeProtocolTranslatorPB.java:128)
	at sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:191)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy11.getTransactionID(Unknown Source)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.countUncheckpointedTxns(SecondaryNameNode.java:641)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.shouldCheckpointBasedOnCount(SecondaryNameNode.java:649)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:393)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Verbindungsaufbau abgelehnt
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:614)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:712)
	at org.apache.hadoop.ipc.Client$Connection.access$2900(Client.java:375)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1528)
	at org.apache.hadoop.ipc.Client.call(Client.java:1451)
	... 17 more
2019-04-27 21:00:46,524 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master/192.168.178.37:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-04-27 21:00:47,531 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master/192.168.178.37:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-04-27 21:00:48,532 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master/192.168.178.37:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-04-27 21:00:49,535 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master/192.168.178.37:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-04-27 21:00:50,536 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master/192.168.178.37:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-04-27 21:00:51,537 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master/192.168.178.37:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-04-27 21:00:52,539 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master/192.168.178.37:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-04-27 21:00:53,542 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master/192.168.178.37:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-04-27 21:00:54,545 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master/192.168.178.37:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-04-27 21:00:55,548 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master/192.168.178.37:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-04-27 21:00:55,549 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.net.ConnectException: Call From master/192.168.178.37 to master:9000 failed on connection exception: java.net.ConnectException: Verbindungsaufbau abgelehnt; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1479)
	at org.apache.hadoop.ipc.Client.call(Client.java:1412)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy10.getTransactionId(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolTranslatorPB.getTransactionID(NamenodeProtocolTranslatorPB.java:128)
	at sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:191)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy11.getTransactionID(Unknown Source)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.countUncheckpointedTxns(SecondaryNameNode.java:641)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.shouldCheckpointBasedOnCount(SecondaryNameNode.java:649)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:393)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Verbindungsaufbau abgelehnt
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:614)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:712)
	at org.apache.hadoop.ipc.Client$Connection.access$2900(Client.java:375)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1528)
	at org.apache.hadoop.ipc.Client.call(Client.java:1451)
	... 17 more
2019-04-27 21:01:40,083 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: RECEIVED SIGNAL 15: SIGTERM
2019-04-27 21:01:40,083 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down SecondaryNameNode at master/192.168.178.37
************************************************************/
2019-04-29 18:05:45,577 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting SecondaryNameNode
STARTUP_MSG:   host = master/192.168.178.37
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.7.3
STARTUP_MSG:   classpath = /home/hadoop-2.7.3/etc/hadoop:/home/hadoop-2.7.3/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/stax-api-1.0-2.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/activation-1.1.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/jersey-server-1.9.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/asm-3.2.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/log4j-1.2.17.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/jets3t-0.9.0.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/httpclient-4.2.5.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/httpcore-4.2.5.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/commons-lang-2.6.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/commons-configuration-1.6.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/commons-digester-1.8.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/avro-1.7.4.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/paranamer-2.3.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/commons-compress-1.4.1.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/xz-1.0.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/gson-2.2.4.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/hadoop-auth-2.7.3.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/zookeeper-3.4.6.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/netty-3.6.2.Final.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/curator-framework-2.7.1.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/curator-client-2.7.1.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/jsch-0.1.42.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/htrace-core-3.1.0-incubating.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/junit-4.11.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/hamcrest-core-1.3.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/mockito-all-1.8.5.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/hadoop-annotations-2.7.3.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/guava-11.0.2.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/jsr305-3.0.0.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/commons-cli-1.2.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/commons-math3-3.1.1.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/xmlenc-0.52.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/commons-httpclient-3.1.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/commons-logging-1.1.3.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/commons-codec-1.4.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/commons-io-2.4.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/commons-net-3.1.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/commons-collections-3.2.2.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/servlet-api-2.5.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/jetty-6.1.26.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/jetty-util-6.1.26.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/jsp-api-2.1.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/jersey-core-1.9.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/jersey-json-1.9.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/jettison-1.1.jar:/home/hadoop-2.7.3/share/hadoop/common/hadoop-common-2.7.3.jar:/home/hadoop-2.7.3/share/hadoop/common/hadoop-common-2.7.3-tests.jar:/home/hadoop-2.7.3/share/hadoop/common/hadoop-nfs-2.7.3.jar:/home/hadoop-2.7.3/share/hadoop/hdfs:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/guava-11.0.2.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-io-2.4.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/asm-3.2.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/htrace-core-3.1.0-incubating.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/hadoop-hdfs-2.7.3.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/hadoop-hdfs-2.7.3-tests.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/hadoop-hdfs-nfs-2.7.3.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/commons-lang-2.6.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/guava-11.0.2.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/commons-cli-1.2.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/log4j-1.2.17.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/activation-1.1.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/xz-1.0.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/servlet-api-2.5.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/commons-codec-1.4.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-core-1.9.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-client-1.9.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/guice-3.0.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/javax.inject-1.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/aopalliance-1.0.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/commons-io-2.4.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-server-1.9.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/asm-3.2.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-json-1.9.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/jettison-1.1.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/commons-collections-3.2.2.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/jetty-6.1.26.jar:/home/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-api-2.7.3.jar:/home/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-common-2.7.3.jar:/home/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-common-2.7.3.jar:/home/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.7.3.jar:/home/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.7.3.jar:/home/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.7.3.jar:/home/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.7.3.jar:/home/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-tests-2.7.3.jar:/home/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-client-2.7.3.jar:/home/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.7.3.jar:/home/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.3.jar:/home/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.7.3.jar:/home/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-registry-2.7.3.jar:/hpme/hadoop-2.7.3/share/hadoop/mapreduce/*:/home/hadoop-2.7.3/contrib/capacity-scheduler/*.jar:/home/hadoop-2.7.3/contrib/capacity-scheduler/*.jar:/home/hadoop-2.7.3/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r baa91f7c6bc9cb92be5982de4719c1c8af91ccff; compiled by 'root' on 2016-08-18T01:41Z
STARTUP_MSG:   java = 1.8.0_101
************************************************************/
2019-04-29 18:05:45,589 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: registered UNIX signal handlers for [TERM, HUP, INT]
2019-04-29 18:05:45,946 WARN org.apache.hadoop.util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2019-04-29 18:05:46,021 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2019-04-29 18:05:46,086 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2019-04-29 18:05:46,086 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: SecondaryNameNode metrics system started
2019-04-29 18:05:46,321 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /tmp/hadoop-root/dfs/namesecondary/in_use.lock acquired by nodename 6161@master
2019-04-29 18:05:46,358 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: No KeyProvider found.
2019-04-29 18:05:46,358 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsLock is fair:true
2019-04-29 18:05:46,396 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.block.invalidate.limit=1000
2019-04-29 18:05:46,396 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true
2019-04-29 18:05:46,397 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
2019-04-29 18:05:46,399 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: The block deletion will start around 2019 Apr 29 18:05:46
2019-04-29 18:05:46,400 INFO org.apache.hadoop.util.GSet: Computing capacity for map BlocksMap
2019-04-29 18:05:46,400 INFO org.apache.hadoop.util.GSet: VM type       = 32-bit
2019-04-29 18:05:46,402 INFO org.apache.hadoop.util.GSet: 2.0% max memory 966.7 MB = 19.3 MB
2019-04-29 18:05:46,402 INFO org.apache.hadoop.util.GSet: capacity      = 2^22 = 4194304 entries
2019-04-29 18:05:46,433 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.block.access.token.enable=false
2019-04-29 18:05:46,433 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: defaultReplication         = 2
2019-04-29 18:05:46,433 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplication             = 512
2019-04-29 18:05:46,433 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: minReplication             = 1
2019-04-29 18:05:46,433 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplicationStreams      = 2
2019-04-29 18:05:46,433 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: replicationRecheckInterval = 3000
2019-04-29 18:05:46,433 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: encryptDataTransfer        = false
2019-04-29 18:05:46,434 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxNumBlocksToLog          = 1000
2019-04-29 18:05:46,435 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsOwner             = root (auth:SIMPLE)
2019-04-29 18:05:46,435 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: supergroup          = supergroup
2019-04-29 18:05:46,435 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: isPermissionEnabled = false
2019-04-29 18:05:46,435 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: HA Enabled: false
2019-04-29 18:05:46,437 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Append Enabled: true
2019-04-29 18:05:46,592 INFO org.apache.hadoop.util.GSet: Computing capacity for map INodeMap
2019-04-29 18:05:46,592 INFO org.apache.hadoop.util.GSet: VM type       = 32-bit
2019-04-29 18:05:46,592 INFO org.apache.hadoop.util.GSet: 1.0% max memory 966.7 MB = 9.7 MB
2019-04-29 18:05:46,592 INFO org.apache.hadoop.util.GSet: capacity      = 2^21 = 2097152 entries
2019-04-29 18:05:46,612 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: ACLs enabled? false
2019-04-29 18:05:46,612 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: XAttrs enabled? true
2019-04-29 18:05:46,612 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: Maximum size of an xattr: 16384
2019-04-29 18:05:46,612 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Caching file names occuring more than 10 times
2019-04-29 18:05:46,618 INFO org.apache.hadoop.util.GSet: Computing capacity for map cachedBlocks
2019-04-29 18:05:46,618 INFO org.apache.hadoop.util.GSet: VM type       = 32-bit
2019-04-29 18:05:46,618 INFO org.apache.hadoop.util.GSet: 0.25% max memory 966.7 MB = 2.4 MB
2019-04-29 18:05:46,618 INFO org.apache.hadoop.util.GSet: capacity      = 2^19 = 524288 entries
2019-04-29 18:05:46,624 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.threshold-pct = 0.9990000128746033
2019-04-29 18:05:46,624 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.min.datanodes = 0
2019-04-29 18:05:46,624 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.extension     = 30000
2019-04-29 18:05:46,627 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.window.num.buckets = 10
2019-04-29 18:05:46,627 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.num.users = 10
2019-04-29 18:05:46,627 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25
2019-04-29 18:05:46,642 INFO org.apache.hadoop.hdfs.DFSUtil: Starting Web-server for secondary at: http://0.0.0.0:50090
2019-04-29 18:05:46,718 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2019-04-29 18:05:46,724 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2019-04-29 18:05:46,728 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.secondary is not defined
2019-04-29 18:05:46,733 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2019-04-29 18:05:46,734 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context secondary
2019-04-29 18:05:46,734 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2019-04-29 18:05:46,734 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2019-04-29 18:05:46,751 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 50090
2019-04-29 18:05:46,751 INFO org.mortbay.log: jetty-6.1.26
2019-04-29 18:05:46,869 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50090
2019-04-29 18:05:46,869 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Web server init done
2019-04-29 18:05:46,873 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Checkpoint Period   :3600 secs (60 min)
2019-04-29 18:05:46,873 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Log Size Trigger    :1000000 txns
2019-04-29 18:39:47,204 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Image has changed. Downloading updated image from NN.
2019-04-29 18:39:47,313 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Opening connection to http://master:50070/imagetransfer?getimage=1&txid=34&storageInfo=-63:2027304554:0:CID-c6975e7b-5169-483f-89cf-bbcabdcf5460
2019-04-29 18:39:47,328 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Image Transfer timeout configured to 60000 milliseconds
2019-04-29 18:39:47,520 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Transfer took 0,01s at 0,00 KB/s
2019-04-29 18:39:47,520 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file fsimage.ckpt_0000000000000000034 size 351 bytes.
2019-04-29 18:39:47,526 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Opening connection to http://master:50070/imagetransfer?getedit=1&startTxId=35&endTxId=36&storageInfo=-63:2027304554:0:CID-c6975e7b-5169-483f-89cf-bbcabdcf5460
2019-04-29 18:39:47,530 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Transfer took 0,00s at 0,00 KB/s
2019-04-29 18:39:47,530 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file edits_tmp_0000000000000000035-0000000000000000036_0000000000003623251 size 0 bytes.
2019-04-29 18:39:47,569 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatPBINode: Loading 1 INodes.
2019-04-29 18:39:47,592 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: Loaded FSImage in 0 seconds.
2019-04-29 18:39:47,592 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Loaded image for txid 34 from /tmp/hadoop-root/dfs/namesecondary/current/fsimage_0000000000000000034
2019-04-29 18:39:47,593 INFO org.apache.hadoop.hdfs.server.namenode.NameCache: initialized with 0 entries 0 lookups
2019-04-29 18:39:47,597 INFO org.apache.hadoop.hdfs.server.namenode.Checkpointer: Checkpointer about to load edits from 1 stream(s).
2019-04-29 18:39:47,601 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading /tmp/hadoop-root/dfs/namesecondary/current/edits_0000000000000000035-0000000000000000036 expecting start txid #35
2019-04-29 18:39:47,601 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file /tmp/hadoop-root/dfs/namesecondary/current/edits_0000000000000000035-0000000000000000036
2019-04-29 18:39:47,615 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file /tmp/hadoop-root/dfs/namesecondary/current/edits_0000000000000000035-0000000000000000036 of size 42 edits # 2 loaded in 0 seconds
2019-04-29 18:39:47,620 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: Saving image file /tmp/hadoop-root/dfs/namesecondary/current/fsimage.ckpt_0000000000000000036 using no compression
2019-04-29 18:39:47,665 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: Image file /tmp/hadoop-root/dfs/namesecondary/current/fsimage.ckpt_0000000000000000036 of size 351 bytes saved in 0 seconds.
2019-04-29 18:39:47,671 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Going to retain 2 images with txid >= 34
2019-04-29 18:39:47,671 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Purging old image FSImageFile(file=/tmp/hadoop-root/dfs/namesecondary/current/fsimage_0000000000000000031, cpktTxId=0000000000000000031)
2019-04-29 18:39:47,671 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Purging old image FSImageFile(file=/tmp/hadoop-root/dfs/namesecondary/current/fsimage_0000000000000000028, cpktTxId=0000000000000000028)
2019-04-29 18:39:47,696 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Uploaded image with txid 36 to namenode at http://master:50070 in 0.015 seconds
2019-04-29 18:39:47,696 WARN org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Checkpoint done. New Image Size: 351
2019-04-29 19:39:48,408 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Image has not changed. Will not download image.
2019-04-29 19:39:48,409 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Opening connection to http://master:50070/imagetransfer?getedit=1&startTxId=37&endTxId=128146&storageInfo=-63:2027304554:0:CID-c6975e7b-5169-483f-89cf-bbcabdcf5460
2019-04-29 19:39:48,506 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Transfer took 0,09s at 167500,00 KB/s
2019-04-29 19:39:48,506 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file edits_tmp_0000000000000000037-0000000000000128146_0000000000007224133 size 0 bytes.
2019-04-29 19:39:48,507 INFO org.apache.hadoop.hdfs.server.namenode.Checkpointer: Checkpointer about to load edits from 1 stream(s).
2019-04-29 19:39:48,507 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading /tmp/hadoop-root/dfs/namesecondary/current/edits_0000000000000000037-0000000000000128146 expecting start txid #37
2019-04-29 19:39:48,507 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file /tmp/hadoop-root/dfs/namesecondary/current/edits_0000000000000000037-0000000000000128146
2019-04-29 19:39:49,508 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader: replaying edit log: 65458/128110 transactions completed. (51%)
2019-04-29 19:39:50,088 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file /tmp/hadoop-root/dfs/namesecondary/current/edits_0000000000000000037-0000000000000128146 of size 15437258 edits # 128110 loaded in 1 seconds
2019-04-29 19:39:50,095 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: Saving image file /tmp/hadoop-root/dfs/namesecondary/current/fsimage.ckpt_0000000000000128146 using no compression
2019-04-29 19:39:50,217 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: Image file /tmp/hadoop-root/dfs/namesecondary/current/fsimage.ckpt_0000000000000128146 of size 872688 bytes saved in 0 seconds.
2019-04-29 19:39:50,221 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Going to retain 2 images with txid >= 36
2019-04-29 19:39:50,221 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Purging old image FSImageFile(file=/tmp/hadoop-root/dfs/namesecondary/current/fsimage_0000000000000000034, cpktTxId=0000000000000000034)
2019-04-29 19:39:50,261 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Uploaded image with txid 128146 to namenode at http://master:50070 in 0.036 seconds
2019-04-29 19:39:50,261 WARN org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Checkpoint done. New Image Size: 872688
2019-04-29 20:27:51,608 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master/192.168.178.37:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-04-29 20:27:52,612 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master/192.168.178.37:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-04-29 20:27:53,617 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master/192.168.178.37:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-04-29 20:27:54,619 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master/192.168.178.37:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-04-29 20:27:55,622 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master/192.168.178.37:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-04-29 20:27:56,623 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master/192.168.178.37:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-04-29 20:27:57,627 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master/192.168.178.37:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-04-29 20:27:58,632 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master/192.168.178.37:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-04-29 20:27:59,636 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master/192.168.178.37:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-04-29 20:28:00,636 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master/192.168.178.37:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-04-29 20:28:00,640 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.net.ConnectException: Call From master/192.168.178.37 to master:9000 failed on connection exception: java.net.ConnectException: Verbindungsaufbau abgelehnt; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1479)
	at org.apache.hadoop.ipc.Client.call(Client.java:1412)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy10.getTransactionId(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolTranslatorPB.getTransactionID(NamenodeProtocolTranslatorPB.java:128)
	at sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:191)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy11.getTransactionID(Unknown Source)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.countUncheckpointedTxns(SecondaryNameNode.java:641)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.shouldCheckpointBasedOnCount(SecondaryNameNode.java:649)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:393)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Verbindungsaufbau abgelehnt
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:614)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:712)
	at org.apache.hadoop.ipc.Client$Connection.access$2900(Client.java:375)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1528)
	at org.apache.hadoop.ipc.Client.call(Client.java:1451)
	... 17 more
2019-04-29 20:28:25,955 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: RECEIVED SIGNAL 15: SIGTERM
2019-04-29 20:28:25,956 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down SecondaryNameNode at master/192.168.178.37
************************************************************/
2019-05-13 18:44:25,381 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting SecondaryNameNode
STARTUP_MSG:   host = master/192.168.178.37
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.7.3
STARTUP_MSG:   classpath = /home/hadoop-2.7.3/etc/hadoop:/home/hadoop-2.7.3/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/stax-api-1.0-2.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/activation-1.1.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/jersey-server-1.9.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/asm-3.2.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/log4j-1.2.17.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/jets3t-0.9.0.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/httpclient-4.2.5.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/httpcore-4.2.5.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/commons-lang-2.6.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/commons-configuration-1.6.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/commons-digester-1.8.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/avro-1.7.4.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/paranamer-2.3.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/commons-compress-1.4.1.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/xz-1.0.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/gson-2.2.4.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/hadoop-auth-2.7.3.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/zookeeper-3.4.6.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/netty-3.6.2.Final.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/curator-framework-2.7.1.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/curator-client-2.7.1.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/jsch-0.1.42.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/htrace-core-3.1.0-incubating.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/junit-4.11.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/hamcrest-core-1.3.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/mockito-all-1.8.5.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/hadoop-annotations-2.7.3.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/guava-11.0.2.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/jsr305-3.0.0.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/commons-cli-1.2.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/commons-math3-3.1.1.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/xmlenc-0.52.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/commons-httpclient-3.1.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/commons-logging-1.1.3.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/commons-codec-1.4.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/commons-io-2.4.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/commons-net-3.1.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/commons-collections-3.2.2.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/servlet-api-2.5.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/jetty-6.1.26.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/jetty-util-6.1.26.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/jsp-api-2.1.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/jersey-core-1.9.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/jersey-json-1.9.jar:/home/hadoop-2.7.3/share/hadoop/common/lib/jettison-1.1.jar:/home/hadoop-2.7.3/share/hadoop/common/hadoop-common-2.7.3.jar:/home/hadoop-2.7.3/share/hadoop/common/hadoop-common-2.7.3-tests.jar:/home/hadoop-2.7.3/share/hadoop/common/hadoop-nfs-2.7.3.jar:/home/hadoop-2.7.3/share/hadoop/hdfs:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/guava-11.0.2.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-io-2.4.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/asm-3.2.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/htrace-core-3.1.0-incubating.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/hadoop-hdfs-2.7.3.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/hadoop-hdfs-2.7.3-tests.jar:/home/hadoop-2.7.3/share/hadoop/hdfs/hadoop-hdfs-nfs-2.7.3.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/commons-lang-2.6.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/guava-11.0.2.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/commons-cli-1.2.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/log4j-1.2.17.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/activation-1.1.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/xz-1.0.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/servlet-api-2.5.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/commons-codec-1.4.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-core-1.9.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-client-1.9.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/guice-3.0.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/javax.inject-1.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/aopalliance-1.0.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/commons-io-2.4.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-server-1.9.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/asm-3.2.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-json-1.9.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/jettison-1.1.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/commons-collections-3.2.2.jar:/home/hadoop-2.7.3/share/hadoop/yarn/lib/jetty-6.1.26.jar:/home/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-api-2.7.3.jar:/home/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-common-2.7.3.jar:/home/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-common-2.7.3.jar:/home/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.7.3.jar:/home/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.7.3.jar:/home/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.7.3.jar:/home/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.7.3.jar:/home/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-tests-2.7.3.jar:/home/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-client-2.7.3.jar:/home/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.7.3.jar:/home/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.3.jar:/home/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.7.3.jar:/home/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-registry-2.7.3.jar:/hpme/hadoop-2.7.3/share/hadoop/mapreduce/*:/home/hadoop-2.7.3/contrib/capacity-scheduler/*.jar:/home/hadoop-2.7.3/contrib/capacity-scheduler/*.jar:/home/hadoop-2.7.3/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r baa91f7c6bc9cb92be5982de4719c1c8af91ccff; compiled by 'root' on 2016-08-18T01:41Z
STARTUP_MSG:   java = 1.8.0_101
************************************************************/
2019-05-13 18:44:25,393 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: registered UNIX signal handlers for [TERM, HUP, INT]
2019-05-13 18:44:25,729 WARN org.apache.hadoop.util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2019-05-13 18:44:25,798 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2019-05-13 18:44:25,855 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2019-05-13 18:44:25,855 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: SecondaryNameNode metrics system started
2019-05-13 18:44:26,028 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /tmp/hadoop-root/dfs/namesecondary/in_use.lock acquired by nodename 6737@master
2019-05-13 18:44:26,063 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: No KeyProvider found.
2019-05-13 18:44:26,063 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsLock is fair:true
2019-05-13 18:44:26,104 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.block.invalidate.limit=1000
2019-05-13 18:44:26,104 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true
2019-05-13 18:44:26,105 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
2019-05-13 18:44:26,107 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: The block deletion will start around 2019 Mai 13 18:44:26
2019-05-13 18:44:26,108 INFO org.apache.hadoop.util.GSet: Computing capacity for map BlocksMap
2019-05-13 18:44:26,108 INFO org.apache.hadoop.util.GSet: VM type       = 32-bit
2019-05-13 18:44:26,109 INFO org.apache.hadoop.util.GSet: 2.0% max memory 966.7 MB = 19.3 MB
2019-05-13 18:44:26,109 INFO org.apache.hadoop.util.GSet: capacity      = 2^22 = 4194304 entries
2019-05-13 18:44:26,135 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.block.access.token.enable=false
2019-05-13 18:44:26,135 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: defaultReplication         = 2
2019-05-13 18:44:26,135 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplication             = 512
2019-05-13 18:44:26,135 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: minReplication             = 1
2019-05-13 18:44:26,135 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplicationStreams      = 2
2019-05-13 18:44:26,135 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: replicationRecheckInterval = 3000
2019-05-13 18:44:26,135 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: encryptDataTransfer        = false
2019-05-13 18:44:26,135 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxNumBlocksToLog          = 1000
2019-05-13 18:44:26,137 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsOwner             = root (auth:SIMPLE)
2019-05-13 18:44:26,137 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: supergroup          = supergroup
2019-05-13 18:44:26,137 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: isPermissionEnabled = false
2019-05-13 18:44:26,137 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: HA Enabled: false
2019-05-13 18:44:26,139 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Append Enabled: true
2019-05-13 18:44:26,283 INFO org.apache.hadoop.util.GSet: Computing capacity for map INodeMap
2019-05-13 18:44:26,283 INFO org.apache.hadoop.util.GSet: VM type       = 32-bit
2019-05-13 18:44:26,283 INFO org.apache.hadoop.util.GSet: 1.0% max memory 966.7 MB = 9.7 MB
2019-05-13 18:44:26,283 INFO org.apache.hadoop.util.GSet: capacity      = 2^21 = 2097152 entries
2019-05-13 18:44:26,304 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: ACLs enabled? false
2019-05-13 18:44:26,304 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: XAttrs enabled? true
2019-05-13 18:44:26,304 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: Maximum size of an xattr: 16384
2019-05-13 18:44:26,304 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Caching file names occuring more than 10 times
2019-05-13 18:44:26,310 INFO org.apache.hadoop.util.GSet: Computing capacity for map cachedBlocks
2019-05-13 18:44:26,310 INFO org.apache.hadoop.util.GSet: VM type       = 32-bit
2019-05-13 18:44:26,310 INFO org.apache.hadoop.util.GSet: 0.25% max memory 966.7 MB = 2.4 MB
2019-05-13 18:44:26,310 INFO org.apache.hadoop.util.GSet: capacity      = 2^19 = 524288 entries
2019-05-13 18:44:26,312 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.threshold-pct = 0.9990000128746033
2019-05-13 18:44:26,312 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.min.datanodes = 0
2019-05-13 18:44:26,312 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.extension     = 30000
2019-05-13 18:44:26,316 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.window.num.buckets = 10
2019-05-13 18:44:26,316 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.num.users = 10
2019-05-13 18:44:26,316 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25
2019-05-13 18:44:26,325 INFO org.apache.hadoop.hdfs.DFSUtil: Starting Web-server for secondary at: http://0.0.0.0:50090
2019-05-13 18:44:26,381 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2019-05-13 18:44:26,386 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2019-05-13 18:44:26,390 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.secondary is not defined
2019-05-13 18:44:26,395 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2019-05-13 18:44:26,396 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context secondary
2019-05-13 18:44:26,396 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2019-05-13 18:44:26,396 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2019-05-13 18:44:26,411 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 50090
2019-05-13 18:44:26,411 INFO org.mortbay.log: jetty-6.1.26
2019-05-13 18:44:26,529 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50090
2019-05-13 18:44:26,529 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Web server init done
2019-05-13 18:44:26,537 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Checkpoint Period   :3600 secs (60 min)
2019-05-13 18:44:26,537 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Log Size Trigger    :1000000 txns
2019-05-13 19:40:27,111 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Image has changed. Downloading updated image from NN.
2019-05-13 19:40:27,221 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Opening connection to http://master:50070/imagetransfer?getimage=1&txid=152370&storageInfo=-63:2027304554:0:CID-c6975e7b-5169-483f-89cf-bbcabdcf5460
2019-05-13 19:40:27,230 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Image Transfer timeout configured to 60000 milliseconds
2019-05-13 19:40:27,419 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Transfer took 0,03s at 40103,45 KB/s
2019-05-13 19:40:27,419 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file fsimage.ckpt_0000000000000152370 size 1191681 bytes.
2019-05-13 19:40:27,426 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Opening connection to http://master:50070/imagetransfer?getedit=1&startTxId=152371&endTxId=152558&storageInfo=-63:2027304554:0:CID-c6975e7b-5169-483f-89cf-bbcabdcf5460
2019-05-13 19:40:27,431 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Transfer took 0,00s at 5250,00 KB/s
2019-05-13 19:40:27,431 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file edits_tmp_0000000000000152371-0000000000000152558_0000000000003617477 size 0 bytes.
2019-05-13 19:40:27,471 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatPBINode: Loading 15006 INodes.
2019-05-13 19:40:27,634 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: Loaded FSImage in 0 seconds.
2019-05-13 19:40:27,634 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Loaded image for txid 152370 from /tmp/hadoop-root/dfs/namesecondary/current/fsimage_0000000000000152370
2019-05-13 19:40:27,634 INFO org.apache.hadoop.hdfs.server.namenode.NameCache: initialized with 0 entries 0 lookups
2019-05-13 19:40:27,639 INFO org.apache.hadoop.hdfs.server.namenode.Checkpointer: Checkpointer about to load edits from 1 stream(s).
2019-05-13 19:40:27,643 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading /tmp/hadoop-root/dfs/namesecondary/current/edits_0000000000000152371-0000000000000152558 expecting start txid #152371
2019-05-13 19:40:27,643 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file /tmp/hadoop-root/dfs/namesecondary/current/edits_0000000000000152371-0000000000000152558
2019-05-13 19:40:27,697 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file /tmp/hadoop-root/dfs/namesecondary/current/edits_0000000000000152371-0000000000000152558 of size 22489 edits # 188 loaded in 0 seconds
2019-05-13 19:40:27,716 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: Saving image file /tmp/hadoop-root/dfs/namesecondary/current/fsimage.ckpt_0000000000000152558 using no compression
2019-05-13 19:40:27,842 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: Image file /tmp/hadoop-root/dfs/namesecondary/current/fsimage.ckpt_0000000000000152558 of size 1194788 bytes saved in 0 seconds.
2019-05-13 19:40:27,847 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Going to retain 2 images with txid >= 152370
2019-05-13 19:40:27,879 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Uploaded image with txid 152558 to namenode at http://master:50070 in 0.028 seconds
2019-05-13 19:40:27,880 WARN org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Checkpoint done. New Image Size: 1194788
2019-05-13 20:22:45,276 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: RECEIVED SIGNAL 15: SIGTERM
2019-05-13 20:22:45,277 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down SecondaryNameNode at master/192.168.178.37
************************************************************/
